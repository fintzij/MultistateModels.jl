# PIJCV Gradient via Full AD (Option A Only)

**Date**: 2026-01-27 (updated 2026-01-28)  
**Branch**: `penalized_splines`  
**Status**: ‚úÖ INTEGRATED INTO CODEBASE - Production `fit()` uses correct gradient

---

## ‚úÖ COMPLETED: Correct PIJCV AD Gradient (2026-01-28)

**The analytical gradient for the CORRECT PIJCV formula is now working.**

### Validation Results

| œÅ | dV/dœÅ (analytical) | dV/dœÅ (FD) | Ratio |
|---|---|---|---|
| 0.0 | 0.38710 | 0.38710 | 1.0000 |
| 1.0 | 0.39951 | 0.39951 | 1.0000 |
| 2.0 | 0.57071 | 0.57071 | 1.0000 |
| 3.0 | 1.02390 | 1.02390 | 1.0000 |
| 4.0 | 1.59791 | 1.59791 | 1.0000 |

**Working implementation**: `scratch/test_correct_pijcv_ad_v5.jl`

### Key Breakthrough: Sign Error Fix

The critical bug was in `dg·µ¢/dœÅ`. Since g·µ¢ = -‚àá‚Ñì·µ¢(Œ≤ÃÇ) and H·µ¢ = -‚àá¬≤‚Ñì·µ¢(Œ≤ÃÇ):

**WRONG**: dg·µ¢/dœÅ = -H·µ¢¬∑dŒ≤ÃÇ/dœÅ  
**CORRECT**: dg·µ¢/dœÅ = +H·µ¢¬∑dŒ≤ÃÇ/dœÅ

### Third Derivatives ARE Required

Contrary to initial hopes, third derivatives (‚àÇH·µ¢/‚àÇŒ≤) are necessary for correct gradients. The "simplified" chain rule (ignoring third derivatives) achieved only 87-105% accuracy. With full third derivatives, we get machine precision.

---

## ‚ö†Ô∏è CRITICAL: Correct PIJCV Formulation

**Previous implementation was WRONG.** We were using a quadratic approximation of V, but the correct NCV criterion (Wood 2024, Section 2, Equation 2) evaluates the loss at **pseudo-estimates**:

### Correct Formula

For leave-one-out cross-validation:

**V(œÅ) = Œ£·µ¢ -‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢)**

where:
- **Œ≤ÃÉ‚Çã·µ¢ = Œ≤ÃÇ(œÅ) - Œî‚Åª‚Å±** is the pseudo-estimate (one Newton step from Œ≤ÃÇ)
- **Œî‚Åª‚Å± = (H_Œª - H·µ¢)‚Åª¬π g·µ¢** is the LOO step
- **g·µ¢ = -‚àá‚Ñì·µ¢(Œ≤ÃÇ)** is the per-subject score at the full MLE
- **H·µ¢ = -‚àá¬≤‚Ñì·µ¢(Œ≤ÃÇ)** is the per-subject Hessian at the full MLE
- **H_Œª = Œ£·µ¢ H·µ¢ + Œ£‚±º Œª‚±º S‚±º** is the penalized Hessian

### Why This Matters for AD

The correct formulation requires **third derivatives** (‚àÇH·µ¢/‚àÇŒ≤) for correct gradients. Initial hopes that third derivatives could be ignored were wrong‚Äîthe "simplified" chain rule only achieved 87-105% accuracy.

### The Wrong Formula (what we had)

V_wrong = Œ£·µ¢ [-‚Ñì·µ¢(Œ≤ÃÇ) + g·µ¢·µÄŒî‚Åª‚Å± + ¬Ω(Œî‚Åª‚Å±)·µÄH·µ¢Œî‚Åª‚Å±]

This is a quadratic approximation of ‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢), not the actual value!

---

## ‚úÖ Working Algorithm (Validated 2026-01-27)

### Step 1: dŒ≤ÃÇ/dœÅ via ImplicitDifferentiation.jl

```julia
implicit_beta = ImplicitFunction(forward_solve, optimality_conditions;
    representation=MatrixRepresentation(),
    linear_solver=DirectLinearSolver())

dbeta_drho = ForwardDiff.jacobian(œÅ_vec -> implicit_beta(œÅ_vec)[1], [œÅ])[:, 1]
```

### Step 2: Third Derivative Tensors ‚àÇH·µ¢/‚àÇŒ≤

```julia
for i in 1:n_subj
    H_flat_jac = ForwardDiff.jacobian(
        Œ≤ -> vec(-ForwardDiff.hessian(b -> loglik_subject(b, data, i), Œ≤)),
        Œ≤_opt
    )
    dH_dbeta_i = reshape(H_flat_jac, n_beta, n_beta, n_beta)
end
```

### Step 3: dH_Œª/dœÅ with Third Derivatives

```julia
dH_Œª_drho = Œª * S_full
for i in 1:n_subj
    for l in 1:n_beta
        dH_Œª_drho .+= dH_dbeta_i[:,:,l] * dbeta_drho[l]
    end
end
```

### Step 4: Per-Subject Gradient (CORRECT SIGNS)

```julia
for i in 1:n_subj
    # g·µ¢ = -‚àá‚Ñì·µ¢(Œ≤ÃÇ), H·µ¢ = -‚àá¬≤‚Ñì·µ¢(Œ≤ÃÇ)
    H_loo = H_Œª - H·µ¢
    Œî·µ¢ = H_loo \ g·µ¢
    Œ≤_tilde_i = Œ≤_opt - Œî·µ¢
    
    grad_ll_at_pseudo = ForwardDiff.gradient(b -> loglik_subject(b, data, i), Œ≤_tilde_i)
    
    # CRITICAL: dg·µ¢/dœÅ = +H·µ¢¬∑dŒ≤ÃÇ/dœÅ (not -H·µ¢!)
    dg·µ¢_drho = H·µ¢ * dbeta_drho
    
    # dH·µ¢/dœÅ = (‚àÇH·µ¢/‚àÇŒ≤)¬∑dŒ≤ÃÇ/dœÅ
    dH·µ¢_drho = zeros(n_beta, n_beta)
    for l in 1:n_beta
        dH·µ¢_drho .+= dH_dbeta_i[:,:,l] * dbeta_drho[l]
    end
    
    # dH_loo/dœÅ = dH_Œª/dœÅ - dH·µ¢/dœÅ
    dH_loo_drho = dH_Œª_drho - dH·µ¢_drho
    
    # dŒî·µ¢/dœÅ
    dDelta_drho = H_loo \ (dg·µ¢_drho - dH_loo_drho * Œî·µ¢)
    dbeta_tilde_drho = dbeta_drho - dDelta_drho
    
    # dV·µ¢/dœÅ = -‚àá‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢)·µÄ ¬∑ dŒ≤ÃÉ‚Çã·µ¢/dœÅ
    dV_i_drho = -dot(grad_ll_at_pseudo, dbeta_tilde_drho)
    dV_drho_total += dV_i_drho
end
```

---

## üìã Current Focus: Implement Correct PIJCV with AD

**Decision**: Option B (analytical gradient) is **REJECTED** due to using wrong formula AND neglecting third derivative terms. We must implement **Option A: Full AD** with the **correct PIJCV formula**.

### Key Insight

The correct V = Œ£·µ¢ -‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢) requires differentiating through:
1. **Œ≤ÃÇ(œÅ)** - handled by ImplicitDifferentiation.jl via IFT
2. **Œî‚Åª‚Å±(Œ≤ÃÇ, œÅ)** - depends on g·µ¢(Œ≤ÃÇ), H·µ¢(Œ≤ÃÇ), H_Œª(Œ≤ÃÇ, œÅ)
3. **‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢)** - standard likelihood evaluation

Since g·µ¢, H·µ¢, and ‚Ñì·µ¢ all depend on Œ≤ÃÇ, and Œ≤ÃÇ depends on œÅ, the chain rule gives:

dV/dœÅ = Œ£·µ¢ [-‚àá‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢)·µÄ ¬∑ dŒ≤ÃÉ‚Çã·µ¢/dœÅ]

where dŒ≤ÃÉ‚Çã·µ¢/dœÅ = dŒ≤ÃÇ/dœÅ - d(Œî‚Åª‚Å±)/dœÅ

### Mooncake Testing Results (2026-01-27)

**Mooncake + ImplicitDifferentiation.jl: ‚ùå DOES NOT WORK**

When using `DifferentiationInterface.gradient(objective, AutoMooncake(), x)`, Mooncake attempts to trace through the *entire* objective function, including the inner optimizer (Ipopt). This fails with `MooncakeRuleCompilationError`.

**ForwardDiff + ImplicitDifferentiation.jl: ‚úÖ WORKS CORRECTLY**

```julia
# This works perfectly:
implicit_beta = ImplicitFunction(forward_solve, optimality_conditions;
    backends=(x=AutoForwardDiff(), y=AutoForwardDiff()))

# dŒ≤ÃÇ/dœÅ matches finite differences to 8 decimal places:
grad_implicit = ForwardDiff.gradient(œÅ -> sum(implicit_beta(œÅ)[1]), [2.0])
# Result: [-0.0636219...]
# FD:     -0.0636219...
# Ratio:  0.9999999870711714
```

### Updated Architecture (ForwardDiff + ID.jl)

The correct approach uses ForwardDiff as the outer AD backend with ImplicitDifferentiation.jl:

```julia
# 1. Define forward solve and conditions for ImplicitDifferentiation.jl
function forward_solve(log_lambda)
    Œª = exp.([ForwardDiff.value(x) for x in log_lambda])
    Œ≤_opt = inner_optimizer(model, Œª)  # Opaque to AD
    return Œ≤_opt, nothing
end

function conditions(log_lambda, Œ≤, z)
    Œª = exp.(log_lambda)
    return ‚àáŒ≤_loglik(Œ≤) - Œ£‚±º Œª‚±º S‚±º Œ≤  # AD-compatible
end

# 2. Create implicit function with ForwardDiff backends
implicit_beta = ImplicitFunction(forward_solve, conditions;
    backends=(x=AutoForwardDiff(), y=AutoForwardDiff()))

# 3. Full PIJCV gradient via chain rule
# dV/dœÅ = (‚àÇV/‚àÇŒ≤)¬∑(dŒ≤ÃÇ/dœÅ) + (‚àÇV/‚àÇŒª)¬∑(dŒª/dœÅ)
function pijcv_gradient(œÅ)
    # dŒ≤ÃÇ/dœÅ from ImplicitDifferentiation.jl (IFT)
    dbeta_drho = ForwardDiff.jacobian(œÅ -> implicit_beta(œÅ)[1], œÅ)
    
    Œ≤_opt = implicit_beta(œÅ)[1]
    Œª = exp.(œÅ)
    
    # ‚àÇV/‚àÇŒ≤ and ‚àÇV/‚àÇŒª via ForwardDiff
    dV_dbeta = ForwardDiff.gradient(Œ≤ -> V(Œ≤, Œª), Œ≤_opt)
    dV_dlambda = ForwardDiff.gradient(Œª -> V(Œ≤_opt, Œª), Œª)
    
    # Chain rule
    return dV_dbeta' * dbeta_drho + dV_dlambda .* Œª
end
```

### Why Option B Failed (2026-01-28)

| Metric | Analytical (Option B) | True (FD) | Problem |
|--------|----------------------|-----------|---------|
| log(Œª) at optimum | 2.77 | 3.12 | **0.35 shift** |
| Œª at optimum | ~16 | ~23 | **30% bias** |
| V at optimum | 160.69 | 160.66 | Negligible |

The analytical formula neglects ‚àÇH·µ¢/‚àÇŒ≤ (third derivatives of log-likelihood). Near the optimum where gradients are small, this omission dominates and causes systematic bias. **This is not a bug‚Äîit's a fundamental limitation of the Newton-step approximation.**

### Option A Path Forward

**Key Insight** (verified 2026-01-27): Zygote CAN differentiate through ForwardDiff.hessian. We can bypass ImplicitDifferentiation.jl's buggy Zygote extension by implementing IFT manually inside a Zygote-differentiable function.

**Architecture**:
```julia
function pijcv_objective_zygote(log_Œª)
    Œª = exp.(log_Œª)
---

## üö´ Why Option B (Analytical Gradient) Was Rejected

**Date**: 2026-01-28

### The Fundamental Problem

The analytical gradient formula:
```
‚àÇV/‚àÇœÅ‚Çñ = g·µÄœÜ‚Çñ + Œ£·µ¢[(Œî‚Åª‚Å±)·µÄH·µ¢œÜ‚Çñ + r·µ¢·µÄœà·µ¢‚Çñ]
```

**assumes ‚àÇH·µ¢/‚àÇŒ≤ = 0** (i.e., per-subject Hessians are constant w.r.t. Œ≤). This is false‚ÄîH·µ¢ involves second derivatives of log-likelihood, so ‚àÇH·µ¢/‚àÇŒ≤ involves third derivatives.

### Empirical Evidence

Testing at log(Œª) values from -1 to 5:

| log(Œª) | Analytical | FD | Ratio |
|--------|------------|-----|-------|
| -1.00 | +0.20 | +0.18 | 1.11 |
| 0.00 | -0.07 | -0.08 | 0.83 |
| 2.00 | -0.19 | -0.26 | 0.71 |
| **3.00 (opt)** | **+0.10** | **-0.05** | **-1.89** |
| 4.00 | +0.89 | +0.64 | 1.39 |

**At the optimum, the signs disagree.** The analytical gradient crosses zero ~0.35 earlier (on log scale) than the true gradient, causing 30% underestimation of optimal Œª.

### Why This Cannot Be Fixed Without Full AD

Adding ‚àÇH·µ¢/‚àÇŒ≤ requires:
1. Computing third derivatives of per-subject log-likelihoods
2. Implementing complex tensor contractions
3. Significant code complexity with high bug potential

**Full AD (Option A) computes the correct gradient automatically** without manually deriving/implementing third derivatives.

---

## ‚úÖ Option A: Full AD with Zygote + Manual IFT

### Why This Works

**Verified (2026-01-27)**: Zygote CAN differentiate through ForwardDiff.hessian:
```julia
function inner_with_hessian(x)
    f(y) = sum(y.^3) + 0.5 * dot(y, y)
    H = ForwardDiff.hessian(f, x)
    return tr(H)
end

grad_zygote = Zygote.gradient(inner_with_hessian, x_test)[1]  # ‚úì Works!
```

This means we can use:
- **ForwardDiff** (forward-mode) for outer differentiation w.r.t. œÅ = log(Œª)
- **ImplicitDifferentiation.jl** with ForwardDiff backends for ‚àÇŒ≤ÃÇ/‚àÇœÅ
- **ForwardDiff** for inner Hessian computation (g·µ¢, H·µ¢)

### AD Backend Summary (2026-01-27)

| Backend | Status | Notes |
|---------|--------|-------|
| **ForwardDiff + ID.jl** | ‚úÖ WORKS | dŒ≤ÃÇ/dœÅ matches FD to 8 decimals |
| **Mooncake + ID.jl** | ‚ùå FAILS | `MooncakeRuleCompilationError` on Ipopt calls |
| **Zygote + ID.jl** | ‚ùå FAILS | `DimensionMismatch` with vector outputs |

**Recommended approach**: ForwardDiff for all differentiation, using ImplicitDifferentiation.jl with `backends=(x=AutoForwardDiff(), y=AutoForwardDiff())`.

---

## ‚ö†Ô∏è CRITICAL CONSTRAINT

**NEVER USE FINITE DIFFERENCES.** This project requires analytical gradients computed via automatic differentiation or closed-form derivations. Finite differences violate core project principles:
1. They are numerically unstable
2. They scale poorly with dimension  
3. They defeat the purpose of implicit differentiation infrastructure
4. They are explicitly forbidden in `.github/copilot-instructions.md`

All gradient computations must use ForwardDiff, ReverseDiff, Zygote, Enzyme, or analytically-derived formulas.

---

## Implementation Plan (Option A Only)

### Phase 0: Implement ForwardDiff + ID.jl Full AD ‚úÖ COMPLETED (2026-01-28)

**Goal**: Compute exact PIJCV gradients using ForwardDiff + ImplicitDifferentiation.jl.

| Task | Status | Description |
|------|--------|-------------|
| 0.1 | [x] | Test Mooncake + ID.jl (FAILED - cannot trace through Ipopt) |
| 0.2 | [x] | Test ForwardDiff + ID.jl for dŒ≤ÃÇ/dœÅ (WORKS - matches FD to 8 decimals) |
| 0.3 | [x] | Implement chain rule: dV/dœÅ = -‚àá‚Ñì·µ¢(Œ≤ÃÉ‚Çã·µ¢)·µÄ¬∑dŒ≤ÃÉ‚Çã·µ¢/dœÅ with third derivatives |
| 0.4 | [x] | Verify gradient matches FD within 5% at log(Œª) ‚àà {0, 1, 2, 3, 4} (ratio ‚âà 1.0000) |
| 0.5 | [x] | Verify analytical and FD zero-crossings agree (validated in test_correct_pijcv_ad_v5.jl) |
| 0.6 | [x] | Integrate into `_nested_optimization_pijcv_implicit` |
| 0.7 | [x] | Run unit test suite (2079/2079 passed) |
| 0.8 | [ ] | Run full test suite with MSM_TEST_LEVEL=full |

### Codebase Integration (2026-01-28)

**Production `fit()` now uses the correct gradient by default.**

Files added/modified:
- `src/inference/smoothing_selection/implicit_diff.jl` (NEW - 1792 lines):
  - `compute_pijcv_with_gradient` with CORRECT formula + third derivatives
  - `_compute_subject_third_derivatives` helper function
  - `ncv_criterion_and_gradient` computes `dbeta_drho` via ImplicitDifferentiation.jl
  - `_nested_optimization_pijcv_implicit` main entry point for ExactData
  - Support for MPanelData and MCEMSelectionData

- `src/types/penalties.jl`:
  - `PIJCVSelector.use_implicit_diff` defaults to `true`

- `src/inference/smoothing_selection/dispatch_exact.jl`:
  - Routes to `_nested_optimization_pijcv_implicit` when `use_implicit_diff=true` (default)

**Validation**:
- Unit tests: 2079/2079 passed
- Gradient vs FD ratio ‚âà 1.0000 across log(Œª) ‚àà {0, 1, 2, 3, 4}

**Note**: Files are currently UNTRACKED in git. Commit pending.

### Implementation Architecture (ForwardDiff + ImplicitDifferentiation.jl)

```julia
using ImplicitDifferentiation, ADTypes

# Step 1: Define forward and conditions functions for ID.jl
function forward_solve(Œª::AbstractVector, model, data)
    # Solve penalized MLE for Œ≤ÃÇ at given Œª
    return actual_inner_optimization(model, data, Œª)
end

function conditions(Œª::AbstractVector, Œ≤::AbstractVector, model, data)
    # Score equation: ‚àá‚Ñì(Œ≤) - SŒªŒ≤ = 0 at optimum
    return compute_penalized_score(Œ≤, Œª, model, data)
end

# Step 2: Create implicit function with ForwardDiff backends
implicit_beta = ImplicitFunction(
    (Œª, args...) -> forward_solve(Œª, args...),
    (Œª, Œ≤, args...) -> conditions(Œª, Œ≤, args...);
    backends=(x=AutoForwardDiff(), y=AutoForwardDiff())
)

# Step 3: V computation that takes (Œ≤, Œª) as inputs
function compute_V(Œ≤::AbstractVector, Œª::AbstractVector, model, data)
    V = zero(promote_type(eltype(Œ≤), eltype(Œª)))
    H_Œª = compute_penalized_hessian(Œ≤, Œª, model, data)
    
    for i in 1:n_subjects
        ‚Ñì·µ¢, g·µ¢, H·µ¢ = compute_subject_derivatives(Œ≤, model, data, i)
        H_loo = H_Œª - H·µ¢
        Œî·µ¢ = H_loo \ g·µ¢
        V += -‚Ñì·µ¢ + dot(g·µ¢, Œî·µ¢) + 0.5 * dot(Œî·µ¢, H·µ¢ * Œî·µ¢)
    end
    return V
end

# Step 4: Full PIJCV objective with chain rule gradient
function pijcv_objective(œÅ, model, data)
    Œª = exp.(œÅ)
    Œ≤ÃÇ = implicit_beta(Œª, model, data)  # ID.jl handles dŒ≤ÃÇ/dŒª via IFT
    return compute_V(Œ≤ÃÇ, Œª, model, data)
end

# Step 5: Get gradient via ForwardDiff + chain rule
# dV/dœÅ = (‚àÇV/‚àÇŒ≤)¬∑(dŒ≤ÃÇ/dœÅ) + (‚àÇV/‚àÇŒª)¬∑(dŒª/dœÅ)
# where dŒª/dœÅ = diag(Œª) and dŒ≤ÃÇ/dœÅ = (dŒ≤ÃÇ/dŒª)¬∑diag(Œª) via IFT
grad_V = ForwardDiff.gradient(œÅ -> pijcv_objective(œÅ, model, data), log_Œª)
```

### Key Insight: Why This Works

1. **ImplicitDifferentiation.jl** uses ForwardDiff to compute both:
   - Forward pass: Jacobian of conditions w.r.t. Œ≤ (for implicit function theorem)
   - Backward pass: Jacobian of conditions w.r.t. Œª (for the derivative dŒ≤ÃÇ/dŒª)

2. **ForwardDiff outer differentiation** computes the total derivative dV/dœÅ via chain rule, with ID.jl providing the correct dŒ≤ÃÇ/dœÅ term.

3. **No tracing through optimizer**: ID.jl uses the implicit function theorem to avoid differentiating through the Ipopt solver calls.

### Key Components to Implement

1. **`compute_subject_derivatives(Œ≤, model, data, i)`**: Returns (‚Ñì·µ¢, g·µ¢, H·µ¢) using ForwardDiff. Must be Dual-number compatible.

2. **Forward solve wrapper**: Calls existing `fit_inner_coefficients` without modification.

3. **Conditions function**: Returns the score equation residual (should be ‚âà0 at optimum).

---

## Implementation Plan (Updated 2026-01-27)

### Phase 0: Test ForwardDiff + ID.jl Full Gradient (CURRENT PRIORITY)

**Goal**: Verify that ForwardDiff + ImplicitDifferentiation.jl computes correct PIJCV gradients via chain rule.

**Status**: dŒ≤ÃÇ/dœÅ verified ‚úÖ, full gradient via chain rule pending

| Task | Status | Description |
|------|--------|-------------|
| 0.1 | [x] | Test Mooncake + ID.jl (FAILED - cannot trace through Ipopt) |
| 0.2 | [x] | Test ForwardDiff + ID.jl for dŒ≤ÃÇ/dœÅ (WORKS - matches FD to 8 decimals) |
| 0.3 | [ ] | Test ForwardDiff for ‚àÇV/‚àÇŒ≤ and ‚àÇV/‚àÇŒª |
| 0.4 | [ ] | Combine via chain rule: dV/dœÅ = (‚àÇV/‚àÇŒ≤)¬∑(dŒ≤ÃÇ/dœÅ) + (‚àÇV/‚àÇŒª)¬∑(dŒª/dœÅ) |
| 0.5 | [ ] | Verify gradient matches FD within 5% at log(Œª) ‚àà {-1, 0, 1, 2, 3, 4, 5} |
| 0.6 | [ ] | Verify analytical and FD zero-crossings agree within 0.1 on log scale |

**Test Script**: `scratch/test_forwarddiff_implicit.jl`

**Key Results (2026-01-27)**:
- dŒ≤ÃÇ/dœÅ via ImplicitDifferentiation.jl: `[-0.0636219...]`
- dŒ≤ÃÇ/dœÅ via finite difference: `[-0.0636219...]`  
- Ratio: `0.9999999870711714` ‚úÖ

### Phase 0b: Debug Option B Analytical Gradient (SECONDARY PRIORITY)

**Goal**: Find and fix the bug causing the analytical gradient to be ~83% of the FD value.

The gradient formula is:
```
‚àÇV/‚àÇœÅ‚Çñ = g·µÄœÜ‚Çñ + Œ£·µ¢[(Œî‚Åª‚Å±)·µÄH·µ¢œÜ‚Çñ + r·µ¢·µÄœà·µ¢‚Çñ]
```

where `r·µ¢ = g·µ¢ + H·µ¢Œî‚Åª‚Å±` and `œà·µ¢‚Çñ = H_{Œª,-i}‚Åª¬π(H·µ¢œÜ‚Çñ - Œª‚ÇñS‚ÇñŒî‚Åª‚Å±)`.

To debug, separate into:
- **Term 1**: `g·µÄœÜ‚Çñ` (contribution from -‚Ñì·µ¢(Œ≤ÃÇ) through Œ≤ dependence)
- **Term 2**: `Œ£·µ¢(Œî‚Åª‚Å±)·µÄH·µ¢œÜ‚Çñ` (contribution from g·µ¢ changing in g·µ¢·µÄŒî‚Åª‚Å±)
- **Term 3**: `Œ£·µ¢r·µ¢·µÄœà·µ¢‚Çñ` (contribution from Œî‚Åª‚Å± changing + quadratic term)

Compare each analytically-computed term against its finite difference.

**Files**:
- Debug script: `scratch/test_gradient.jl`
- Main implementation: `src/inference/smoothing_selection/implicit_diff.jl` lines 875-1065
- Unit tests: `MultistateModelsTests/unit/test_implicit_diff.jl` section 4b

### Phase 1: Enable Integration Tests (After Phase 0)

| Task | Status | Description |
|------|--------|-------------|
| 1.1 | [ ] | Uncomment section 5 in test_implicit_diff.jl |
| 1.2 | [ ] | Verify Œª matches legacy within 10% (tighter than before) |
| 1.3 | [ ] | Add performance benchmark test |

### Phase 2: Remove Legacy Code and Option B (After Phase 1)

| Task | Status | Description |
|------|--------|-------------|
| 2.1 | [ ] | Delete `_nested_optimization_pijcv` from dispatch_general.jl |
| 2.2 | [ ] | Delete `_nested_optimization_pijcv_markov` from dispatch_markov.jl |
| 2.3 | [ ] | Delete `_nested_optimization_pijcv_mcem` from dispatch_mcem.jl |
| 2.4 | [ ] | Remove `use_implicit_diff` field from PIJCVSelector |
| 2.5 | [ ] | **Delete `compute_pijcv_with_gradient` (Option B code)** |
| 2.6 | [ ] | Simplify dispatch files |
| 2.7 | [ ] | Update all docstrings |

---

## Code Complexity Estimate

| Component | Lines | Description |
|-----------|-------|-------------|
| `compute_V_zygote()` | ~80 | Zygote-compatible V computation |
| `compute_subject_derivatives()` | ~60 | Per-subject ‚Ñì·µ¢, g·µ¢, H·µ¢ via ForwardDiff |
| Custom Zygote adjoint for IFT | ~40 | Manual IFT pullback |
| Integration wrapper | ~50 | Connect to existing optimizer |
| Tests | ~100 | Comprehensive gradient verification |
| **Total** | **~330** |

### Performance Considerations

For p=50 params, n=500 subjects, n_Œª=4:
- **Zygote outer + ForwardDiff inner**: Reverse-mode outer is efficient; ForwardDiff Hessians are fast
- **Memory**: Zygote tape + per-subject Hessians ~200MB for this size
- **Expected speedup vs FD outer**: ~4-10x (FD requires 2√ón_Œª function evaluations)

---

## Appendix A: Mathematical Foundation

### PIJCV Criterion

V(œÅ) = Œ£·µ¢ [ -‚Ñì·µ¢(Œ≤ÃÇ) + g·µ¢·µÄ Œî‚Åª‚Å± + ¬Ω(Œî‚Åª‚Å±)·µÄ H·µ¢ Œî‚Åª‚Å± ]

where:
- œÅ‚Çñ = log(Œª‚Çñ) (log-smoothing parameters)
- Œ≤ÃÇ = Œ≤ÃÇ(œÅ) is the penalized MLE
- g·µ¢ = -‚àá‚Ñì·µ¢(Œ≤ÃÇ) is subject i's loss gradient
- H·µ¢ = -‚àá¬≤‚Ñì·µ¢(Œ≤ÃÇ) is subject i's loss Hessian  
- Œî‚Åª‚Å± = H_{Œª,-i}‚Åª¬π g·µ¢ is the Newton step for LOO estimate
- H_{Œª,-i} = H_Œª - H·µ¢ (LOO penalized Hessian)
- H_Œª = Œ£·µ¢ H·µ¢ + Œ£‚±º Œª‚±º S‚±º (full penalized Hessian)

### Complete Gradient (via Full AD)

The full gradient ‚àÇV/‚àÇœÅ requires differentiating through:
1. Œ≤ÃÇ(œÅ) ‚Äî handled by IFT: ‚àÇŒ≤ÃÇ/‚àÇœÅ‚Çñ = -H_Œª‚Åª¬π(Œª‚ÇñS‚ÇñŒ≤ÃÇ)
2. g·µ¢(Œ≤ÃÇ) ‚Äî requires ‚àÇg·µ¢/‚àÇŒ≤ = H·µ¢ (second derivatives)
3. H·µ¢(Œ≤ÃÇ) ‚Äî requires ‚àÇH·µ¢/‚àÇŒ≤ (THIRD derivatives)
4. Œî‚Åª‚Å±(Œ≤ÃÇ, œÅ) ‚Äî depends on both Œ≤ÃÇ and Œª

**Option B (analytical) ignores term 3**, causing ~30% bias in Œª.

**Option A (full AD)** computes all terms automatically via Zygote + ForwardDiff.

### IFT Formula for Custom Adjoint

The implicit function theorem gives:
```
‚àÇŒ≤ÃÇ/‚àÇŒª‚Çñ = -H_Œª‚Åª¬π(S‚ÇñŒ≤ÃÇ)
```

In log-scale (œÅ‚Çñ = log Œª‚Çñ):
```
‚àÇŒ≤ÃÇ/‚àÇœÅ‚Çñ = Œª‚Çñ ¬∑ ‚àÇŒ≤ÃÇ/‚àÇŒª‚Çñ = -Œª‚Çñ ¬∑ H_Œª‚Åª¬π(S‚ÇñŒ≤ÃÇ)
```

This is implemented in the custom Zygote adjoint for `fit_inner_coefficients_for_zygote`.

### Optimizations (Performance tuning after correctness verified)

1. **Pre-factor H_Œª**: Compute Cholesky factorization once, reuse for all solves
2. **Parallelize over subjects**: Independent LOO computations
3. **Cache subject derivatives**: Reuse g·µ¢, H·µ¢ across optimizer iterations if Œ≤ changes slowly

---

## Appendix B: Testing Strategy

### Unit Tests (Phase 0)

1. **Finite difference verification** (test only, NOT in production):
   ```julia
   @testset "PIJCV gradient accuracy (Full AD)" begin
       for log_Œª in [[0.0], [2.0], [3.0], [4.0]]
           V, grad_zygote = pijcv_with_gradient_zygote(log_Œª, model, data)
           
           grad_fd = FiniteDiff.finite_difference_gradient(
               œÅ -> pijcv_objective(œÅ, model, data), log_Œª
           )
           
           @test isapprox(grad_zygote, grad_fd, rtol=0.05)  # 5% tolerance
       end
   end
   ```

2. **Zero-crossing agreement**:
   ```julia
   @testset "Optimum location agreement" begin
       # Find where Zygote gradient = 0
       opt_zygote = optimize(œÅ -> pijcv_with_gradient_zygote(œÅ, model, data)...)
       
       # Find where FD gradient = 0
       opt_fd = optimize(œÅ -> pijcv_objective(œÅ, model, data), ...)
       
       # Must agree within 0.1 on log scale
       @test isapprox(opt_zygote, opt_fd, atol=0.1)
   end
   ```

3. **IFT pullback verification**: Verify custom adjoint matches ForwardDiff through solve

### Integration Tests (Phase 1)

1. **Optimization convergence**: PIJCV optimization converges to reasonable Œª
2. **Recovery of true Œª**: On simulated data with known truth, ŒªÃÇ close to Œª*
3. **Comparison with legacy**: Results match legacy within 10%

---

## Current State (Updated 2026-01-28)

### Completed
- [x] AutoFiniteDiff removed from all src/ files  
- [x] 2079 package tests pass
- [x] Core implicit diff infrastructure works (tests 1-4)
- [x] ImplicitDifferentiation.jl correctly computes dŒ≤/dœÅ
- [x] `ImplicitFunction` created via `make_implicit_beta_function()`
- [x] **Verified Zygote CAN diff through ForwardDiff.hessian** (key finding)
- [x] **Option B REJECTED**: ~30% Œª bias due to missing third derivatives
- [x] **Decision**: Proceed with Option A (Zygote + Manual IFT)

### In Progress (Phase 0)
- [ ] Implement `compute_V_zygote()` - Zygote-compatible V computation
- [ ] Implement `compute_subject_derivatives()` - per-subject ‚Ñì·µ¢, g·µ¢, H·µ¢  
- [ ] Implement custom Zygote adjoint for inner optimization (IFT pullback)
- [ ] Verify gradient matches FD within 5%
- [ ] Verify zero-crossings agree within 0.1 on log scale

### Blocked
- Integration tests (section 5) remain commented out pending Full AD implementation

### To Delete (Phase 2)
- `compute_pijcv_with_gradient()` - Option B code with ~30% bias
- Legacy nested optimization paths
- `use_implicit_diff` field

---

## Files to Modify

### Phase 0 (Current - Implement Full AD)

| File | Changes |
|------|---------|
| `src/inference/smoothing_selection/implicit_diff.jl` | Add Zygote-based PIJCV gradient computation |
| `src/inference/smoothing_selection/zygote_pijcv.jl` | **NEW**: Zygote + manual IFT implementation |
| `MultistateModelsTests/unit/test_implicit_diff.jl` | Add Full AD gradient tests |

### Phase 2 (Cleanup - After Validation)

| File | Changes |
|------|---------|
| `src/inference/smoothing_selection/implicit_diff.jl` | **Delete `compute_pijcv_with_gradient()` (Option B)** |
| `src/inference/smoothing_selection/dispatch_general.jl` | Delete legacy `_nested_optimization_pijcv` |
| `src/inference/smoothing_selection/dispatch_exact.jl` | Simplify to always use Full AD |
| `src/inference/smoothing_selection/dispatch_markov.jl` | Delete legacy, simplify |
| `src/inference/smoothing_selection/dispatch_mcem.jl` | Delete legacy, simplify |
| `src/types/penalties.jl` | Remove `use_implicit_diff` field |
| `MultistateModelsTests/unit/test_implicit_diff.jl` | Uncomment section 5, add tests |

---

## Verification Checklist

- [ ] Pkg.test() passes all tests
- [ ] **Zygote gradient matches FD within 5%** at log_Œª ‚àà {-1, 0, 1, 2, 3, 4, 5}
- [ ] **Zero-crossings agree within 0.1** on log scale (CRITICAL)
- [ ] Full AD PIJCV Œª within 10% of legacy Œª  
- [ ] Performance acceptable (<5x legacy runtime)
- [ ] No AutoFiniteDiff in src/
- [ ] No use_implicit_diff parameter (after Phase 2)
- [ ] Option B code deleted (after Phase 2)
- [ ] Legacy functions deleted (after Phase 2)

---

## Risk Assessment

| Risk | Severity | Mitigation |
|------|----------|------------|
| Zygote + ForwardDiff integration issues | **Medium** | Verified working in isolation; test incrementally |
| Custom IFT pullback bugs | **Medium** | Verify against ID.jl's ForwardDiff output |
| Memory for Zygote tape | **Medium** | For p=50, n=500: ~200MB. Chunk if needed. |
| Threading incompatible with Zygote | **Medium** | Use sequential path for AD; parallel for evaluation only |
| Performance worse than Option B | **Low** | Option B has ~30% bias‚Äîcorrectness > speed |

### Verified Non-Issues
- ‚úÖ Zygote CAN diff through ForwardDiff.hessian (tested 2026-01-27)
- ‚úÖ Linear algebra `\` with Dual matrices (tested, works)
- ‚úÖ IFT formula verified correct (matches ID.jl ForwardDiff output)

---

## References

1. **Zygote.jl**: https://fluxml.ai/Zygote.jl/
   - Reverse-mode AD for Julia
   - Can differentiate through ForwardDiff calls

2. **ForwardDiff.jl**: https://juliadiff.org/ForwardDiff.jl/
   - Forward-mode AD via dual numbers
   - Used for per-subject Hessian computation

3. **ImplicitDifferentiation.jl**: https://gdalle.github.io/ImplicitDifferentiation.jl/
   - Reference for IFT implementation
   - We bypass its Zygote extension due to bugs

4. **Wood (2011)**: "Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models" JRSSB

---

## Change Log

- **2026-01-28**: **MAJOR REVISION**: Option B REJECTED due to ~30% Œª bias. Plan now Option A ONLY.
- **2026-01-28**: Discovered Option B gradient has systematic shift causing wrong optimum location
- **2026-01-27**: Verified Zygote can diff through ForwardDiff.hessian
- **2026-01-27**: Adversarial review completed
- **2026-01-27**: Initial plan based on mgcv analysis
