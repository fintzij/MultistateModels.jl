Survival Analysis via Ordinary Diﬀerential Equations
Weijing Tang∗ Kevin He† Gongjun Xu∗ Ji Zhu∗
Abstract
This paper introduces an Ordinary Diﬀerential Equation (ODE) notion for survival
analysis. The ODE notion not only provides a uniﬁed modeling framework, but more
importantly, also enables the development of a widely applicable, scalable, and easy-
to-implement procedure for estimation and inference. Speciﬁcally, the ODE modeling
framework uniﬁes many existing survival models, such as the proportional hazards model,
the linear transformation model, the accelerated failure time model, and the time-varying
coeﬃcient model as special cases. The generality of the proposed framework serves as
the foundation of a widely applicable estimation procedure. As an illustrative example,
we develop a sieve maximum likelihood estimator for a general semi-parametric class of
ODE models. In comparison to existing estimation methods, the proposed procedure has
advantages in terms of computational scalability and numerical stability. Moreover, to ad-
dress unique theoretical challenges induced by the ODE notion, we establish a new general
sieve M-theorem for bundled parameters and show that the proposed sieve estimator is
consistent and asymptotically normal, and achieves the semi-parametric eﬃciency bound.
The ﬁnite sample performance of the proposed estimator is examined in simulation studies
and a real-world data example.
Keywords— survival analysis, ordinary diﬀerential equation, linear transformation model, time
varying eﬀects, sieve maximum likelihood estimator, semi-parametric eﬃciency.
1 Introduction
Survival analysis is an important branch of statistical modeling, where the primary outcome of interest
is the time to a certain event. In practice, event times may not be observed due to a limited observation
time window or missing follow-up during the study, which is referred to as censored data. Many
statistical models have been developed to deal with censored data in the literature. For example,
the Cox proportional hazard model is probably the most classical semi-parametric model for handling
∗Department of Statistics, University of Michigan, Ann Arbor, Michigan.
†Department of Biostatistics, School of Public Health, University of Michigan, Ann Arbor, Michigan.
1
arXiv:2009.03449v2  [stat.ME]  5 Dec 2021
censored data (Cox, 1975), and it assumes that the covariates have a constant multiplicative eﬀect
on the hazard function. Although easy to interpret, the constant hazard ratio assumption is often
considered as overly strong for real-world applications. As a result, many other semi-parametric
models have been proposed as attractive alternatives, such as accelerated failure time (AFT) models,
transformation models, and additive hazards models. See Aalen (1980), Buckley and James (1979),
Gray (1994), Bennett (1983), Cheng et al. (1995), Fine et al. (1998), and Chen et al. (2002) for a
sample of references. Given diﬀerent assumptions made in these semi-parametric models, diﬀerent
estimation and inference procedures have also been developed accordingly, such as maximum partial
likelihood based estimators (MPLE) (Zucker and Karr, 1990; Gray, 1994; Bagdonavicius and Nikulin,
2001; Chen et al., 2002), least square and rank-based methods (Buckley and James, 1979; Lai and
Ying, 1991; Tsiatis, 1990; Jin et al., 2003, 2006), non-parametric maximum likelihood estimators
(NPMLE) (Murphy et al., 1997; Zeng and Lin, 2007b), and sieve maximum likelihood estimators
(MLE) (Huang, 1999; Shen and Wong, 1994; Ding and Nan, 2011; Zhao et al., 2017).
In this paper, we introduce a novel Ordinary Diﬀerential Equation (ODE) notion and show that
it provides a uniﬁed view of aforementioned survival models and, more importantly, facilitates the
development of a scalable and easy-to-implement estimation and inference procedure, which can be
applied to a wide range of ODE survival models. We note that the proposed approach is founded
upon well-established numerical solvers and sensitivity analysis tools for ODEs, and it overcomes
various practical limitations of existing estimation methods when applied to diﬀerent survival models
for large-scale studies.
Speciﬁcally, the proposed framework models the dynamic change of the cumulative hazard function
through an ODE. Let T be the event time and X be covariates. Denote the conditional cumulative
hazard function of T given X = xas Λx(t). Then Λ x(t) is characterized by the following ODE with a
ﬁxed initial value
{
Λ′
x(t) = f(t,Λx(t),x)
Λx(t0) = c(x)
, (1)
where the derivative is with respect tot, f(·) and c(·) are functions to be speciﬁed, andt0 is a predeﬁned
initial time point. In particular, function c(·) determines the probability of an event occurring after
t0; for instance, Λ x(0) = 0 corresponds to the case when no event occurs before time 0. Further,
function f(·) determines how covariates x aﬀect the hazard function at time t given an individual’s
own cumulative hazard. Thus, diﬀerent speciﬁcations of the function f(·) lead to diﬀerent ODE
models.
Next, we comment on both beneﬁts of the ODE approach in terms of modeling and computation
and new theoretical challenges induced by the ODE notion.
• Firstly, the ODE modeling framework is general enough to unify many aforementioned existing
survival models through diﬀerent speciﬁcations of the function f(·), which serves as the foun-
dation of a widely applicable estimation procedure that will be developed later. For example,
the ODE (1) is equivalent to the Cox model when f(·) takes the form α(t) exp
(
xTβ
)
for some
2
function α(·), and it is equivalent to the AFT model whenf(·) takes the form q(Λx(t)) exp
(
xTβ
)
for some function q(·). Similarly, we can obtain many more models such as the time-varying
variants of the Cox model, the linear transformation model, and the additive hazards model
to name a few (see Section 2 for details). We note that the ODE notion can provide new and
sometimes more explicit interpretations in terms of the hazard by re-writing the existing models
in the ODE form. In addition, the generality of the proposed framework oﬀers an opportunity
for designing more ﬂexible model structures and model diagnostics.
• Secondly, and also more importantly, introducing the ODE notion facilitates the development
of a general and easy-to-implement procedure for estimation and inference in large-scale sur-
vival analysis. In this paper, we illustrate the proposed procedure by using a general class of
ODE models as an example. In particular, this general class includes the most ﬂexible linear
transformation model, where both the transformation function and the error distribution are
unspeciﬁed. Since the f(·) function for the general model contains both ﬁnite-dimensional and
inﬁnite-dimensional parameters, we propose a spline-based sieve MLE that directly maximizes
the likelihood in a sieve space. We provide an easy-to-implement gradient-based optimization
algorithm founded upon local sensitivity analysis tools for ODEs (Dickinson and Gelinas, 1976),
where numerical ODE solvers are used to compute the log-likelihood function and its gradients.
Since eﬃcient implementations of both ODE solvers and splines are available in many software,
the resultant algorithm is easy to carry out in practice. It is worth noting that, in comparison
to existing estimation methods, the proposed procedure has advantages in various aspects, such
as scalability against MPLE for the time-varying Cox model, optimization-parameter eﬃciency
against NPMLE, statistical eﬃciency and numerical stability against rank-based methods for
the linear transformation model. We demonstrate these advantages through extensive simula-
tion studies. For example, when the sample size is 8 ,000, it takes the proposed ODE approach
about 6 seconds to estimate the semi-parametric ODE-AFT model while the rank-based method
needs 350 seconds.
• Finally, we note that the ODE notion brings new challenges to asymptotic distributional theory.
While many asymptotic distributional theories for M-estimation in semi-parametric models have
been developed (see Huang (1999), Shen (1997), Ai and Chen (2003), Wellner and Zhang (2007),
Zhang et al. (2010), He et al. (2010), Ding and Nan (2011) for a sample of references), they
cannot be directly applied to our setting. Among them, the proposed theory in Ding and Nan
(2011) considers bundled parameters where the inﬁnite-dimensional parameter is an unknown
function of the ﬁnite-dimensional Euclidean parameter and has been applied to the AFT model,
and recently, to the accelerated hazards model in Zhao et al. (2017). However, for the general
class of ODE models, the estimation criterion is parameterized with more general bundled
parameters where the nuisance parameter is an unknown function of not only ﬁnite-dimensional
regression parameters of interest but also other inﬁnite-dimensional nuisance parameters. To
accommodate this diﬀerent and challenging scenario induced by the ODE notion, we develop a
3
new sieve M-theorem for more general bundled parameters. By applying it to the general class
of ODE models along with ODE related methodologies (Walter, 1998), we show consistency,
asymptotic normality, and semi-parametric eﬃciency for the estimated regression parameters.
The proposed theory can also be extended to develop the asymptotic normality of estimators
for other ODE models.
The rest of the paper is organized as follows. We introduce the ODE framework and present a
general class of ODE models as special cases in Section 2. We provide the estimation procedure in
Section 3 and establish theoretical properties in Section 4. Simulation studies and a real-world data
example are presented in Sections 5 and 6 respectively.
2 The ODE Framework
To characterize the conditional distribution of T given X, the conditional hazard function, denoted as
λx(t) = Λ′
x(t), provides a popular modeling target as it describes the instantaneous rate at which the
event occurs given survival. In this paper, we view the hazard function as the dynamic change of the
cumulative hazard function and quantify them using an ODE.
In our ODE framework, the hazard function depends not only on the time and covariates but also
on the cumulative hazard as shown in (1), where function f(·) speciﬁes the dynamic change of Λ x(t)
and covariates x serve as additional parameters in terms of the ODE. The initial value in (1) implies
that, for an individual with covariates x, the probability for an event to occur after t0 is controlled by
exp(−c(x)). For example, it is often the case that time 0 is deﬁned prior to the occurrence of events,
which implies that an event always occurs after time 0, i.e. the survival function Sx(0) = 1, and it
follows that Λx(0) = 0. We use this initial value in the ODE framework hereafter for simplicity, while
the estimation method and the theoretical properties established later can be extended to the general
case where c(x) can be a function of covariates. Under certain smoothness conditions (Walter, 1998,
page 108), the initial value problem (1) has exactly one solution, which uniquely characterizes the
conditional distribution of the event time.
Next, we present a general class of ODE models as an instantiation of the ODE framework. Suppose
there are two groups of covariates denoted by X ∈Rd1 and Z ∈Rd2 respectively. We consider ODE
models in the form of
Λ′
x,z(t) = α(t) exp
(
xTβ+ zTη(t)
)
q(Λx,z(t)), (2)
where α(·) and q(·) are two unknown positive functions, and given an individual’s own cumulative haz-
ard, both covariatesxand zhave multiplicative eﬀects on the hazard, one with time-independent coeﬃ-
cients β ∈Rd1 and the other with time-varying coeﬃcientsη(t) ∈Rd2 . Here η(·) = (η1(·),...,η d2 (·))T.1
We note that this general class of ODE models is a speciﬁc example; other examples beyond this class
are included in Remark 2 to further illustrate the ﬂexibility of the proposed ODE framework. In
particular, this general class covers many existing models as special cases. As shown below, model (2)
1Throughout this paper, we bold vectors only when each element is a function.
4
reduces to the time-varying Cox model when q(·) = 1, to the linear transformation model when co-
variates z are not considered, and further reduces to the AFT model if α(·) = 1. In the following
subsections, we will also show that by rewriting many existing models under the format (1), the ODE
framework brings them new interpretations in terms of the hazard function.
2.1 Cox model and time-varying Cox model
The Cox proportional hazard model assumes that the covariates have a multiplicative eﬀect on the
hazard function, i.e. λx(t) = α(t) exp
(
xTβ
)
, where α(t) is a baseline hazard function and exp
(
xTβ
)
is the relative risk, and extensions of the Cox model allow for time-varying coeﬃcients (Zucker and
Karr, 1990; Gray, 1994). Here we write the Cox model with both time-independent and time-varying
eﬀects as a simple ODE, whose right-hand side does not depend on the cumulative function, i.e.
Λ′
x,z(t) = α(t) exp
(
xTβ+ zTη(t)
)
, (3)
which allows covariates x to have time-independent eﬀects and covariates z to have time-varying
eﬀects on the hazard function. The baseline hazard function α(t) and time-varying eﬀects η(t) can be
speciﬁed in a parametric model or left unspeciﬁed in a semi-parametric model.
2.2 Accelerated failure time model
The AFT model assumes that the log transformation of T is linearly correlated with covariates, i.e.
log T = −XTβ+ ϵ. In the proposed ODE framework, the AFT model can be written as
Λ′
x(t) = q(Λx(t)) exp
(
xTβ
)
, (4)
where the function q(·) uniquely determines the distribution of error ϵ in the following way. Let
Hq(u) =
∫−ln u
0 q−1(v)dv and Gq(u) = H−1
q (u), then Gq is the survival function of δ = exp( ϵ) as
shown in Bagdonavicius and Nikulin (2001). For example, if q(t) = vk
1
vt1−1
v, then δ follows a Weibull
distribution with Gq(t) = exp(−ktv). When the error distribution is unknown (as in a semi-parametric
AFT model), we can leave the function q(·) unspeciﬁed.
The ODE (4) provides a new and clear interpretation on how covariates aﬀect the hazard for the
AFT model. Speciﬁcally, it implies that given an individual’s own cumulative hazard, covariates x
have a multiplicative constant eﬀect on the hazard function. Further, besides the direct eﬀects of
covariates, if q(·) is a monotonic increasing function, then an individual with a higher cumulative
hazard at a particular time would have a higher “baseline” hazard. Note that although we can also
present the hazard directly as a function of covariates and time, i.e. λx(t) = λδ(texp
(
xTβ
)
) exp
(
xTβ
)
,
the covariate eﬀects are entangled with the baseline hazard λδ in this representation, which is more
diﬃcult to interpret.
5
2.3 Linear transformation model
As an extension of the AFT model, the linear transformation model assumes that, after a monotonic
increasing transformation ϕ(·), the event time T is linearly correlated with covariates, i.e. ϕ(T) =
−XTβ+ ϵ. In the proposed ODE framework, it can be written as
Λ′
x(t) = q(Λx(t)) exp
(
xTβ
)
α(t), (5)
where q(·) corresponds to the distribution of ϵ in the same way as in the AFT model, and α(·) is
uniquely determined by the equation ϕ(t) = log
∫t
0 α(s)ds. In comparison to model (4), the hazard
function at time tdepends not only on the current cumulative hazard and covariates, but also on the
current time t directly.
Diﬀerent speciﬁcations of ϕ(·) and ϵ have been proposed in the literature for the linear trans-
formation model. We consider the case where both the transformation and the error distribution
are unknown. This speciﬁcation is especially preferred when parametric assumptions on the trans-
formation function or the error distribution cannot be properly justiﬁed. However, when both q(·)
and α(·) are unknown, they may not be identiﬁable. The equivalent linear regression representation,
ϕ(T) = −xTβ + ϵ, allows us to see the identiﬁability issue clearly. Note that, when no covariate
is associated with survival, i.e., β = 0, non-identiﬁability issue arises because parameters ( ϕ,ϵ) and
(f(ϕ),f(ϵ)) give the same event time distribution for any arbitrary function f. Therefore, we consider
β ̸= 0, in which case Horowitz (1996) showed that the model parameters are identiﬁable up to a
scale and a location normalization under certain regularity conditions. Following that result, we have
developed Proposition 1 that characterizes the identiﬁability of parameters in (5), while Proposition
2 provides necessary and suﬃcient degeneration conditions for AFT and Cox models. The proofs are
given in the Supplemental Material.
Proposition 1.Suppose at least one of the covariates in xis continuous and this covariate has a non-
zero β coeﬃcient, which without loss of generality is assumed to be positive. Let (q(·),β,α (·)) specify
the survival distribution through (5). Then for any other (˜q(·),˜β,˜α(·)) that gives the same survival
distribution, if and only if there exist positive constants c1 and c2 such that ˜β = c1β,
∫t
0 ˜α(s)ds =
c2(
∫t
0 α(s)ds)c1 , and
∫t
0 ˜q−1(s)ds= c2(
∫t
0 q−1(s)ds)c1 for any t> 0.
Proposition 2. Suppose the conditions in Proposition 1 hold, then the linear transformation model
in (5) coincides with the Cox model if and only if there exist positive constants c1 and c2 such that
q(u) = c2u1−c1 , and it coincides with the AFT model if and only if there exist positive constants c1
and c2 such that α(t) = c2tc1−1 for t> 0.
Remark 1. Note that the original forms of the AFT model and the linear transformation model do
not directly take time-varying coeﬃcients. Existing works on the linear transformation model that
consider varying coeﬃcients choose to model them as a function of certain covariates rather than
a function of time (Chen and Tong, 2010; Qiu and Zhou, 2015). In contrast, the equivalent ODE
forms of the AFT model in (4) and the linear transformation model in (5) can naturally accommodate
6
time-varying coeﬃcients. For example, we can consider the generalization in (2), where given an
individual’s own cumulative hazard covariates z have time-varying multiplicative eﬀects η(t) on the
hazard. In particular, this generalization is equivalent to a covariate-dependent transformation model
ϕZ(T) = −XTβ+ ϵ,
where ϕz(t) = log
∫t
0 α(s) exp
(
z⊤η(s)
)
ds, i.e., covariates z have multiplicative time-varying eﬀect η(t)
on the gradient of exp(ϕz(t)).
Remark 2. The proposed ODE framework is general enough to cover other existing models as well.
For example, both the additive hazard model (Aalen, 1980; Mckeague and Sasieni, 1994) and the
additive-multiplicative hazard model (Lin and Ying, 1995) can be viewed as a speciﬁc ODE model, i.e.
Λ′
x,z(t) = r1(xTβ) + α(t)r2(zTη), where r1(·) and r2(·) are some known link functions. Subsequently,
the generalized additive hazards model and the generalized additive-multiplicative hazards model (Bag-
donavicius and Nikulin, 2001) can be written as Λ′
x(t) = q(Λx(t))(r1(x) + α(t)r2(x)). The generalized
Sedyakin’s model (Bagdonavicius and Nikulin, 2001), which was proposed as an extension of the AFT
model, can also be viewed as a special case of (1) with Λ′
x(t) = f(Λx(t),x).
Remark 3. Further, the proposed ODE framework and the estimation method in Section 3 can
also be extended to deal with time-varying covariates. Suppose the covariate is a stochastic pro-
cess X(t),t ≥ 0 and TX(·) is the failure time under X(·). Denote the conditional survival, the
hazard function, and the cumulative function by Sx(·)(t) = P{TX(·) ≥t|X(s) = x(s),0 ≤s ≤t},
λx(·)(t) = −
S′
x(·)(t)
Sx(·)(t) , and Λx(·)(t) = −log
(
Sx(·)(t)
)
, respectively. Then the ODE (1) can be extended to
Λ′
x(·)(t) = f(t,Λx(·)(t),x(t)). This extension also covers many existing models as special cases. For
example, the linear transformation model with time-varying covariates (Zeng and Lin, 2006) can be
written as Λ′
x(·)(t) = q(Λx(·)(t)) exp
(
x(t)Tβ
)
α(t), and the Cox model with time-varying covariates can
be viewed as a special case with q(·) ≡1. For presentation simplicity, we focus on models in the form
of (2) in this paper.
2.4 Related estimation methods and their limitations
The maximum partial likelihood estimator (MPLE) (Cox, 1975) was ﬁrst proposed for the Cox model,
and the asymptotic property of MPLE was established by Andersen and Gill (1982) via the counting
process martingale theory. For time-varying Cox models, many diﬀerent estimation methods have been
developed while relying on maximizing the partial likelihood (Zucker and Karr, 1990; Gray, 1994).
However, evaluating the partial likelihood for an uncensored individual requires access to all other
observations who were in its risk set. This prevents parallel computing for partial likelihood-based
methods, which is a drawback when analyzing large scale data.
For the linear transformation model, diﬀerent speciﬁcations of the transformation and the error
distribution along with diﬀerent estimation methods have been proposed. For example, Cheng et al.
(1995), Fine et al. (1998), Shen (1998), Chen et al. (2002), and Bagdonavicius and Nikulin (1999)
7
have considered an unknown transformation with a known error distribution, which includes the
Cox model and the proportional odds model (Bennett, 1983) as special cases. The corresponding
modiﬁed MPLE (Chen et al., 2002; Bagdonavicius and Nikulin, 1999), sieve MLE (Shen, 1998), and
NPMLE (Murphy et al., 1997; Zeng and Lin, 2007b) have also been developed. However, due to the
large number of nuisance parameters, it is diﬃcult to obtain NPMLE in practice, especially in large-
scale applications. Alternatively, Cai et al. (2005) considered a parametric Box-Cox transformation
with an unknown error distribution, which includes the semi-parametric AFT model as a special case,
and least square and rank-based methods have been proposed to estimate the regression parameters
(Buckley and James, 1979; Lai and Ying, 1991; Tsiatis, 1990; Jin et al., 2003, 2006). Nevertheless,
they are not asymptotically eﬃcient and may suﬀer additional numerical errors resulting from discrete
objective functions. Subsequently, under the AFT model, Zeng and Lin (2007a) and Lin and Chen
(2012) proposed eﬃcient estimators based on a kernel-smoothed proﬁle likelihood, and Ding and
Nan (2011) developed an eﬃcient sieve MLE. When both the transformation function and the error
distribution are unknown, a partial rank-based method has been proposed (Khan and Tamer, 2007;
Song et al., 2006), and its computation is analogous to that of the partial likelihood, where the
rank of an uncensored individual is determined by all other individuals in its risk set, and thus the
computational challenge for large-scale applications still remains.
As evident from the above discussion, many existing estimation methods suﬀer from important
limitations in practice. In Section 3, we propose a scalable, easy-to-implement and eﬃcient estimation
method that can be applied to a wide range of models.
3 Maximum Likelihood Estimation
In this section, we propose a general estimation procedure that can be applied to a wide range of ODE
models. Here we use the ODE model in (2) as an illustrative example, and the proposed estimation
method can also be applied to other models such as those mentioned in Remark 2.
We denote the event time as T, the censoring time as C. Let Y = min{T,C}and ∆ = 1 (T ≤C),
where 1 (·) denotes the indicator function. Our data consist ofnindependent and identically distributed
observations {Yi,∆i,Xi,Zi}, i= 1,...,n . Since α(·) and q(·) in (2) are positive, we set γ(·) = log α(·)
and g(·) = log q(·). Under the conditional independence between T and C given covariates (X,Z), the
log-likelihood function of the parameters ( β,γ(·),η(·),g(·)) is given by
ln(β,γ(·),g(·),η(·)) = 1
n
n∑
i=1
[∆i{γ(Yi) + XT
i β+ ZT
i η(Yi) + g(Λi(Yi; β,γ,g, η))} (6)
−Λi(Yi; β,γ, η,g)],
where Λi(t; β,γ, η,g) denotes the solution of ODE (2) parameterized by ( β,γ, η,g) given covariates
X = Xi and Z = Zi. The log-likelihood function (6) includes both ﬁnite-dimensional parameter β
and inﬁnite-dimensional parameters γ,η,g.
8
We propose a sieve MLE that maximizes the log-likelihood over a sequence of ﬁnite-dimensional
parameter spaces that are dense in the original parameter space as the sample size increases. The sieve
space can be chosen as linear spans of many types of basis functions with desired properties (Chen,
2007). In particular, we construct the sieve space using polynomial splines due to their capacity in
approximating complex functions and the simplicity of their construction. Under suitable smoothness
conditions, γ0(·), η0(·), and g0(·), the true parameters associated with the data generating distribution,
can be well approximated by some functions in the space of polynomial splines as deﬁned in Schumaker
(2007, page 108, Deﬁnition 4.1). Further, there exists a group of spline bases such that functions in
the space of polynomial splines can be written as linear combinations of the spline bases (Schumaker,
2007, page 117, Corollary 4.10). Diﬀerent groups of spline bases may be used for the estimation of
diﬀerent parameters (γ,η) and g because of their diﬀerent domains.
Speciﬁcally, we construct the proposed sieve estimator as follows. Let B⊂ Rd1 be the parameter
space of β. Let {B1
j,1 ≤j ≤q1
n}and {B2
j,1 ≤j ≤q2
n}be two groups of spline bases that are used for
the estimation of parameters ( γ,η) and g respectively. Here the number of spline bases, qi
n, should
grow sublinearly in rate O(nvi) for some vi ∈(0,0.5), i= 1,2 for convergence guarantee (see Section 4
for rigorous deﬁnitions). Overall, we wish to ﬁnd d2 + 1 members ( γ,η1,··· ,ηd2 ) from the space of
polynomial splines associated with {B1
j}, one member g from that associated with {B2
j}, along with
β ∈B to maximize the log-likelihood function (6). Let Zi0 = 1, Zi = ( Zi1,··· ,Zid2 )T. Then the
objective function can be written as
ln(β,a,b ) = 1
n
n∑
i=1

∆i{XT
i β+
d2∑
l=0
q1
n∑
j=1
al
jB1
j(Yi)Zil +
q2
n∑
j=1
bjB2
j(Λi(Yi; β,a,b ))}−Λi(Yi; β,a,b )

, (7)
where a =
(
al
j
)
j=1,···,q1n,l=0,···,d2
and b = ( bj)j=1,···,q2n
are the coeﬃcients of the spline bases, and
Λi(t; β,a,b ) is the solution of



Λ′
i(t) = exp
(
XT
i β+ ∑d2
l=0
∑q1
n
j=1 al
jB1
j(t)Zil + ∑q2
n
j=1 bjB2
j(Λi(t))
)
,
Λi(0) = 0.
(8)
The proposed sieve estimators are given by ˆβn = ˆβ, ˆηn(·) =
(∑q1
n
j=1 ˆa1
jB1
j(·),..., ∑q1
n
j=1 ˆad2
j B1
j(·)
)
,
ˆγn(·) = ∑q1
n
j=1 ˆa0
jB1
j(·), and ˆgn(·) = ∑q2
n
j=1 ˆbjB2
j(·), where ( ˆβ,ˆa,ˆb) maximizes the objective function (7).
Note that the objective function (7) contains the solution of a parameterized ODE (i.e. (8)),
and this is diﬀerent from most traditional optimization problems. In particular, it is nontrivial to
evaluate the objective function and its gradient with respect to parameters when there is no closed-form
solution for the ODE. To address this optimization challenge, we develop a gradient-based optimization
algorithm by taking advantage of local sensitivity analysis (Dickinson and Gelinas, 1976; Petzold et al.,
2006) and well-implemented ODE solvers. Speciﬁcally, we evaluate the objective function and its
gradient as follows:
1. we numerically calculate Λ i(Yi; β,a,b ) by solving (8) given the current parameter estimates β,
9
a, b and covariates Xi, Zi, the initial value at t0 = 0, and the evaluating time t= Yi;
2. we evaluate the derivative of Λ i(Yi; β,a,b ) with respect to the parameters β, a, and b through
solving another ODE which is derived by local sensitivity analysis, and calculate the gradient
of the objective function by the chain rule.
We summarize the results of the local sensitivity analysis in the following, and provide detailed
derivations in the Supplemental Material. The local sensitivity analysis is a technique that studies the
rate of change in the solution of an ODE system with respect to the parameters. There are two ways
to obtain the sensitivity: forward sensitivity analysis and adjoint sensitivity analysis. Both of them
require solving another ODE with some ﬁxed initial value. For example, we consider to compute the
gradient of Λ(y; θ) with respect to its parameter θ, where Λ( t; θ) is the solution of (8) and θ consists
of parameters β, a, and b in our case. For presentation simplicity, we denote the right-hand side of
(8) by the function f(t,Λ; θ), i.e.
f(t,Λ; θ) = exp

XTβ+
d2∑
l=0
q1
n∑
j=1
al
jB1
j(t)Zj +
q2
n∑
j=1
bjB2
j(Λ)

,
and its partial derivative with respect to θ and Λ by f′
θ and f′
Λ respectively. In forward sensitivity
analysis, it can be shown that the partial derivative of Λ(y; θ) with respect to θis given by the solution
of (9) at t= y, i.e. Λ ′
θ(y; θ) = F1(y) with F1 satisfying
{
F′
1(t) = f′
θ(t,Λ; θ) + f′
Λ(t,Λ; θ)F1,
F1(0) = 0.
(9)
In the alternative adjoint sensitivity analysis, we can show that the partial derivative can also be
obtained by evaluating the solution of (10) at t= 0, i.e. Λ ′
θ(y; θ) = F2(0) with F2 satisfying
{
(κ(t); F′
2(t)) = (−κ·f′
Λ(t,Λ; θ); −κ·f′
θ(t,Λ; θ)),
(κ(t); F2(t))|t=y = (1; 0).
(10)
Thus, after plugging the form of f(t,Λ; θ) into either (9) or (10), we can obtain the gradients through
solving the corresponding ODE. In Remark 4, we compare the computational complexity of forward
and adjoint sensitivity analyses and provide a general guidance on which sensitivity analysis to use
when computing gradients under survival ODE models.
It is worth noting that the proposed estimation method can be easily implemented using existing
computing packages. For example, the “Optimization Toolbox” in MATLAB contains “fminunc” for
unconstrained optimization and “fmincon” for constrained optimization; both require initialization
and the objective function. In our implementation, we also provide evaluation of the gradient for
faster and more reliable computations. In particular, we compute both the objective function and the
gradient by well-implemented ODE solvers in MATLAB. In addition, we construct the sieve space
using B-splines for its numerical simplicity, whose implementation is available in the “Curve Fitting
10
Toolbox”.
Remark 4. In general, forward sensitivity analysis is computationally more eﬃcient when the di-
mension of the ODE system is relatively large and the number of parameters is small, while adjoint
sensitivity analysis is best suited in the complementary scenario. See Dickinson and Gelinas (1976)
and Petzold et al. (2006) for more details. For a general ODE model such as (1) where the size of the
ODE system is 1 and the number of parameters increases as the sample size n grows, we can use the
adjoint sensitivity analysis along with parallel computing for n independent individuals. Alternatively,
if the memory permits, we can combine ODEs for n individuals into a large ODE system with n di-
mensions, which is larger than the number of parameters, and then the forward sensitivity analysis is
preferred.
Remark 5. Moreover, we introduce a computational trick for the general class of ODE models in
(2) that can signiﬁcantly accelerate the evaluation of the objective and gradients, where we need to
solve ODEs for n independent individuals. Speciﬁcally, the trick transforms the problem of solving n
diﬀerent ODEs at their respective observed times into a problem of solving a single ODE at ndiﬀerent
time points. More generally, this trick can be applied to any ODE model where the right-hand side is
separable in the way that f(t,Λx; θ,x) = f1(t; θ,x)f2(Λx; θ) with two functions f1 and f2. We refer to
the Supplemental Material for more details about this computational trick.
Remark 6. The proposed sieve MLE can also be applied to many existing models. For example, for
the time-varying Cox model where q(·) = 1, we can remove the function g(·) from the objective func-
tion (6). For the semi-parametric AFT model where Z is not considered and α(·) = 1, we can just keep
parameters β and g(·) in (6). For the linear transformation model, if either q(·) or α(·) is speciﬁed, we
can replace the corresponding term in (6) with the speciﬁed ﬁnite-dimensional parametric form. Also
note that in comparison to existing estimation methods in Section 2.4, the proposed estimation method
allows parallel computing, which is especially important for large-scale applications. Speciﬁcally, since
the log-likelihood of each individual only depends on its own observations, the evaluation for indepen-
dent data points can be carried out simultaneously. Further, compared with the NPMLE where the
number of optimization parameters is linear in n (Murphy et al., 1997; Zeng and Lin, 2007b), the
number of optimization parameters used in sieve MLE increases more slowly with the sample size.
Remark 7. The objective function (7) is convex with respect to β and a for the (time-varying) Cox
model, where the parameter b is not included, and the global optimum can be achieved quickly. For the
general case, the objective function is nonconvex and the optimization algorithm may converge to a local
optimum. Nevertheless, based on our extensive simulation studies, the algorithm generally performs
well with appropriately chosen initialization, such as initializing the algorithm with the estimates from
the Cox model.
Remark 8. Note that diﬀerent identiﬁability conditions are required for diﬀerent survival models.
Thus, we need to add corresponding constraints in the optimization algorithm.
11
• For the general ODE model (2) where both covariates X (with time-independent eﬀects) and
Z (with at least one non-zero time-varying eﬀect) are considered, two groups of parameters
(β,γ,g, η) and ( ˜β,˜γ,˜g, ˜η) give the same survival distribution if and only if β = ˜β, γ = ˜γ+ c,
g = ˜g−c, and η = ˜η for some constant c. To guarantee the identiﬁability, we can constrain
either the value of γ(·) at a ﬁxed time point t∗or the norm of γ(·), in which the former leads to
a linear constraint on the coeﬃcients of spline bases.
• For the linear transformation model where the time-varying eﬀects are not considered and at least
one component of X has a non-zero coeﬃcient, parameters (β,γ,g ) are identiﬁable up to two
scaling factors as shown in Proposition 1. To guarantee identiﬁability, we can put constraints
on β and γ. For β, we can either constrain the ﬁrst element of β to be 1 (Khan and Tamer,
2007; Song et al., 2006), which can be naturally achieved by arranging covariates if we know
which covariate has a non-zero eﬀect, or set ∥β∥= 1. For γ, we can add a similar constraint as
that for the general ODE model (2). Alternatively, we can put constraints on γ and g by setting∫t∗
0 exp(γ(s))ds= c1 and
∫t∗
0 exp(−g(s))ds= c2, with some positive constants c1 ̸= c2 >0 and
a ﬁxed time point t∗. In our implementation, we choose to use two linear constraints, i.e. set
the ﬁrst element of β to 1 and γ(t∗) = 0 for simplicity in optimization.
4 Theoretical Properties
In this section, we study the theoretical properties of the proposed sieve MLE. Although many works
have investigated asymptotic distributional theories for M-estimation with bundled parameters (Ai
and Chen, 2003; Chen et al., 2003; Ding and Nan, 2011), their results cannot be directly applied
to our setting. In particular, the nuisance parameters in existing works often take the form of an
unknown function of only some ﬁnite-dimensional Euclidean parameters of interest. However, our
work focuses on a more general scenario, where the nuisance parameter is an unknown function of
not only the Euclidean parameters but also some other inﬁnite-dimensional nuisance parameters. To
deal with theoretical challenges due to the additional functional nuisance parameters, we develop a
new sieve M-theorem for the asymptotic theory of a general family of semi-parametric M-estimators.
Moreover, we apply the proposed general theorem to establish the asymptotic normality and semi-
parametric eﬃciency of the proposed sieve MLE ˆβn when the convergence rate of the sieve estimator of
the nuisance parameter can be slower than √n. We present regularity conditions and main theorems
in this section and give all the proofs in the Supplemental Material.
For the simplicity of notation, we focus on model (2) without covariates Z, i.e. the linear trans-
formation model (5), and the results can be similarly extended to the general case with additional
regularity conditions on Z (see Remark 11). Recall that we have set γ(·) = log α(·) and g(·) = log q(·)
to ensure the positivity of α(·) and q(·) in (5). Then we reformulate the ODE model as follows,
{
Λ′(t) = exp
(
xTβ+ γ(t) + g(Λ(t))
)
Λ(0) = 0
. (11)
12
Note that the parameter β is identiﬁable when time-varying eﬀects are considered, but in (11) it is
identiﬁable only up to a scaling factor when both γ and g are unknown as shown in Proposition 1.
To guarantee the identiﬁability, we constrain the ﬁrst element of β to be 1 and γ(t∗) = c with some
constant c for simplicity in optimization. Speciﬁcally, denote X = (X(1),X(−1)), β = (1,¯βT)T, ¯γ(·) =
γ(·) −γ(t∗) with ¯γ(t∗) ≡0, and ¯X(1) = X(1) + γ(t∗), then we have XTβ+ γ(t) = ¯X(1) + XT
(−1)
¯β+ ¯γ(t).
We substitute ¯β, ¯γ, and ¯X(1) by β, γ, and X(1) respectively for notational simplicity hereafter, and
the ODE (11) is then equivalent to



Λ′(t) = exp
(
x(1) + xT
(−1)β+ γ(t) + g(Λ(t))
)
Λ(0) = 0
, (12)
with γ(t∗) ≡ 0. Before stating the regularity conditions, we ﬁrst introduce some notations. We
denote the solution of (12) by Λ( t,x,β,γ,g ) to explicitly indicate that the solution of (12) depends
on covariates x and parameters ( β,γ,g ). We denote the true parameters associated with the data
generating distribution by ( β0,γ0,g0) and simplify Λ( t,x,β 0,γ0,g0) as Λ 0(t,x). In addition, some
commonly used notations in the empirical process literature will be used in this section as well. Let
Pf =
∫
f(x)Pr(dx), where Pr is a probability measure, and denote the empirical probability measure
as Pn.
Then we assume the following regularity conditions.
(C1) The true parameter β0 is an interior point of a compact set B⊂ Rd.
(C2) The density of X is bounded below by a constant c> 0 over its domain X, which is a compact
subset of Rd+1, and P(X(−1)XT
(−1)) is nonsingular.
(C3) There exists a truncation time τ <∞such that, for some positive constant δ0, Pr(Y >τ|X) ≥
δ0 almost surely with respect to the probability measure of X. Then there is a constant µ =
supx∈XΛ0(τ,x) ≤− log δ0 such that Λ 0(τ,X) = −log Pr(T > τ|X) ≤µ almost surely with
respect to the probability measure of X.
(C4) Let Sp([a,b]) be the collection of bounded functions f on [a,b] with bounded derivatives f(j),
j = 1,...,k , where the kth derivative f(k) satisﬁes the m-H¨ older continuity condition:
|f(k)(s) −f(k)(t)|≤ L|s−t|m for s,t ∈[a,b],
where k is a positive integer and m∈(0,1] with p= m+ k, and L< ∞is a constant. The true
function γ0(·) belongs to Γ p1 = {γ ∈Sp1 ([0,τ]) : γ(t∗) = 0}with p1 ≥2 and the true function
g0(·) belongs to Sp2 ([0,µ + δ1]) = Gp2 with some positive constant δ1 and p2 ≥3.
(C5) Denote R(t) =
∫t
0 exp(γ0(s))ds, V = X(1) + XT
(−1)β0, and U = eVR(Y). There exists η1 ∈(0,1)
such that for all u∈Rd with ∥u∥= 1,
uTVar(X(−1) |U,V )u≥η1uTP(X(−1)XT
(−1) |U,V )u almost surely.
13
(C6) Let ψ(t,x,β,γ,g ) = x(1) + xT
(−1)β+ γ(t) + g(Λ(t,x,β,γ,g )) and denote its functional deriva-
tives with respect to γ(·) and g(·) along the direction v(·) and w(·) at the true parameter by
ψ′
0γ(t,x)[v] and ψ′
0g(t,x)[w] respectively, whose rigorous deﬁnitions are given by (S19)-(S20) in
the Supplemental Material. For any v(·) ∈Γp1 and w(·) ∈Gp2 , there exists η2 ∈(0,1) such that
(P{ψ′
0γ(Y,X)[v]ψ′
0g(Y,X)[w] |∆ = 1})2 ≤η2P{(ψ′
0γ(Y,X)[v])2 |∆ = 1}P{(ψ′
0g(Y,X)[w])2 |∆ = 1}
almost surely.
Conditions (C1)-(C3) are common regularity assumptions in survival analysis. Condition (C4)
requires p2 ≥3 to control the error rates of the spline approximation for the true functiong0 and its ﬁrst
and second derivatives. Moreover, together withp1 ≥2, (C4) will also be used to verify the assumptions
(A4)-(A6) for the general M-theorem (Theorem 3) when we apply it to derive the asymptotic normality
of the proposed sieve MLE (Theorem 2). A similar condition to (C5) was imposed by Wellner and
Zhang (2007) for the panel count data, by Ding and Nan (2011) for the linear transformation model
with a known transformation, and by Zhao et al. (2017) for the accelerated hazards model. When
the transformation function is known, condition (C5) is equivalent to the assumption C7 in Ding and
Nan (2011) and can be veriﬁed in many applications as shown in Wellner and Zhang (2007). For
the general case where both the transformation function and the error distribution are unspeciﬁed,
condition (C6) is assumed to avoid strong collinearity between ψ′
0γ(Y,X)[v] and ψ′
0g(Y,X)[w].
Note that the parameter g(·) takes Λ(t,x,β,γ,g ) as its argument in (12), which involves the other
parameters β and γ(·). Thus, β, γ(·) and g(·) are bundled parameters. For any g(·) ∈Gp2 , we directly
consider the composite function g(Λ(t,x,β,γ,g )) as a function from T ×X×B× Γp1 to R. And we
deﬁne the collection of functions
Hp2 = {ζ(·,β,γ ) : ζ(t,x,β,γ ) =g(Λ(t,x,β,γ,g )),t ∈[0,τ],x ∈X,β ∈B,γ ∈Γp1 ,
g∈Gp2 such that sup
t∈[0,τ],x∈X
|Λ(t,x,β,γ,g )|≤ µ+ δ1},
with δ1 given in condition (C4). For any ζ(·,β,γ ) ∈Hp2 , we deﬁne its norm as
∥ζ(·,β,γ )∥2 =
[∫
X
∫ τ
0
[ζ(t,x,β,γ )]2dΛ0(t,x)dFX(x)
]1/2
,
where FX(x) is the cumulative distribution function ofX. Denote the parameter θ= (β,γ(·),ζ(·,β,γ ))
and the true parameter θ0 = (β0,γ0(·),ζ0(·,β0,γ0)) with ζ0(t,x,β 0,γ0) = g0(Λ(t,x,β 0,γ0,g0)). Denote
the parameter space by Θ = B×Γp1 ×Hp2 . For any θ1 and θ2 in Θ, we deﬁne the distance
d(θ1,θ2) =
(
∥β1 −β2∥2 + ∥γ1 −γ2∥2
2 + ∥ζ1(·,β1,γ1) −ζ2(·,β2,γ2)∥2
2
)1/2
,
where ∥·∥ is the Euclidean norm and ∥γ∥2 = (
∫τ
0 (γ(t))2dt)1/2 is the L2 norm.
Next, we construct the sieve space as follows. Let 0 = t0 < t1 < ··· < tK1n < tK1n+1 = τ be a
14
partition of [0,τ] with K1
n = O(nν1 ) and max1≤j≤K1n+1 |tj−tj−1|= O(n−ν1 ) for some ν1 ∈(0,0.5). Let
TK1n = {t1,··· ,tK1n}denote the set of partition points and Sn(TK1n,K1
n,p1) be the space of polynomial
splines of order p1 as deﬁned in Schumaker (2007, page 108, Deﬁnition 4.1). Similarly, let TK2n be
a set of partition points of [0 ,µ] with K2
n = O(nν2 ) and max 1≤j≤K2n+1 |tj −tj−1| = O(n−ν2 ) for
some ν2 ∈(0,0.5), and Sn(TK2n,K2
n,p2) be the space of polynomial splines of order p2. According to
Schumaker (2007, page 117, Corollary 4.10), there exist two sets of B-spline bases {B1
j,1 ≤j ≤q1
n}
with q1
n = K1
n + p1 and {B2
j,1 ≤j ≤q2
n}with q2
n = K2
n + p2 such that for any s1 ∈Sn(TK1n,K1
n,p1)
and s2 ∈Sn(TK2n,K2
n,p2), we can write s1(t) = ∑q1
n
j=1 ajB1
j(t) and s2(t) = ∑q2
n
j=1 bjB2
j(t). Let Γ p1
n =
{γ ∈Sn(TK1n,K1
n,p1) : γ(0) = 0}, Gp2
n = Sn(TK2n,K2
n,p2), and
Hp2
n = {ζ(·,β,γ ) : ζ(t,x,β,γ ) = g(Λ(t,x,β,γ,g )),g ∈Gp2
n ,t ∈[0,τ],x ∈X,β ∈B,γ ∈Γp1
n }.
Let Θn = B×Γp1
n ×Hp2
n be the sieve space. It is not diﬃcult to see that Θn ⊂Θn+1 ⊂···⊂ Θ. We con-
sider the sieve estimator ˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)), where ˆζn(t,x, ˆβn,ˆγn) = ˆgn(Λ(t,x, ˆβn,ˆγn,ˆgn)),
that maximizes the log-likelihood (6) (without covariates Z and parameter η) over the sieve space
Θn. The consistency and convergence rate of the sieve MLE ˆθn are then established in the following
theorem.
Theorem 1.(Convergence rate of ˆθn.) Let ν1 and ν2 satisfy the restrictions max{ 1
2(2+p1) , 1
2p1
−ν2
p1
}<
ν1 < 1
2p1
, max{ 1
2(1+p2) , 1
2(p2−1) − 2ν1
p2−1 } < ν2 < 1
2p2
, and 2 min{2ν1,ν2} > max{ν1,ν2}. Suppose
conditions (C1)-(C6) hold, then we have
d(ˆθn,θ0) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }).
Theorem 1 gives the convergence rate of the proposed estimator ˆθn to the true parameter θ0, and
its proof is provided in the Supplemental Material by verifying the conditions in Shen and Wong (1994,
Theorem 1). Note the subscripts 1 and 2 correspond to the space of the spline approximation for two
inﬁnite-dimensional parameters γ and g, respectively. The restrictions on ν1 and ν2 are feasible for p1
and p2 not far away from each other. For example, if p1 = p2 = pand ν1 = ν2 = ν, the restriction on ν
is equivalent to 1
2(1+p) < v <1
2p, and the convergence rate becomes d(ˆθn,θ0) = Op(n−min{pν,1−ν
2 }),
which is the same as the case when there is only one inﬁnite-dimensional parameter (Ding and Nan,
2011; Zhao et al., 2017). Further, if ν = 1
1+2p, we have d(ˆθn,θ0) = Op(n− p
1+2p), which achieves the
optimal convergence rate in the nonparametric regression setting.
Although the convergence rate for the nuisance parameter is slower than the typical rate n1/2, we
will show that the sieve MLE of the regression parameter, i.e. ˆβn, is still asymptotically normal and
achieves the semi-parametric eﬃciency bound. First, we introduce two additional regularity conditions
which are stated below.
(C7) There exist v∗ = ( v∗
1,··· ,v∗
d)T and w∗ = ( w∗
1,··· ,w∗
d)T, where v∗
j ∈ Γ2 and w∗
j ∈ G2 for
j = 1,··· ,d, such that P{∆A∗(U,X)ψ′
0γ(Y,X)[v]}= 0 and P{∆A∗(U,X)ψ′
0g(Y,X)[w]}= 0
15
hold for any v∈Γp1 and w∈Gp2 . Here U and V are deﬁned the same as in condition (C5) and
A∗(t,X) = −
(
g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1
)
X(−1)
+ g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ t
0
v∗(R−1(se−V))ds+ v∗(R−1(te−V))
+ g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ ˜Λ0(t)
0
exp(−g0(s))w∗(s)ds+ w∗(˜Λ0(t)),
where ˜Λ0(t) is the solution of ˜Λ′
0(t) = exp
(
g0(˜Λ0)
)
with ˜Λ0(0) = 0.
(C8) Let l∗(β0,γ0,ζ0; W) =
∫
A∗(t,X)dM(t), where M(t) = ∆1 (U ≤t) −
∫t
0 1 (U ≥s)d˜Λ0(s) is the
event counting process martingale. The information matrix I(β0) = P(l∗(β0,γ0,ζ0; W)⊗2) is
nonsingular. Here for a vector a, a⊗2 = aaT.
The additional condition (C7) essentially requires the existence of the least favorable direction that
is used to establish the semi-parametric eﬃciency bound. The directions v∗ and w∗ may be found
through the equations in (C7). We illustrate how to construct v∗ and w∗ for the Cox model and
the linear transformation model with a known transformation respectively in Remark 10. Condition
(C8) is a natural assumption that requires the information matrix to be invertible. The following
theorem establishes the asymptotic normality and semi-parametric eﬃciency of the sieve MLE ˆβn of
the regression parameter for the general linear transformation model.
Theorem 2.(Asymptotic normality of ˆβn) Suppose the conditions in Theorem 1 and (C7)-(C8) hold,
then we have
√n( ˆβn −β0) = √nI−1(β0)Pnl∗(β0,γ0,ζ0; W) + op(1) →d N(0,I−1(β0))
with I(β0) given in condition (C8) and →d denoting convergence in distribution.
Theorem 2 states that ˆβn is asymptotically normal with variance as the inverse of the information
matrix. In practice, the information matrix can be approximated by the estimated information matrix
of all parameters including the coeﬃcients of spline bases.
We note that the existing sieve M-theorem for bundled parameters (Ding and Nan, 2011; Zhao
et al., 2017) cannot be directly applied to prove Theorem 2, because it does not allow the inﬁnite-
dimensional nuance parameter to be a function of other inﬁnite-dimensional nuance parameters.
Therefore, to study the asymptotic distribution of ˆβn, we ﬁrst establish a new general M-theorem
for bundled parameters where the inﬁnite-dimensional nuisance parameter is a function of not only
the Euclidean parameter of interest but also other inﬁnite-dimensional nuisance parameters. The es-
tablished M-theorem under such a general scenario then enables us to prove Theorem 2 by verifying
its assumptions for the linear transformation model. The details are provided in the Supplemental
Material. Since the new M-theorem can be useful for developing the asymptotic normality of sieve
estimators for other ODE models, we state it below for readers of interest.
16
We ﬁrst introduce the general setting and notation for the proposed sieve M-theorem. Let m(θ; W)
be an objective function of unknown parameters θ= (β,γ(·),ζ(·,β, γ)) given a single observation W,
where β is the ﬁnite-dimensional parameter of interest, γ(·) = ( γ1(·),...,γ d2 (·)) denotes inﬁnite-
dimensional nuisance parameters, and ζ(·,β, γ) is another inﬁnite-dimensional nuisance parameter
that can be a function of β and γ. Here “ ·” represents some components of W. Given i.i.d. obser-
vations {Wi}n
i=1, the sieve estimator ˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) maximizes the objective function,
Pnm(θ; W), over certain sieve space. For example, ˆθn becomes the sieve MLE if mis the log-likelihood
function. We denote the derivative of m with respect to β as m′
β, the functional derivative of m with
respect to γj along the direction v(·) as m′
γj[v] for 1 ≤j ≤d2, and the functional derivative of mwith
respect to ζ along the direction h(·) as m′
ζ[h], whose rigorous deﬁnitions are given in the Supplemental
Material. The following theorem then establishes the asymptotic normality of the sieve estimator, ˆβn,
under the above general setting.
Theorem 3. (A general M-theorem for bundled parameters.) Under assumptions (A1)-(A6) in the
Supplemental Material, we have
√n( ˆβn −β0) = A−1√nPnm∗(β0,γ0(·),ζ0(·,β0,γ0); W) + op(1)
→d N(0,A−1B(A−1)T),
where
m∗(β0,γ0(·),ζ0(·,β0,γ0); W) = m′
β(β0,γ0(·),ζ0(·,β0,γ0); W) −
d2∑
j=1
m′
γj(β0,γ0(·),ζ0(·,β0,γ0); W)[v∗
j]
−m′
ζ(β0,γ0(·),ζ0(·,β0,γ0); W)[h∗(·,β0,γ0)],
B = P{m∗(β0,γ0(·),ζ0(·,β0,γ0); W)m∗(β0,γ0(·),ζ0(·,β0,γ0); W)T},
with v∗
j = (v∗
j1,...,v ∗
jd1 )T, h∗= (h∗
1,...,h ∗
d)T and A given in the assumption (A3).
Remark 9. The assumptions needed in Theorem 3 are similar to those in Ding and Nan (2011) (see
the Supplemental Material for details). However, our proposed theorem signiﬁcantly diﬀers from the
main theorem in Ding and Nan (2011), because the latter considers ζ(·,β) to be a function of only
the ﬁnite-dimensional parameter β, while we consider a more general scenario of bundled parameters,
where the nuisance parameter ζ(·,β, γ) can be a function of both the ﬁnite-dimensional parameter β
and other inﬁnite-dimensional nuisance parameters γ. The proposed theorem nontrivially extends the
asymptotic distributional theories for M-estimation under this general scenario.
Remark 10. We note that to ﬁnd the least favorable directions v∗ and w∗ required in (C7), we
may solve the equations in (C7), which can be simpliﬁed to equations (S37) and (S39) provided in
the Supplemental Material. For illustration, we provide explicit constructions of the least favorable
directions for the Cox model and for the linear transformation model with a known transformation
17
respectively. Speciﬁcally, for the Cox model, we have g0 ≡0 and v∗ can be derived as
v∗(t) = P{1 (Y ≥t)eXTβ0 X}
P{1 (Y ≥t)eXTβ0 } ;
for the linear transformation model where γ0 is known, w∗ can be obtained as
w∗(t) = φφφ(t) −g′
0(t)
∫ t
0
φφφ(s)ds,
where
φφφ(t) =
(
g′
0(t) exp(g0(t))˜Λ−1
0 (t) + 1
)P{1 (Λ0(Y,X) ≥t)X}
P{1 (Λ0(Y,X) ≥t)}
with ˜Λ0 deﬁned in (C7).
Given the above constructions of the least favorable directions, we can further simplify the non-
singularity condition of the information matrix in (C8). For the Cox model, the information matrix can
be derived as I(β0) =
∫∞
0 P
(
[−X+ µ(t)]⊗2 1 (U ≥t)
)
dt,where µ(t) = P{1 (U ≥t)eXTβ0 X}/P{1 (U ≥
t)eXTβ0 }with U deﬁned in (C5). Respectively, for the linear transformation where γ0 is known, the
information matrix can be derived as I(β0) =
∫∞
0 m2(t) ·Var(X|U ≥t) ·P(U ≥t) ·exp
(
g0(˜Λ0(t))
)
dt,
where m(t) = g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1. The non-singularity condition requires the integral of a
covariance matrix to be positive deﬁnite.
Remark 11. Moreover, for the general class of ODE models that include covariates Z with time-
varying coeﬃcients η(·) in (2), we have further established the same convergence rate of the sieve
estimator ˆθn in Theorem 4 and the asymptotic normality of ˆβn in Theorem 5 in the Supplemental
Material. In particular, the conditions (C1)-(C8) have been revised to (C1 ′)-(C8′) with additional
regularity conditions on covariates Z. We refer to the Supplemental Material for the full list of
conditions, rigorous statements of theorems, and their proofs.
5 Simulation Studies
In this section, we use simulation studies to show the ﬁnite sample performance of the sieve MLE
under the time-varying Cox model and the general linear transformation model.
5.1 Time-varying Cox model
We generate event times from the model
Λ′
x(t) = α(t) exp(β1x1 + β2x2 + β3x3 + β4x4 + η(t)x5),
where ( x1,x2,x3,x4,x5) follows a multivariate normal distribution with mean 0 and autoregressive
covariance truncated at ±2, β1 = β4 = 1, and β2 = β3 = −1. Let η(t) = sin
(3
4 πt
)
be a time-varying
18
coeﬃcient for x5 and the coeﬃcients of all other covariates be time-independent. The baseline hazard
α(t) is set to 0.5. The censoring times are generated from an independent uniform distribution U(0,3),
which leads to a censoring rate around 50%. The sample size N varies from 1,000 to 8 ,000. We ﬁt
both the log-transformed baseline hazard function log α(t) and time-varying coeﬃcient η(t) by cubic
B-splines and set the number of knots Kn = ⌊N′1
5 ⌋, i.e., the largest integer smaller than N′1
5 , where N′
is the number of distinct observation time points. The interior knots are located at the Kn quantiles
of the N′ distinct observation time points. We compare the estimation accuracy and the computing
time of the proposed sieve MLE with those of the partial likelihood-based estimator implemented in
the “coxph” function in R with the “tt” argument set as the same cubic B-spline transformation of
time.
Table 1: Simulation results under time-varying Cox model.
N Method β1 = 1 β2 = −1 IMSE( η(t))
Bias SE ESE CP Bias SE ESE CP Mean SD
1000 ODE .008 .070 .070 .958 -.012 .076 .078 .955 .053 .041
Cox-MPLE .006 .070 .068 .952 -.010 .075 .075 .950 .109 .094
2000 ODE .004 .048 .048 .958 -.004 .053 .054 .957 .029 .021
Cox-MPLE .002 .048 .048 .956 -.003 .053 .053 .959 .053 .041
4000 ODE .003 .033 .034 .952 -.003 .038 .038 .938 .016 .011
Cox-MPLE .003 .033 .034 .950 -.002 .038 .037 .936 .026 .020
8000 ODE .000 .024 .024 .962 -.001 .026 .026 .938 .009 .006
Cox-MPLE .000 .023 .024 .959 -.001 .026 .026 .936 .013 .009
Bias is the diﬀerence between the mean of estimates and the true value, SE is the sample standard error
of the estimates, Mean is the mean of IMSE, and SD is the standard deviation of IMSE. ESE is the
mean of the standard error estimators by inverting the estimated information matrix of all parameters,
including the coeﬃcients of spline bases, and CP is the corresponding coverage proportion of 95%
conﬁdence intervals.
Figure 1: True α0(t) and mean of ˆα(t) (left); true η(t) and mean of ˆη(t) (middle) with the
sample size N = 8000; log-log plot of mean relative computation time (right) with respect to
the sample size under the time-varying Cox model.
19
Table 1 summarizes the estimates of regression coeﬃcients β1 and β2 based on 1000 replications.
The estimates of the other two regression coeﬃcients β3 and β4 perform similarly, and the results are
included in the Supplemental Material. For the time-varying coeﬃcient η(t), we report the integrated
mean square error (IMSE), which is the weighted sum of mean square error (MSE) of pointwise
estimates over simulated time points from 0 to 2. As one can see, the mean and standard deviation
of IMSE of the proposed sieve estimator decrease as the sample size increases. Remarkably, they
are consistently smaller than those of the partial likelihood-based estimator. For time-independent
coeﬃcients, the proposed sieve estimator performs as well as the partial likelihood-based estimator.
The mean of the standard error estimator, which is obtained by inverting the estimated information
matrix of all parameters including the coeﬃcients of spline bases, is approximate to the sample standard
error, and the corresponding 95% conﬁdence interval achieves a proper coverage proportion. From the
left and middle panels of Figure 1, we can see that the means of the estimated α(t) and η(t) are close
to the true functions, and the 95% pointwise conﬁdence bands cover the true functions well.
It is also worth noting that, in comparison to the partial likelihood-based estimation method whose
relative computing time with respect to that with the smallest sample size increases quickly as the
sample size grows, the proposed estimation method is computationally more eﬃcient, especially when
the sample size is large (see the right panel of Figure 1). When the number of knots increases with
the sample size, the computation time of the proposed method grows at a rate slightly larger than the
linear rate (but far below the quadratic rate).
5.2 Linear transformation model
We generate event times from the model Λ′
x(t) = q(Λx(t)) exp(β1x1 + β2x2 + β3x3)α(t). The covariates
are independent normal with mean 0 and standard deviation 0 .5 truncated at ±2. We consider four
diﬀerent settings for q(·) and α(·): 1) a constant q(t) = 1 and a monotonic increasing α(t) = t3, in
which case the Cox model is correctly speciﬁed; 2) a monotonic decreasing q(t) = e−t and a constant
α(t) = 2; 3) a monotonic decreasing q(t) = 2 /(1 + t) and a constant α(t) = 1; 4) an increasing
q(t) = log(1 + t) + 2 and an increasing α(t) = log(1 + t). In each setting, we generate the censoring
time from an independent uniform distribution U(0,c), where c is chosen to achieve approximately
25-30% censoring rates. The sample size N varies from 1,000 to 8,000.
In setting 1), we compare the proposed sieve MLE for the ODE-Cox model, where the functionq(·)
is set to 1, with the partial-likelihood based estimator implemented using the R package survival. We
ﬁt log α(·) by cubic B-splines with ⌊N′1
5 ⌋interior knots that are located at the quantiles of the distinct
observation time points. In setting 2), we compare the proposed sieve MLE for the ODE-LT model,
where the function q(·) is set to e−t, with the NPMLE for the equivalent logarithmic transformation
model considered in Zeng and Lin (2007b). We ﬁt log α(·) by cubic B-splines with the same placement
of interior knots. In setting 3), we compare the proposed sieve MLE for the ODE-AFT model, where
the function αis set to 1, with the rank-based estimation approach implemented using the R package
aftgee. We ﬁt log q(t) by cubic B-splines with ⌊N
1
7 ⌋interior knots that are located at the quantiles of
the estimated cumulative hazards under the Cox model. In setting 4) (as well as settings 1)-3)), we
20
Table 2: Estimates of regression coeﬃcients under correctly-speciﬁed ODE-Cox with q(·) ≡1,
ODE-LT with q(t) = e−t, and ODE-AFT with α(·) ≡1. Bias, SE, ESE and CP contain the
same meanings as those in Table 1.
β1 = 1 β2 = 1 β3 = 1
Method Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1) MPLE .002 .076 .075 .934 -.003 .075 .075 .941 -.001 .074 .075 .954
ODE-Cox .003 .076 .076 .936 -.002 .075 .076 .942 .000 .074 .076 .955
2) NPMLE .004 .117 .115 .949 -.001 .114 .115 .951 .003 .113 .115 .960
ODE-LT .005 .117 .115 .950 -.000 .114 .115 .951 .003 .113 .115 .961
3) Rank-based .004 .105 .102 .944 -.001 .102 .102 .950 .002 .100 .103 .954
ODE-AFT .000 .102 .097 .944 -.005 .100 .097 .944 -.002 .097 .097 .950
Setting 1): the Cox model is correctly speciﬁed. Setting 2): the logarithmic transformation model is correctly
speciﬁed. Setting 3): the AFT model is correctly speciﬁed.
Figure 2: The log-log plots of mean relative computing time with respect to the sample size
under the ODE-LT, the ODE-AFT model, and the ODE-Flex model are provided from left to
right respectively.
ﬁt the general linear transformation model (ODE-Flex) where both q(·) and α(·) are unspeciﬁed, and
compare the sieve MLE with the smoothed partial rank (SPR) method in Song et al. (2006). Both
methods constrain β1 = 1 for identiﬁability guarantee. For the sake of space, the results of the setting
4) are provided in the Supplemental Material.
Tables 2 and 3 summarize the estimates of regression coeﬃcients with the sample size N = 4,000
based on 1000 replications. Full results for the other sample sizes are provided in the Supplemental
Material. Table 2 indicates that when any of the Cox model, the logarithmic transformation model, or
the AFT model is correctly speciﬁed, the sieve estimator for the corresponding correctly speciﬁed ODE
model achieves similar performance as the partial-likelihood based estimator for the Cox model, the
NPMLE for the logarithmic transformation model, or the rank-based estimator for the AFT model.
However, the relative computing time of the proposed ODE approach increases linearly as the sample
size grows while that of the NPMLE for the logarithmic transformation model or the rank-based
method for the AFT model increases in a quadratic rate as shown in Figure 2.
21
Table 3: Estimates of regression coeﬃcients under the general linear transformation model
ODE-Flex with both q(·) and α(·) unspeciﬁed. Bias, SE, ESE and CP contain the same
meanings as those in Table 1.
β2 = 1 β3 = 1
Setting Bias SE ESE CP Bias SE ESE CP
1) .008 .106 .107 .947 .012 .104 .107 .959
2) -.019 .161 .151 .927 -.016 .159 .151 .938
3) -.014 .134 .131 .941 -.012 .131 .132 .945
4) .001 .092 .090 .939 .005 .091 .090 .954
Figure 3: The solid blue curves are the true q(·) (upper row) and α(·) (lower row). The solid
red curves are the means of corresponding estimated ˆ q(·) and ˆα(·) under the general linear
transformation model. The dashed yellow curves represent 95% pointwise conﬁdence bands
over 1,000 replications. From left to right, the four columns correspond to settings (1)-(4)
respectively.
For the general linear transformation model, we ﬁnd that the proposed ODE-Flex method has
advantages against the existing SPR method in terms of estimation accuracy, numerical stability, and
computational eﬃciency. We refer to the Supplemental Material for detailed results and comparison
with SPR. From Table 3, we can see that the bias of the ODE-Flex estimator is nearly negligible in all
settings. The standard error estimators are close to the sample standard errors, and the corresponding
95% conﬁdence intervals achieve a reasonable coverage proportion. When the Cox model, the loga-
22
rithmic transformation model, or the AFT model is correctly speciﬁed, their estimators (in Table 2)
achieve smaller standard errors than those for ODE-Flex (in Table 3), which is expected because both
q(·) and α(·) are unspeciﬁed in ODE-Flex. Figure 3 shows the mean of ˆ α(·) and ˆq(·) respectively. As
one can see, the means of ˆα(·) and ˆq(·) under the general linear transformation model are all close to
the true functions. Moreover, the relative computing time of ODE-Flex increases in a much smaller
rate than that of SPR as the sample size grows as shown in the right panel of Figure 2.
Note we have also considered other alternative knots placements (see the Supplemental Material)
and our numerical results suggest that knot selection does not appear critical for the proposed method.
6 Data Example
In this section, we apply the proposed method to a kidney post-transplantation mortality study. End-
stage renal disease (ESRD) is one of the most deadly and costly diseases in the United States. From
2004 to 2016, ESRD incident cases increased from 345.6 to 373.4 per million people, with Medicare
expenditures escalating from 18 to 35 billion dollars (Saran et al., 2017). Kidney transplantation is the
renal replacement therapy for the majority of patients with ESRD. Successful kidney transplantation
is associated with improved survival, improved quality of life, and health care cost savings when
compared to dialysis. However, despite aggressive eﬀorts to increase the number of donor kidneys, the
demand far exceeds the supply of donor kidneys for transplantation and hence, the donor waiting list
is very long. Currently about 130,000 patients are waiting for lifesaving organ transplants in the U.S.,
among whom 100,000 await kidney transplants and fewer than 15% of patients will receive transplants
in their lifetime. To optimize the organ allocation, further research is essential to determine the risk
factor associated with post-transplant mortality.
To better understand this problem, we considered the data obtained from the Organ Procurement
and Transplantation Network (OPTN). There were 146,248 patients who received transplants between
1990 and 2008. Failure time (recorded in years) was deﬁned as the time from transplantation to
graft failure or death, whichever occurred ﬁrst, where graft failure was considered to occur when
the transplanted kidney ceased to function. Patient survival was censored 6 year post-transplant
or at the end of study (2008). The median follow-up time was around 6 years and the censoring
rate was 62%. Covariates included in this study were age at transplantation, race, gender, cold
ischemic time, donation after cardiac death (DCD), BMI, expanded criteria donor (ECD), dialysis
time, comorbidity conditions such as glomerulonephritis, polycystic kidney disease, diabetes, and
hypertension. Detecting and accounting for time-varying eﬀects are particularly important in the
context of kidney transplantation, as non-proportional hazards have already been reported in the
literature (Wolfe et al., 1999; He et al., 2017). Also, analyses with time-varying eﬀects provide valuable
clinical information that could be obscured otherwise.
However, existing statistical softwares become computationally infeasible when ﬁtting a time-
varying eﬀects model on a data set as large as what we have here. Thus, to estimate the potential
time-varying eﬀects, we ﬁt the time-varying Cox model using the proposed sieve MLE, which is com-
23
putationally scalable. Speciﬁcally, based on previous studies, DCD, Polycystic, Diabetes and Hyper-
tension are modeled with time-independent eﬀects, and the remaining variables are estimated with
time-varying eﬀects. The time-varying eﬀects are all implemented by cubic B-splines with 5 interior
knots, which is chosen based on the Bayesian information criterion. Figure 4 shows the estimated
baseline hazard function. We can see that the post-transplant mortality is high in the short term after
surgery, with a weakening association over time. Table 4 summarizes the estimated time-independent
eﬀects, and Figure 5 shows examples of ﬁtted time-varying eﬀects with 95% pointwise conﬁdence inter-
vals, where the standard error estimators were obtained by inverting the estimated information matrix
of all parameters including time-independent coeﬃcients and the coeﬃcients of spline bases. As one
can see, the eﬀects of baseline age varied over time, resulting in an eventually strengthened association.
Speciﬁcally, compared with the reference group (age at transplantation between 19-39), patients 40 to
49 years of age had a protective eﬀect in the short term after transplantation. We can also see that
the high cold ischemic time is a risk factor for mortality in the short run, with a weakening association
over time. Thus, special care should be dedicated to improve the short-term outcome. As expected,
longer waiting times on dialysis (greater than 5 years) negatively impact post-transplant survival,
especially in the short run. Male gender was not signiﬁcantly associated with mortality immediately
after the renal transplantation but became a risk factor in the long run. As can be seen in Figure
5, underweight shows a protective eﬀect in the short run, and then a slightly weakening association
over time, which conﬁrms the previous ﬁnding of Lafranca et al. (2015). The results regarding high
BMI should be interpreted with caution. Although higher levels of BMI in the general population
are typically associated with high mortality, in chronic kidney diseases, such as patients with kidney
dialysis and kidney transplantation, higher BMI has been associated with better survival, which has
been labeled as reverse epidemiology (Dekker et al., 2008; Kovesdy et al., 2010). Our results show
that both overweight and obesity improved survival in the short term after kidney transplantation,
but obesity became a risk factor after long-term exposure. One possible explanation is that BMI is a
complex marker of visceral and nonvisceral adiposity and also of nutritional status including muscle
mass (Kovesdy et al., 2010), and the improved short-term outcome associated with higher BMI may
be related to diﬀerential beneﬁts by one or more of these components. Our ﬁndings indicate a need
to critically reassess the role of BMI in the risk stratiﬁcation of kidney transplantation. A further
assessment (such as sub-group analysis) of high BMI that diﬀerentiates between visceral adiposity,
nonvisceral adiposity and higher muscle mass may improve risk stratiﬁcation in kidney transplant
recipients. In addition, our results show that graft survival for patients with Glomerulonephritis is
better than patients with other primary diseases. Regarding racial disparities, the long-term survival
outcomes for African Americans continue to lag behind non-African Americans. Finally, as expected,
the eﬀect of expanded criteria donor (ECD) is not as good as optimal donor. When a sub-optimal
organ becomes available, patients and physicians must decide whether to accept the oﬀer and special
care must be dedicated to improve the survival beneﬁt.
24
Table 4: Summary of estimates for time-independent eﬀects in kidney post-transplantation
mortality study
Variables DCD Polycystic Diabetes Hypertension
EST −0.081 −0.511 0 .333 −0.146
ESE 0 .038 0 .021 0 .012 0 .014
95% CI [ −0.156,−0.007] [ −0.553,−0.469] [ 0 .310, 0.357] [ −0.172,−0.119]
p-value 0 .033 <0.001 <0.001 <0.001
* EST is the estimated time-independent eﬀect, ESE is the estimated standard error by inverting the
estimated information matrix of all parameters including the coeﬃcients of spline basis, and CI is the
conﬁdence interval.
0 1 2 3 4 5 6 7 8
Time after transplantation (Year)
0.02
0.04
0.06
0.08
0.1
0.12
0.14 Baseline Hazard
Baseline Hazard Estimate
95% CI
Figure 4: Estimated baseline hazard ˆ α(t) using the the proposed sieve MLE method for the
kidney transplantation data.
7 Discussion
In this paper, we have proposed a novel ODE framework for survival analysis, which uniﬁes the current
literature, along with a general estimation procedure which is scalable and easy to implement. The
ODE framework provides a new perspective for modeling censored data, which further allows us to
utilize well-developed numerical solvers and local sensitivity analysis tools for ODEs in parameter
estimation. Although we have only focused on one class of ODE models in this paper, the ODE
framework and the estimation method oﬀer new opportunities for investigating more ﬂexible model
structures.
We note that a few recent works also use ODEs for survival analysis. Speciﬁcally, Tang et al.
(2020) model the cumulative hazard as in the ODE (1) with the function f(·) being a neural network
to improve feature representation. The method proposed in Tang et al. (2020) can be viewed as a
neural-network-based extension of the general framework studied in this work, which demonstrates
that the proposed ODE framework can be used to build ﬂexible models. Groha et al. (2020) propose
a neural-network-based ODE approach to model the Kolmogorov forward equation that characterizes
25
0 2 4 6 8
Time after transplantation (Year)
-0.5
0
0.5
Age 40-49
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
0
0.2
0.4
0.6
0.8
Age 50-59
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
0
0.5
1
Age 60+
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.2
-0.1
0
0.1
0.2
0.3
High Cold Ischemic Time
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
0
0.2
0.4
0.6
Long Waiting Time
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.2
-0.1
0
0.1
0.2
0.3
Male
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.4
-0.2
0
0.2
Underweight
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.4
-0.2
0
0.2
Overweight
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.4
-0.2
0
0.2
Obesity
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.6
-0.4
-0.2
0
Glomerulonephritis
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
-0.2
0
0.2
African American
Time-varying Estimate
Zero Line
95% CI
0 2 4 6 8
Time after transplantation (Year)
0
0.2
0.4
0.6
Expanded Criteria Donor
Time-varying Estimate
Zero Line
95% CI
Figure 5: Estimated time-varying eﬀects using the proposed sieve MLE method for the kidney
transplantation data.
26
the transition probabilities for multi-state survival analysis. Both the aforementioned works focus
on developing ﬂexible models with powerful representation learning via neural networks to improve
prediction performance. In this work, instead, we focus on estimation and inference for a general class
of semi-parametric ODE models, in which case the eﬀects of certain covariates are often of interest.
More importantly, we revisit the rich literature of survival analysis and provide a uniﬁed view of many
existing survival models, which is the key insight that diﬀerentiates this work and the aforementioned
ones. This uniﬁcation merit serves as the foundation of the proposed widely applicable estimation
procedure. We also establish the consistency and semi-parametric eﬃciency of the proposed sieve
estimator for a general class of semi-parametric ODE models, with a new general sieve M-theorem.
The proposed general theory derives the asymptotic distribution of bundled parameters, where
the nuisance parameter is a function of not only the regression parameters of interest but also other
inﬁnite-dimensional nuisance parameters. Though we have only illustrated the eﬃcient estimation
in the linear transformation model as an example to motivate such a theoretical development, the
proposed general theory can be extended to other models.
In addition, an interesting application of the uniﬁed ODE framework is to check the model speci-
ﬁcation. In particular, the estimation and inference for a general ODE model can help test whether a
nested model is appropriate for a dataset. For example, Proposition 2 implies that the function q(·)
or α(·) in the linear transformation model (5) should be a power function when it coincides with the
Cox or the AFT model. Though we have established the consistency of the functional parameters
q(·) and α(·) in the nonparametric linear transformation model, it is worthwhile to further investigate
their asymptotic distributional theory for model diagnostics as future work. As a preliminary study,
we have explored a heuristic parametric approach for model diagnostics and provided its ﬁnite sample
performance in the Supplemental Material.
Finally, we note that a few recent works have tried to address the computation burden of certain
estimation methods for speciﬁc models on massive time-to-event data. In particular, Wang et al. (2019)
proposed an eﬃcient divide-and-conquer (DAC) algorithm for the sparse Cox model. Kawaguchi et al.
(2020) developed an algorithm for reducing the computation cost of ﬁtting the Fine-Gray (Fine and
Gray, 1999) proportional subdistributional hazards model by exploiting its special structure. Zuo
et al. (2021) proposed a subsampling procedure to approximate the full-data estimator for the additive
hazard model. Note that most of these methods are tailored for a speciﬁc model while our method can
be applied more broadly. Further, our estimation procedure and these methods are not competitors.
In contrast, some of the techniques used in these methods, such as DAC, can be naturally integrated
into the proposed estimation procedure, which is an interesting future direction to be explored.
References
Aalen, O. (1980). A model for nonparametric regression analysis of counting processes. InMathematical
Statistics and Probability Theory , pp. 1–25. New York, NY: Springer.
27
Ai, C. and X. Chen (2003). Eﬃcient estimation of models with conditional moment restrictions
containing unknown functions. Econometrica 71 (6), 1795–1843.
Andersen, P. K. and R. D. Gill (1982). Cox’s regression model for counting processes: A large sample
study. The Annals of Statistics 10 (4), 1100–1120.
Bagdonavicius, V. and M. Nikulin (2001). Accelerated Life Models: Modeling and Statistical Analysis .
New York, NY: Chapman and Hall/CRC.
Bagdonavicius, V. B. and M. S. Nikulin (1999). Generalized proportional hazards model based on
modiﬁed partial likelihood. Lifetime Data Analysis 5 (4), 329–350.
Bennett, S. (1983). Analysis of survival data by the proportional odds model. Statistics in
Medicine 2 (2), 273–277.
Billingsley, P. (2008). Convergence of Probability Measures. John Wiley & Sons, Ltd.
Buckley, J. and I. James (1979). Linear regression with censored data. Biometrika 66 (3), 429–436.
Cai, T., L. Tian, and L. J. Wei (2005). Semiparametric Box-Cox power transformation models for
censored survival observations. Biometrika 92 (3), 619–632.
Chen, K., Z. Jin, and Z. Ying (2002). Semiparametric analysis of transformation models with censored
data. Biometrika 89 (3), 659–668.
Chen, K. and X. Tong (2010). Varying coeﬃcient transformation models with censored data.
Biometrika 97 (4), 969–976.
Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. In Handbook of
Econometrics (1 ed.), Volume 6B, Chapter 76. Elsevier.
Chen, X., O. Linton, and I. Van Keilegom (2003). Estimation of semiparametric models when the
criterion function is not smooth. Econometrica 71 (5), 1591–1608.
Cheng, S. C., L. J. Wei, and Z. Ying (1995). Analysis of transformation models with censored data.
Biometrika 82 (4), 835–845.
Cox, D. R. (1975). Partial likelihood. Biometrika 62 (2), 269–276.
Dekker, F., R. Mutsert, P. Dijk, C. Zoccali, and K. Jager (2008). Survival analysis: time-dependent
eﬀects and time-varying risk factors. Kidney International 74 , 994–997.
Dickinson, R. P. and R. J. Gelinas (1976). Sensitivity analysis of ordinary diﬀerential equation
systems—A direct method. Journal of Computational Physics 21 (2), 123–143.
28
Ding, Y. and B. Nan (2011). A sieve M-theorem for bundled parameters in semiparametric models,
with application to the eﬃcient estimation in a linear model for censored data. The Annals of
Statistics 39 (6), 3032–3061.
Fine, J. P. and R. J. Gray (1999). A proportional hazards model for the subdistribution of a competing
risk. Journal of the American Statistical Association 94 (446), 496–509.
Fine, J. P., Z. Ying, and L. J. Wei (1998). On the linear transformation model for censored data.
Biometrika 85 (4), 980–986.
Gray, R. J. (1994). Spline-based tests in survival analysis. Biometrics 50 (3), 640.
Groha, S., S. M. Schmon, and A. Gusev (2020). Neural odes for multi-state survival analysis. arXiv
preprint arXiv:2006.04893 .
He, K., Y. Yang, Y. Li, J. Zhu, and Y. Li (2017). Modeling time-varying eﬀects with large-scale survival
data: an eﬃcient quasi-newton approach. Journal of Computational and Graphical Statistics 26 (3),
635–645.
He, X., H. Xue, and N. Shi (2010). Sieve maximum likelihood estimation for doubly semiparametric
zero-inﬂated poisson models. Journal of Multivariate Analysis 101 (9), 2026–2038.
Horowitz, J. L. (1996). Semiparametric estimation of a regression model with an unknown transfor-
mation of the dependent variable. Econometrica 64 (1), 103–137.
Huang, J. (1999). Eﬃcient estimation of the partly linear additive Cox model. The Annals of Statis-
tics 27 (5), 1536–1563.
Jin, Z., D. Y. Lin, L. J. Wei, and Z. Ying (2003). Rank-based inference for the accelerated failure time
model. Biometrika 90 (2), 341–353.
Jin, Z., D. Y. Lin, and Z. Ying (2006). On least-squares regression with censored data.
Biometrika 93 (1), 147–161.
Kalbﬂeisch, J. D. and R. L. Prentice (2011). The statistical analysis of failure time data , Volume 360.
John Wiley & Sons.
Kawaguchi, E. S., J. I. Shen, M. A. Suchard, and G. Li (2020). Scalable algorithms for large competing
risks data. Journal of Computational and Graphical Statistics 0 (0), 1–9.
Khan, S. and E. Tamer (2007). Partial rank estimation of duration models with general forms of
censoring. Journal of Econometrics 136 (1), 251–280.
Kovesdy, C., M. Czira, A. Rudas, A. Ujszaszi, L. Rosivall, M. Novak, K. Kalantar-Zadeh, M. Mol-
nar, and I. Mucsi (2010). Survival analysis: time-dependent eﬀects and time-varying risk factors.
American Journal of Transplantation 10 (12), 2644–2651.
29
Lafranca, J., J. IJermans, M. Betjes, and J. Frank (2015). Body mass index and outcome in renal
transplant recipients: a systematic review and meta-analysis. BMC Medicine 13 (111).
Lai, T. L. and Z. Ying (1991). Large sample theory of a modiﬁed Buckley-James estimator for
regression analysis with censored data. The Annals of Statistics 19 (3), 1370–1402.
Lin, D. Y. and Z. Ying (1995). Semiparametric analysis of general additive-multiplicative hazard
models for counting processes. The Annals of Statistics 23 (5), 1712–1734.
Lin, Y. and K. Chen (2012). Eﬃcient estimation of the censored linear regression model.
Biometrika 100 (2), 525–530.
Mckeague, I. W. and P. D. Sasieni (1994). A partly parametric additive risk model. Biometrika 81 (3),
501–514.
Murphy, S. A., A. J. Rossini, and A. W. van der Vaart (1997). Maximum likelihood estimation in the
proportional odds model. Journal of the American Statistical Association 92 (439), 968–976.
Petzold, L., S. Li, Y. Cao, and R. Serban (2006). Sensitivity analysis of diﬀerential-algebraic equations
and partial diﬀerential equations. Computers and Chemical Engineering 30 (10), 1553–1559.
Qiu, Z. and Y. Zhou (2015). Partially linear transformation models with varying coeﬃcients for
multivariate failure time data. Journal of Multivariate Analysis 142 , 144–166.
Royston, P. and M. K. B. Parmar (2002). Flexible parametric proportional-hazards and proportional-
odds models for censored survival data, with application to prognostic modelling and estimation of
treatment eﬀects. Statistics in Medicine 21 (15), 2175–2197.
Saran, R., B. Robinson, K. C. Abbott, L. Y. Agodoa, J. Bragg-Gresham, R. Balkrishnan, N. Bhave,
et al. (2017). US renal data system 2016 annual data report: Epidemiology of kidney disease in the
United States. American Journal of Kidney Diseases 65 (5), A7–A8.
Schumaker, L. (2007). Spline Functions: Basic Theory (3rd ed.). Cambridge Mathematical Library.
Cambridge: Cambridge University Press.
Shen, X. (1997). On methods of sieves and penalization. The Annals of Statistics 25 (6), 2555–2591.
Shen, X. (1998, 03). Propotional odds regression and sieve maximum likelihood estimation.
Biometrika 85 (1), 165–177.
Shen, X. and W. H. Wong (1994). Convergence rate of sieve estimates. The Annals of Statistics 22 (2),
580–615.
Song, X., S. Ma, J. Huang, and X. Zhou (2006). A semiparametric approach for the nonparametric
transformation survival model with multiple covariates. Biostatistics 8 (2), 197–211.
30
Tang, W., J. Ma, Q. Mei, and J. Zhu (2020). Soden: A scalable continuous-time survival model
through ordinary diﬀerential equation networks. arXiv preprint arXiv:2008.08637 .
Tsiatis, A. A. (1990). Estimating regression parameters using linear rank tests for censored data. The
Annals of Statistics 18 (1), 354–372.
Van Der Vaart, A. W. and J. A. Wellner (1996). Weak convergence. InWeak convergence and empirical
processes, pp. 16–28. Springer.
Walter, W. (1998). First order systems. Equations of higher order. In Ordinary Diﬀerential Equations,
pp. 105–157. New York, NY: Springer New York.
Wang, Y., C. Hong, N. Palmer, Q. Di, J. Schwartz, I. Kohane, and T. Cai (2019, 09). A fast divide-
and-conquer sparse Cox regression. Biostatistics.
Wellner, J. A. and Y. Zhang (2007). Two likelihood-based semiparametric estimation methods for
panel count data with covariates. The Annals of Statistics 35 (5), 2106–2142.
Wolfe, R., V. Ashbyv, E. Milfordv, A. Ojov, R. Ettengerv, L. Agodoav, P. Heldv, and F. Portv (1999).
Comparison of mortality in all patients on dialysis, patients on dialysis awaiting transplantation,
and recipients of a ﬁrst cadaveric transplant. The New England Journal of Medicine 341 (23),
1725–1730.
Zeng, D. and D. Y. Lin (2006). Eﬃcient estimation of semiparametric transformation models for
counting processes. Biometrika 93 (3), 627–640.
Zeng, D. and D. Y. Lin (2007a). Eﬃcient estimation for the accelerated failure time model. Journal
of the American Statistical Association 102 (480), 1387–1396.
Zeng, D. and D. Y. Lin (2007b). Maximum likelihood estimation in semiparametric regression mod-
els with censored data. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy) 69 (4), 507–564.
Zhang, Y., L. Hua, and J. Huang (2010). A spline-based semiparametric maximum likelihood esti-
mation method for the Cox model with interval-censored data. Scandinavian Journal of Statis-
tics 37 (2), 338–354.
Zhao, X., Y. Wu, and G. Yin (2017). Sieve maximum likelihood estimation for a general class of
accelerated hazards models with bundled parameters. Bernoulli 23 (4B), 3385–3411.
Zucker, D. M. and A. F. Karr (1990). Nonparametric survival analysis with time-dependent covariate
eﬀects: A penalized partial likelihood approach. The Annals of Statistics 18 (1), 329–353.
Zuo, L., H. Zhang, H. Wang, and L. Liu (2021). Sampling-based estimation for massive survival data
with additive hazards model. Statistics in Medicine 40 (2), 441–450.
31
Supplemental Material: Survival Analysis via Ordinary
Diﬀerential Equations
This supplementary material is structured as follows. We provide the detailed derivation of the
local sensitivity analysis and optimization algorithm in Section 1. We present the proposed general
M-theorem for bundled parameters (Theorem 3) and its proof in Section 2. The proofs of Theorems 1
and 2 are given in Section 3, those of Propositions 1 and 2 are given in Section 4. We further establish
the convergence rate and the asymptotic normality of the proposed sieve estimator for the general class
of ODE models in the presence of covariates Z with time-varying coeﬃcients in Section 5. Additional
simulation studies are provided in Section 6.
1 Optimization Algorithm With Local Sensitivity Anal-
ysis
In this section, we ﬁrst derive two types of local sensitivity analysis that can be used to compute the
gradient of the log-likelihood function when it contains the solution of a general ODE. When the ODE
is separable in the model formulation, we introduce a trick to further accelerate the evaluation of the
objective for n independent observations in subsection 1.1.
We consider any parameterized survival model in the form of
{
dΛx(t)/dt= f(t,Λx(t); x,θ)
Λx(t0) = c(x,θ)
, (S1)
where θ denotes all the parameters. For example, for the general class of ODE models in (8), the
function f is given by the right hand side of (8), the parameter θ consists of β, a, and b, the initial
time point t0 = 0, and the initial value c(·) equals to zero. Denote the solution of (S1) by Λ x(t; θ).
Then under the non-informative censoring, the log-likelihood function is given by
ln(θ) = 1
n
n∑
i=1
[∆ilog f(Yi,ΛXi(Yi; θ); Xi,θ) −ΛXi(Yi; θ)] .
To obtain the maximum likelihood estimator, we propose a gradient-based optimization algorithm
which utilizes the local sensitivity analysis to compute the gradient. By applying the chain rule, the
gradient is given by
d ln(θ)
dθ = 1
n
n∑
i=1
{[
∆i
f′
2 (Yi,ΛXi(Yi; θ); Xi,θ)
f(Yi,ΛXi(Yi; θ); Xi,θ) −1
]∂Λxi(Yi; θ)
∂θ + ∆i
f′
4 (Yi,ΛXi(Yi; θ); Xi,θ)
f(Yi,ΛXi(Yi; θ); Xi,θ)
}
,
where we use the subscript 2 and 4 in the derivatives to indicate that the derivatives are taken with
respect to the ﬁrst and the fourth argument of the function f respectively. Then as long as we
can derive the gradient of Λ x(y; θ) with respect to θ for a given y, we can obtain the gradient of the
1
likelihood function for faster gradient-based computations.
There are two commonly used types of local sensitivity analyses: forward sensitivity analysis and
adjoint sensitivity analysis (Dickinson and Gelinas, 1976; Petzold et al., 2006). We ﬁrst derive the
corresponding ODE for the forward sensitivity analysis. Denote the partial derivatives of f(t,Λ; x,θ)
with respect to θ and Λ by f′
θ and f′
Λ, respectively. Under certain smoothness condition of f, there is
one unique solution Λ x(t; θ) of (S1) and it satisﬁes
Λx(t; θ) =
∫ t
t0
f(s,Λx(s; θ); x,θ)ds+ c(x,θ).
By interchanging the integral and partial diﬀerential operators, it follows that
∂Λx(t; θ)
∂θ = ∂
∂θ
∫ t
t0
f(s,Λx(s; θ); x,θ)ds+ c′
θ(x,θ) (S2)
=
∫ t
t0
(
f′
θ(s,Λx(s; θ); x,θ) + f′
Λ(s,Λx(s; θ); x,θ)∂Λx(s; θ)
∂θ
)
ds+ c′
θ(x,θ),
where c′
θ(x,θ) is the derivative of c(x,θ) with respect to θ. Therefore, ∂Λx(y; θ)/∂θ = F1(y) with F1
satisfying
{
dF1(t)/dt= f′
θ(t,Λx(t; θ); x,θ) + f′
Λ(t,Λx(t; θ); x,θ) ·F1
F1(t0) = c′
θ(x,θ)
. (S3)
After plugging t0 = 0 and c(·) = 0, (S3) becomes the initial value problem (9) in Section 3.
Next, we derive the corresponding ODE for the adjoint sensitivity analysis. Since Λ x(t; θ) is
solution of (S1), for some appropriately chosen diﬀerentiable function κ(t,θ) to be speciﬁed later, we
have
Λx(t; θ) = Λx(t; θ) −
∫ t
t0
κ(s,θ)
[∂Λx(s; θ)
∂s −f(s,Λx(s,θ); x,θ)
]
ds.
By taking derivatives with respect to θ on both sides, it follows that
∂Λx(t; θ)
∂θ = ∂Λx(t; θ)
∂θ − ∂
∂θ
∫ t
t0
κ(s,θ)
[∂Λx(s; θ)
∂s −f(s,Λx(s,θ); x,θ)
]
ds
= ∂Λx(t; θ)
∂θ −
∫ t
t0
κ(s,θ) ∂
∂θ
[∂Λx(s; θ)
∂s −f(s,Λx(s,θ); x,θ)
]
ds
=
∫ t
t0
(1 + κ(s,θ)) ∂
∂θf(s,Λx(s; θ); x,θ)ds+ c′
θ(x,θ) −
∫ t
t0
κ(s,θ) ∂
∂s
[∂Λx(s; θ)
∂θ
]
ds,
where the second equality holds because
∫ t
t0
∂κ(s,θ)
∂θ
[∂Λx(s; θ)
∂s −f(s,Λx(s,θ); x,θ)
]
ds= 0,
2
and the last equality holds by plugging (S2) and exchanging the order of derivatives. Using integral
by parts, we have
∫ t
t0
κ(s,θ) d
ds
[dΛx(s; θ)
dθ
]
ds+
∫ t
t0
dκ(s,θ)
ds
dΛx(s; θ)
dθ ds=
(
κ(s,θ)dΛx(s; θ)
dθ
)⏐⏐⏐
t
t0
.
Then it follows that
dΛx(t; θ)
dθ =
∫ t
t0
(1 + κ(s,θ))
(
f′
θ(s,Λx(s; θ); x,θ) + f′
Λ(s,Λx(s; θ); x,θ)dΛx(s; θ)
dθ
)
ds+ c′
θ(x,θ)
+
∫ t
t0
dκ(s,θ)
ds
dΛx(s; θ)
dθ ds−
(
κ(s,θ)dΛx(s; θ)
dθ
)⏐⏐⏐
t
t0
=
∫ t
t0
(1 + κ(s,θ))f′
θ(s,Λx(s; θ); x,θ)ds
+
∫ t
t0
dΛx(s; θ)
dθ
(dκ(s,θ)
ds + (1 +κ(s,θ))f′
Λ(s,Λx(s; θ); x,θ)
)
ds
+ (1 +κ(t0,θ))c′
θ(x,θ) −κ(t,θ)dΛx(t; θ)
dθ .
Denote ˜κ(t,θ) ≜ κ(t,θ) + 1 and choose proper ˜κ(t,θ) that satisﬁes
{
d˜κ(t,θ)/dt= −˜κ(t; θ)f′
Λ(t,Λx(t,θ); x,θ)
˜κ(y; θ) = 1
, (S4)
then the gradient of Λ x(t,θ) with respect to θ is given by
dΛx(y; θ)
dθ =
∫ y
t0
˜κ(s,θ)f′
θ(s,Λx(s; θ); x,θ)ds+ ˜κ(t0,θ)c′
θ(x,θ).
After plugging t0 = 0 and c(·) = 0, the above equation becomes
dΛx(y; θ)
dθ =
∫ y
0
˜κ(s,θ)f′
θ(s,Λx(s; θ); x,θ)ds.
Together with (S4), it shows that the solution of (10) at t = 0 gives the gradient of Λ x(y; θ) with
respect to θ. Note that to solve (10) at t = 0, it requires evaluating the entire trajectory of Λ x(t,θ)
from y to 0. In our implementation, we combine ODEs (S1) and (10) into a larger ODE system, i.e.,
{
(Λ′(t); κ′(t); F′
2(t)) = (f(t,Λ; θ); −κ·f′
Λ(t,Λ; θ); −κ·f′
θ(t,Λ; θ))
(Λ(t); κ(t); F2(t))|t=y = (Λx(y; θ); 1;0)
,
and evaluate it at t = 0, where Λ x(y; θ) is available when computing the likelihood function. As
discussed in Section 3 of the main text, the proposed estimation methods can be easily implemented
using existing computing packages.
3
1.1 Acceleration trick for simultaneously solving separable ODEs
for n independent observations
Recall that evaluating the log-likelihood function requires solving ODEs for n independent observa-
tions. For a general ODE model, as suggested in Remark 4, we can use either the adjoint method
along with parallel computing or the forward method by combining nODEs into a large ODE system
with n dimensions. The complexity of both methods scales linearly with the sample size. We further
introduce a trick to reduce the absolute magnitude of computing time for separable ODEs, which cover
the general class of ODE models in (2) as a special case.
Speciﬁcally, we consider the separable ODE model in the form of
{
dΛx(t)/dt= f1(t; x,θ1) ·f2(Λx; θ2)
Λx(t0) = c
, (S5)
with two functions f1 and f2. In particular, for the general class of ODE models in (8), f1(t; x,z,θ 1) =
exp
(
xTβ+ ∑d2
l=0
∑q1
n
j=1 al
jB1
j(t)zl
)
and f2(Λx,z; θ2) = exp
(∑q2
n
j=1 bjB2
j(Λx,z(t))
)
. For n independent
observations {∆i,Xi,Yi}n
i=1, we need to evaluate the solution of n diﬀerent ODEs in (S5), each of
which is associated with Xi, at their respective observed times Yi. The acceleration trick is based on
the key observation that solving (S5) at y is equivalent to solving the problem
{
dG(t)/dt= f2(G; θ2)
G(t0) = c
(S6)
at
∫y
t0
f1(t; x,θ1)dt+ t0, i.e.,
Λx(y; θ1,θ2) = G(
∫ y
t0
f1(t; x,θ1)dt+ t0; θ2).
Therefore, we can instead solve a single ODE (S6) at ndiﬀerent points {
∫Yi
t0
f1(t; Xi,θ1)dt+ t0}n
i=1 to
compute ΛXi(Yi; θ1,θ2) for 1 ≤i ≤n. Moreover, given Λ Xi(Yi; θ1,θ2), the gradient of Λ Xi(Yi; θ1,θ2)
with respect to θ1 can be computed by
∂ΛXi(Yi; θ1,θ2)
∂θ1
= f2(ΛXi(Yi; θ1,θ2); θ2)
∫ Yi
t0
∂f1(t; Xi,θ1)
∂θ1
dt.
And we can obtain the gradient of Λ Xi(Yi; θ1,θ2) with respect to θ2 by solving another single ODE at
n diﬀerent points:
∂ΛXi(Yi; θ1,θ2)
∂θ2
= G2(
∫ y
t0
f1(t; x,θ1)dt+ t0; θ2),
4
where ˜G(·; θ2) is the solution of
{
d ˜G(t)/dt= f2′
θ2 (G; θ2) + f2′
G(G; θ2) ·˜G
˜G(t0) = 0
.
Based on our experiments, the proposed acceleration trick can signiﬁcantly reduce the absolute com-
puting time of simultaneously solving separable ODEs for n independent observations.
2 The General Sieve M-theorem for Bundled Parame-
ters (Theorem 3) and Its Proof
In this section, we establish a new general sieve M-theorem for studying the asymptotic normality of
M-estimators when the estimation criterion is parameterized with more general bundled parameters.
Note that the proposed M-theorem signiﬁcantly diﬀers from Theorem 2.1 in Ding and Nan (2011) and
Theorem 6.1 in Wellner and Zhang (2007). They consider either well-separated parameters (Wellner
and Zhang, 2007) or bundled parameters where the nuisance parameter can be a function of only the
ﬁnite-dimensional parameters (Ding and Nan, 2011); while we consider a more general scenario of
bundled parameters where the nuisance parameter can be a function of both the ﬁnite-dimensional
parameter β and other inﬁnite-dimensional parameters. Therefore, the proposed theorem nontrivially
extends the asymptotic distributional theories for M-estimation under this general scenario and is
crucial for studying the asymptotic normality of the sieve MLE for the general ODE model in (2).
Speciﬁcally, given i.i.d. observations W1,··· ,Wn ∈W, we maximize an objective function
1
n
n∑
1
m(β,γ(·),ζ(·,β, γ); Wi)
to estimate the unknown parameters ( β,γ(·),ζ(·,β, γ)). Here β ∈Rd1 denotes the ﬁnite-dimensional
parameter of interest, γ(·) = (γ1(·),...,γ d2 (·)) denotes nuisance inﬁnite-dimensional parameters and
ζ(·,β, γ) denotes another nuisance inﬁnite-dimensional parameter that is a function of β and γ(·). To
accommodate this diﬀerent and challenging scenario bundled parameters, we develop a new general
sieve M-theorem. We ﬁrstly introduce notation in Section 2.1, and establish the asymptotic normality
of the sieve estimator that maximizes the objective function over some sieve parameter space in
Section 2.2.
2.1 Notation
Here we follow notation used in Ding and Nan (2011) and Wellner and Zhang (2007). Let θ =
(β,γ(·),ζ(·,β, γ)), β ∈B⊂ Rd1 , γ∈Γd2 , and ζ ∈H, where Bis the parameter space of β, Γ is a class
of functions mapping from Wto R and His a class of functions mapping from W×B× Γd2 to R.
5
Let Θ = B×Γd2 ×H be the parameter space of θ. The distance between θ1 and θ2 ∈Θ is deﬁned as
d(θ1,θ2) = {∥β1 −β2∥2 +
d2∑
j=1
∥γ1j −γ2j∥2
Γ + ∥ζ1(·,β1,γ1) −ζ2(·,β2,γ2)∥2
H}1/2,
where ∥·∥ is the Euclidean norm, ∥·∥Γ is some norm of Γ, and ∥·∥His some norm of H. Let Θ n be the
sieve parameter space, where Θ n ⊂Θn+1 ⊂···⊂ Θ and the sequence becomes dense as n→∞. We
obtain the sieve M-estimator ˆθn = ( ˆβn,ˆγn,ˆζn(·,ˆβn,ˆγn)) ∈Θn by maximizing the objective function
over the sieve parameter space. We study the asymptotic normality of the sieve M-estimator of the
Euclidean parameter of interest, ˆβn, as follows.
For any ﬁxed γ(·) ∈Γ, let {γη(·) : η in a neighborhood of 0 ∈R}be a smooth curve in Γ running
through γ(·) at η= 0, that is γη(·)|η=0 = γ(·). Similarly, for any ﬁxed ζ(·,β, γ) ∈H, let {ζη(·,β, γ) : η
in a neighborhood of 0 ∈R}be a smooth curve in Hrunning through ζ(·,β, γ) at η = 0, that is
ζη(·,β, γ)|η=0 = ζ(·,β, γ). Assume all ζ(·,β, γ) ∈H are twice Frechet diﬀerentiable with respect to β
and γ, and denote
V = {v: v(·) = ∂γη(·)
∂η |η=0,γη ∈Γ},
H = {h: h(·,β, γ) = ∂ζη(·,β, γ)
∂η |η=0,ζη ∈H,β ∈B,γ∈Γd2 }.
Assume the objective function m is twice Frechet diﬀerentiable. For 1 ≤j ≤d2, we use the subscript
1, 2(j) or 3 in the derivatives to indicate that the derivatives are taken with respect to the ﬁrst, the
j-th component of the second or the third argument of the function, respectively. We use function v
or h inside the square brackets to denote the direction of the functional derivative with respect to γj
or ζ. Since for a small δ, we have ζ(·,β + δ,γ) −ζ(·,β, γ) = ζ′
β(·,β, γ)δ+ o(δ), where ζ′
β(·,β, γ) =
∂ζ(·,β, γ)/∂β; then as shown in Ding and Nan (2011) on page 3036, it follows that
lim
δ→0
1
δ{m(β,γ(·),ζ(·,β + δ,γ); W) −m(β,γ(·),ζ(·,β, γ); W)}
= m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′
β(·,β, γ)],
lim
δ→0
1
δ{m′
3(β,γ(·),ζ(·,β + δ,γ); W)[h(·,β, γ)] −m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)]}
= m′′
33(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ),ζ′
β(·,β, γ)],
lim
δ→0
1
δ{m′
2(j) (β,γ(·),ζ(·,β + δ,γ); W)[v] −m′
2(β,γ(·),ζ(·,β, γ); W)[v]}
= m′′
2(j)3(β,γ(·),ζ(·,β, γ); W)[v,ζ′
β(·,β, γ)], for 1 ≤j ≤d2,
6
and
lim
δ→0
1
δ{m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β + δ,γ)] −m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)]}
= m′
3(β,γ(·),ζ(·,β, γ); W)[h′
β(·,β, γ)].
Let ej = (0,..., 1,..., 0) ∈Rd2 with the j-th element being 1. For 1 ≤j ≤d2, we have ζ(·,β, γ+
v·ej)−ζ(·,β, γ) = ζ′
γj(·,β, γ)[v]+ o(∥v∥Γ) for a small v; then by the deﬁnition of functional derivatives,
it follows that, for 1 ≤j ≤d2,
m(β,γ(·),ζ(·,β, γ+ v·ej); W) −m(β,γ(·),ζ(·,β, γ); W)
= m(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v] + o(∥v∥Γ); W) −m(β,γ(·),ζ(·,β, γ); W)
= {m(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v] + o(∥v∥Γ); W) −m(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v]; W)}
+ {m(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v]; W) −m(β,γ(·),ζ(·,β, γ); W)}
= m′
3(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v]; W)[o(∥v∥Γ)]+
m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v]] + o(∥ζ′
γj(·,β, γ)[v]∥Γ)
= m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v]] + o(∥v∥Γ),
where the last equality holds because
lim
v→0
m(β,γ(·),ζ(·,β, γ) + ζ′
γj(·,β, γ)[v]; W)
[o(∥v∥Γ)
∥v∥Γ
]
= 0,
and o(∥ζ′
γj(·,β, γ)[v]∥Γ) = o(∥v∥Γ) for bounded functional derivatives. Similarly we have for 1 ≤j,ℓ ≤
d2,
m′
2(j) (β,γ(·),ζ(·,β, γ+ v·eℓ); W)[v1] −m′
2(j) (β,γ(·),ζ(·,β, γ); W)[v1]
= m′′
2(j)3(β,γ(·),ζ(·,β, γ); W)[v1,ζ′
γℓ(·,β, γ)[v]] + o(∥v∥Γ),
m′
3(β,γ(·),ζ(·,β, γ+ v·ej); W)[h(·,β, γ)] −m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)]
= m′′
33(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ),ζ′
γj(·,β, γ)[v]] + o(∥v∥Γ),
m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ+ v·ej)] −m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)]
= m′
3(β,γ(·),ζ(·,β, γ); W)[h′
γj(·,β, γ)[v]] + o(∥v∥Γ).
7
Based on the chain rule of the functional derivative, we have for 1 ≤j,ℓ ≤d2,
m′
β(β,γ(·),ζ(·,β, γ); W) = ∂m(β,γ(·),ζ(·,β, γ); W)
∂β
= m′
1(β,γ(·),ζ(·,β, γ); W) + m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′
β(·,β, γ)],
m′
γj(β,γ(·),ζ(·,β, γ); W)[v] = m′
2(j) (β,γ(·),ζ(·,β, γ); W)[v] + m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v]],
m′
ζ(β,γ(·),ζ(·,β, γ); W)[h] = m′
3(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)],
m′′
ββ(β,γ(·),ζ(·,β, γ); W) =
∂m′
β(β,γ(·),ζ(·,β, γ); W)
∂β
= m′′
11(β,γ(·),ζ(·,β, γ); W) + m′′
13(β,γ(·),ζ(·,β, γ); W)[ζ′
β(·,β, γ)]
+ m′′
31(β,γ(·),ζ(·,β, γ); W)[ζ′
β(·,β, γ)]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[ζ′
β(·,β, γ),ζ′
β(·,β, γ)]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′′
ββ(·,β, γ)],
m′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[v1,v2] = m′′
2(j)2(ℓ) β,γ(·),ζ(·,β, γ); W)[v1,v2]
+ m′′
2(j)3(β,γ(·),ζ(·,β, γ); W)[v1,ζ′
γℓ(·,β, γ)[v2]]
+ m′′
32(ℓ) (β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v1],v2]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v1],ζ′
γℓ(·,β, γ)[v2]]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′′
γjγℓ(·,β, γ)[v1,v2]],
m′′
ζζ(β,γ(·),ζ(·,β, γ); W)[h1,h2] = m′′
33(β,γ(·),ζ(·,β, γ); W)[h1(·,β, γ),h2(·,β, γ)],
m′′
γjβ(β,γ(·),ζ(·,β, γ); W)[v] =
∂m′
γj(β,γ(·),ζ(·,β, γ); W)[v]
∂β
= m′′
2(j)1(β,γ(·),ζ(·,β, γ); W)[v]
+ m′′
2(j)3(β,γ(·),ζ(·,β, γ); W)[v,ζ′
β(·,β, γ)]
+ m′′
31(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v]]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v],ζ′
β(·,β, γ)]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[ζ′′
γjβ(·,β, γ)[v]]
m′′
ζβ(β,γ(·),ζ(·,β, γ); W)[h] =
∂m′
ζ(β,γ(·),ζ(·,β, γ); W)[h]
∂β
= m′′
31(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ)]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ),ζ′
β(·,β, γ)]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[h′
β(·,β, γ)],
8
m′′
ζγj(β,γ(·),ζ(·,β, γ); W)[h,v] = m′′
32(j) (β,γ(·),ζ(·,β, γ); W)[h(·,β, γ),v]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[h(·,β, γ),ζ′
γj(·,β, γ)[v]]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[h′
γj(·,β, γ)[v]],
m′′
γjζ(β,γ(·),ζ(·,β, γ); W)[v,h] = m′′
2(j)3(β,γ(·),ζ(·,β, γ); W)[v,h(·,β, γ)]
+ m′′
33(β,γ(·),ζ(·,β, γ); W)[ζ′
γj(·,β, γ)[v],h(·,β, γ)]
+ m′
3(β,γ(·),ζ(·,β, γ); W)[h′
γj(·,β, γ)[v]].
Following Wellner and Zhang (2007), we further deﬁne
S′
β(β,γ(·),ζ(·,β, γ)) = Pm′
β(β,γ(·),ζ(·,β, γ); W),
S′
γj(β,γ(·),ζ(·,β, γ))[v] = Pm′
γj(β,γ(·),ζ(·,β, γ); W)[v],
S′
ζ(β,γ(·),ζ(·,β, γ))[h] = Pm′
ζ(β,γ(·),ζ(·,β, γ); W)[h],
S′
β,n(β,γ(·),ζ(·,β, γ)) = Pnm′
β(β,γ(·),ζ(·,β, γ); W),
S′
γj,n(β,γ(·),ζ(·,β, γ))[v] = Pnm′
γj(β,γ(·),ζ(·,β, γ); W)[v],
S′
ζ,n(β,γ(·),ζ(·,β, γ))[h] = Pnm′
ζ(β,γ(·),ζ(·,β, γ); W)[h],
S′′
ββ(β,γ(·),ζ(·,β, γ)) = Pm′′
ββ(β,γ(·),ζ(·,β, γ); W),
S′′
γjγℓ(β,γ(·),ζ(·,β, γ))[v1,v2] = Pm′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[v1,v2],
S′′
ζζ(β,γ(·),ζ(·,β, γ))[h1,h2] = Pm′′
ζζ(β,γ(·),ζ(·,β, γ); W)[h1,h2],
S′′
γjβ(β,γ(·),ζ(·,β, γ))[v] = S′′
βγj(β,γ(·),ζ(·,β, γ))[v] = Pm′′
γjβ(β,γ(·),ζ(·,β, γ); W)[v],
S′′
ζβ(β,γ(·),ζ(·,β, γ))[h] = S′′
βζ(β,γ(·),ζ(·,β, γ))[h] = Pm′′
ζβ(β,γ(·),ζ(·,β, γ); W)[h],
S′′
ζγj(β,γ(·),ζ(·,β, γ))[h,v] = Pm′′
ζγj(β,γ(·),ζ(·,β, γ); W)[h,v],
S′′
γjζ(β,γ(·),ζ(·,β, γ))[v,h] = Pm′′
γjζ(β,γ(·),ζ(·,β, γ); W)[v,h].
Furthermore, for h = (h1,··· ,hd1 )T ∈Hd1 and v = (v1,··· ,vd1 )T ∈Vd1 , denote that
m′
γj(β,γ(·),ζ(·,β, γ); W)[v] = (m′
γj(β,γ(·),ζ(·,β, γ); W)[v1],··· ,m′
γj(β,γ(·),ζ(·,β, γ); W)[vd1 ])T,
m′
ζ(β,γ(·),ζ(·,β, γ); W)[h] = (m′
ζ(β,γ(·),ζ(·,β, γ); W)[h1],··· ,m′
ζ(β,γ(·),ζ(·,β, γ); W)[hd1 ])T,
m′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[v,v] = (m′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[v1,v],··· ,m′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[vd1 ,v])T,
m′′
ζζ(β,γ(·),ζ(·,β, γ); W)[h,h] = (m′′
ζζ(β,γ(·),ζ(·,β, γ); W)[h1,h],··· ,m′′
ζζ(β,γ(·),ζ(·,β, γ); W)[hd1 ,h])T,
9
m′′
γjβ(β,γ(·),ζ(·,β, γ); W)[v] = (m′′
γjβ(β,γ(·),ζ(·,β, γ); W)[v1],··· ,m′′
γjβ(β,γ(·),ζ(·,β, γ); W)[vd1 ])T,
m′′
ζβ(β,γ(·),ζ(·,β, γ); W)[h] = (m′′
ζβ(β,γ(·),ζ(·,β, γ); W)[h1],··· ,m′′
ζβ(β,γ(·),ζ(·,β, γ); W)[hd1 ])T,
m′′
ζγj(β,γ(·),ζ(·,β, γ); W)[h,v] = (m′′
ζγj(β,γ(·),ζ(·,β, γ); W)[h1,v],··· ,m′′
ζγj(β,γ(·),ζ(·,β, γ); W)[hd1 ,v])T,
m′′
γjζ(β,γ(·),ζ(·,β, γ); W)[v,h] = (m′′
γjζ(β,γ(·),ζ(·,β, γ); W)[v1,h],··· ,m′′
γjζ(β,γ(·),ζ(·,β, γ); W)[vd1 ,h])T.
We deﬁne correspondingly
S′
γj(β,γ(·),ζ(·,β, γ))[v] = Pm′
γj(β,γ(·),ζ(·,β, γ); W)[v],
S′
ζ(β,γ(·),ζ(·,β, γ))[h] = Pm′
ζ(β,γ(·),ζ(·,β, γ); W)[h],
S′
γj,n(β,γ(·),ζ(·,β, γ))[v] = Pnm′
γj(β,γ(·),ζ(·,β, γ); W)[v],
S′
ζ,n(β,γ(·),ζ(·,β, γ))[h] = Pnm′
ζ(β,γ(·),ζ(·,β, γ); W)[h],
S′′
γjγℓ(β,γ(·),ζ(·,β, γ))[v,v] = Pm′′
γjγℓ(β,γ(·),ζ(·,β, γ); W)[v,v],
S′′
ζζ(β,γ(·),ζ(·,β, γ))[h,h] = Pm′′
ζζ(β,γ(·),ζ(·,β, γ); W)[h,h],
S′′
γjβ(β,γ(·),ζ(·,β, γ))[v] = Pm′′
γjβ(β,γ(·),ζ(·,β, γ); W)[v],
S′′
ζβ(β,γ(·),ζ(·,β, γ))[h] = Pm′′
ζβ(β,γ(·),ζ(·,β, γ); W)[h],
S′′
ζγj(β,γ(·),ζ(·,β, γ))[h,v] = Pm′′
ζγj(β,γ(·),ζ(·,β, γ); W)[h,v],
S′′
γjζ(β,γ(·),ζ(·,β, γ))[v,h] = Pm′′
γjζ(β,γ(·),ζ(·,β, γ); W)[v,h].
2.2 The general sieve M-theorem
Recall that the sieve M-estimator ˆθn = ( ˆβn,ˆγn,ˆζn(·,ˆβn,ˆγn)) ∈Θn maximizes the objective function
over the sieve parameter space Θn. Next, we establish the asymptotic normality of the sieve estimator
ˆβn. The key diﬀerence between the proposed new sieve M-theorem in this paper and Theorem 2.1 in
Ding and Nan (2011) is that the nuisance parameter ζ(·,β, γ) can be a function of not only Euclidean
parameter β but also other nuisance parameters γ(·).
To establish the asymptotic normality, we assume the following assumptions.
(A1) (Rate of convergence) For an estimator ˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) ∈Θn and the true param-
eter θ0 = (β0,γ0(·),ζ0(·,β0,γ0)) ∈Θ, d(ˆθn,θ0) = Op(n−ξ) for some positive ξ.
(A2) S′
β(β0,γ0(·),ζ0(·,β0,γ0)) = 0, S′
γj(β0,γ0(·),ζ0(·,β0,γ0))[v] = 0 for all v ∈Γp1 and 1 ≤j ≤d2,
and S′
ζ(β0,γ0(·),ζ0(·,β0,γ0))[h] = 0 for all h∈H.
(A3) (Positive information) There exists v∗
j = ( v∗
j1,··· ,v∗
jd1 )T ∈ Vd1 , 1 ≤ j ≤ d2, and h∗ =
10
(h∗
1,··· ,h∗
d1 )T ∈Hd1 such that for any v∈V and h∈H, 1 ≤ℓ≤d2
S′′
βγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v] =
d2∑
j=1
S′′
γjγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,v]
+ S′′
ζγℓ(β0,γ0(·),ζ0(·,β0,γ0))[h∗,v],
S′′
βζ(β0,γ0(·),ζ0(·,β0,γ0))[h] =
d2∑
j=1
S′′
γjζ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,h]
+ S′′
ζζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗,h].
Furthermore, the matrix
A= −S′′
ββ(β0,γ0(·),ζ0(·,β0,γ0)) +
d2∑
j=1
S′′
γjβ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j]
+ S′′
ζβ(β0,γ0(·),ζ0(·,β0,γ0))[h∗]
= −P{m′′
ββ(β0,γ0(·),ζ0(·,β0,γ0); W) +
d2∑
j=1
m′′
γjβ(β0,γ0(·),ζ0(·,β0,γ0); W)[v∗
j]
+ m′′
ζβ(β0,γ0(·),ζ0(·,β0,γ0); W)[h∗]}
is nonsingular.
(A4) The estimator ˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) satisﬁes S′
β,n( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) = op(n−1/2),
S′
γj,n( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn))[v∗
j] = op(n−1/2) for 1 ≤j ≤d2, and S′
ζ,n( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn))[h∗] =
op(n−1/2).
(A5) (Stochastic equicontinuity) For some positive C,
sup
d(θ,θ0)≤Cn−ξ,θ∈Θn
∥√n(S′
β,n −S′
β)(β,γ(·),ζ(·,β, γ))
−√n(S′
β,n −S′
β)(β0,γ0(·),ζ0(·,β0,γ0))∥= op(1),
sup
d(θ,θ0)≤Cn−ξ,θ∈Θn
|√n(S′
γj,n −S′
γj)(β,γ(·),ζ(·,β, γ))[v∗
j]
−√n(S′
γj,n −S′
γj)(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j]|= op(1), for 1 ≤j ≤d2,
and
sup
d(θ,θ0)≤Cn−ξ,θ∈Θn
|√n(S′
ζ,n −S′
ζ)(β,γ(·),ζ(·,β, γ))[h∗(·,β, γ)]
−√n(S′
ζ,n −S′
ζ)(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)]|= op(1).
(A6) (Smoothness of the model) For some α >1 with αξ >1
2 , and for θ ∈Θn satisfying d(θ,θ0) ≤
11
Cn−ξ,
∥S′
β(β,γ(·),ζ(·,β, γ)) −S′
β(β0,γ0(·),ζ0(·,β0,γ0)) −S′′
ββ(β0,γ0(·),ζ0(·,β0,γ0))(β−β0)
−
d2∑
j=1
S′′
βγj(β0,γ0(·),ζ0(·,β0,γ0))[ej(γ−γ0)T]
−S′′
βζ(β0,γ0(·),ζ0(·,β0,γ0))[ζ(·,β, γ) −ζ0(·,β0,γ0)]∥
= O(dα(θ,θ0)),
|S′
γj(β,γ(·),ζ(·,β, γ))[v∗
j] −S′
γj(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j] −S′′
γjβ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j](β−β0)
−
d2∑
ℓ=1
S′′
γjγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,eℓ(γ−γ0)T]
−S′′
γjζ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,ζ(·,β, γ) −ζ0(·,β0,γ0)]|
= O(dα(θ,θ0)), for 1 ≤j ≤d2,
and
|S′
ζ(β,γ(·),ζ(·,β, γ))[h∗(·,β, γ)] −S′
ζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)]
−S′′
ζβ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)](β−β0)
−
d2∑
j=1
S′′
ζγj(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0),ej(γ−γ0)T]
−S′′
ζζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0),ζ(·,β, γ) −ζ0(·,β0,γ0)]|
= O(dα(θ,θ0)).
The convergence rate in (A1) is a prerequisite for the asymptotic normality. Assumption (A2) is
a common regularity assumption when mis the likelihood function, and it usually holds for the score
functions. The direction v∗
j and h∗in (A3) are the least favorable directions for maximum likelihood
estimation, which may be found through solving the equations in (A3). Assumptions (A4) and (A5)
can be obtained by the maximal inequality in Lemma 3.4.2 of (Billingsley, 2008, page 324) and the
Markov’s inequality. Assumption (A6) can be usually veriﬁed by the Taylor expansion. We repeat
Theorem 3 below for readers’ convenience, which is a general sieve M-theorem for bundled parameters
where the nuisance parameter ζ(·,β, γ) is a function of the Euclidean parameter β and other nuisance
parameters γ(·).
Theorem. Suppose that assumptions (A1)-(A6) hold, then
√n( ˆβn −β0) = A−1√nPnm∗(β0,γ0(·),ζ0(·,β0,γ0); W) + op(1)
→d N(0,A−1B(A−1)T),
12
where
m∗(β0,γ0(·),ζ0(·,β0,γ0); W) = m′
β(β0,γ0(·),ζ0(·,β0,γ0); W) −
d2∑
j=1
m′
γj(β0,γ0(·),ζ0(·,β0,γ0); W)[v∗
j]
−m′
ζ(β0,γ0(·),ζ0(·,β0,γ0); W)[h∗(·,β0,γ0)],
B = P{m∗(β0,γ0(·),ζ0(·,β0,γ0); W)m∗(β0,γ0(·),ζ0(·,β0,γ0); W)T},
and A is given in the assumption (A3).
Proof of Theorem 3. We prove the theorem by following the proof of Theorem 6.1 in Wellner and
Zhang (2007) and Theorem 2.1 in Ding and Nan (2011). Assumptions (A1) and (A5) lead to
√n(S′
β,n −S′
β)( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) −√n(S′
β,n −S′
β)(β0,γ0(·),ζ0(·,β0,γ0)) = op(1).
Note that S′
β(β0,γ0(·),ζ0(·,β0,γ0)) = 0 by (A2), S′
β,n( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) = op(n−1/2) by (A4), we
have √nS′
β( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)) + √nS′
β,n(β0,γ0(·),ζ0(·,β0,γ0)) = op(1). (S7)
After combining the equation (S7) and the equations in assumptions (A2) and (A6), we have
S′
β,n(β0,γ0(·),ζ0(·,β0,γ0)) + S′′
ββ(β0,γ0(·),ζ0(·,β0,γ0))( ˆβn −β0)
+
d2∑
j=1
S′′
βγj(β0,γ0(·),ζ0(·,β0,γ0))[ej(ˆγn −γ0)T]
+ S′′
βζ(β0,γ0(·),ζ0(·,β0,γ0))[ˆζn(·,ˆβn,ˆγn) −ζ0(·,β0,γ0)]
= O(dα(ˆθn,θ0)) + op(n−1/2) = op(n−1/2).
The last equation holds because for α> 1 with αξ >1
2 , assumption (A1) implies that
O(dα(ˆθn,θ0)) = Op(n−αξ) = op(n−1/2).
Similarly, we have for 1 ≤j ≤d2
S′
γj,n(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j] + S′′
γjβ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j]( ˆβn −β0)
+
d2∑
ℓ=1
S′′
γjγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,eℓ(ˆγn −γ0)T]
+ S′′
γjζ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,ˆζn(·,ˆβn,ˆγn) −ζ0(·,β0,γ0)]
= op(n−1/2)
13
and
S′
ζ,n(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)] + S′′
ζβ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)]( ˆβn −β0)
+
d2∑
j=1
S′′
ζγj(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0),ej(ˆγn −γ0)T]
+ S′′
ζζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0),ˆζn(·,ˆβn,ˆγn) −ζ0(·,β0,γ0)]
= op(n−1/2).
Combining these equations with assumption (A3) leads to
{S′′
ββ(β0,γ0(·),ζ0(·,β0,γ0)) −
d2∑
j=1
S′′
γjβ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j]
−S′′
ζβ(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)]}( ˆβn −β0)
= −{S′
β,n(β0,γ0(·),ζ0(·,β0,γ0)) −
d2∑
j=1
S′
γj,n(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j]
−S′
ζ,n(β0,γ0(·),ζ0(·,β0,γ0))[h∗(·,β0,γ0)]}
+ op(n−1/2),
and equivalently,
−A( ˆβn −β0) = −Pnm∗(β0,γ0(·),ζ0(·,β0,γ0); W) + op(n−1/2).
Then under assumptions (A4) and (A5),
√n( ˆβn −β0) = A−1√nPnm∗(β0,γ0(·),ζ0(·,β0,γ0); W) + op(1)
→d N(0,A−1B(A−1)T).
3 Proof of Theorems 1 and 2
Without loss of generality, we prove Theorems 1 and 2 in the case that X(1) is not included in (12).
The results in this section still hold if X(1) is included due to the boundedness of X(1). For notational
simplicity, we further replace X(−1) by X in (12), which then becomes equivalent to the ODE in (11).
We ﬁrst introduce some common notations that will be used in the proof hereafter. For any ﬁxed
γ(·) ∈Γp1 , let {γη(·) : η in a neighborhood of 0 ∈R}be a smooth curve in Γ p1 running through γ(·)
at η = 0, that is γη(·)|η=0 = γ(·). Similarly, for any ﬁxed g(·) ∈Gp2 , let {gη(·) : η in a neighborhood
14
of 0 ∈R}be a smooth curve in Gp2 running through g(·) at η= 0, that is gη(·)|η=0 = g(·). Denote
V = {v: v(·) = ∂γη(·)
∂η |η=0,γη ∈Γp1 }
and
W = {w: w(·) = ∂gη(·)
∂η |η=0,gη ∈Gp2 }.
Recall that Λ 0(t,x) = Λ(t,x,β 0,γ0,g0) and R(t) =
∫t
0 exp(γ0(s))ds. Let ˜Λ0(t) denote the solution of
˜Λ′
0(t) = exp
(
g0(˜Λ0)
)
with ˜Λ0(0) = 0. It is straightforward to show that ˜Λ0(·) is the cumulative hazard
function of R(T)eXTβ0 and Λ0(t,X) = ˜Λ0(R(t)eXTβ0 ). We use symbol ≳ to denote that the left side
is bounded below by a constant times the right side. We also use symbol ≲ to denote that the left
side is bounded above by a constant times the right side. If without further explanation, by default,
the L2 norm of a function f(·) of t and x is given by
∥f(·)∥2 =
[∫
X
∫ τ
0
(f(t,x))2dΛ0(t,x)dFX(x)
]1/2
,
and the supreme norm is given by ∥f(·)∥∞= supt∈[0,τ],x∈X|f(t,x)|. For any g ∈Gp2 , the L2 norm is
given by ∥g∥2 = (
∫µ
0 (g(t))2dt)1/2 and the supreme norm is given by ∥g∥∞= supt∈[0,µ] |g(t)|.
The rest of this section is structured as follows. Subsection 3.1 introduces several lemmas which
will be used to prove Theorem 1 and 2. Subsections 3.2 and 3.3 provide the proof of Theorem 1
by checking the conditions C1-3 in Shen and Wong (1994, Theorem 1) and the proof of Theorem 2
by verifying assumptions (A1)-(A6) of the proposed general M-theorem, respectively. Furthermore,
we derive in subsection 3.4 the equivalent but more feasible equations for ﬁnding the least favorable
directions required in condition (C7) and provide explicit constructions for the Cox model and the
linear transformation model with a known transformation as illustration. Subsequently, we simplify
the non-regularity assumption in Condition (C8) in subsection 3.5.
3.1 Lemmas
Lemma 1. (Existence and uniqueness theorem.) For any x ∈ X,β ∈ B,γ ∈Γp1 ,g ∈ Gp2 under
conditions (C1)-(C4), the initial value problem (11) has exactly one bounded and continuous solution
Λ(t,x,β,γ,g ) on [0,τ]. And its ﬁrst and second derivatives with respect to β ∈B,γ ∈Γp1 and the ﬁrst
derivative with respect to g∈Gp2 are also bounded and continuous on [0,τ].
Proof of Lemma 1. Let f(t,Λ) = exp
(
xTβ+ γ(t) + g(Λ)
)
, then by the mean value theorem
|f(t,Λ) −f(t,˜Λ)|≤ exp
(
xTβ+ γ(t) + g(c)
)
|g′(c)|·|Λ −˜Λ|≤ L|Λ −˜Λ|
holds for any ( t,Λ) and ( t,˜Λ) in [0 ,τ] ×[0,µ], where c ∈[Λ,˜Λ] and L <∞under conditions (C1)-
(C4). This implies that f(t,Λ) satisﬁes the Lipschitz condition with respect to Λ in [0 ,τ] ×[0,µ]. By
Theorem 10.VI in Walter (1998, page 108), there is exactly one solution to the initial value problem
15
(11). The solution Λ( t,x,β,γ,g ) is bounded, continuous, and satisﬁes
Λ(t,x,β,γ,g ) =
∫ t
0
exp
(
xTβ+ γ(s) + g(Λ(s,x,β,γ,g ))
)
ds. (S8)
In the following, we write Λ( t) = Λ(t,x,β,γ,g ) for simplicity. Similarly to the above derivation, for
any β ∈B,v ∈V,w ∈W, we have unique, bounded, and continuous solutions of the following initial
value problems:
dΛ′
β(t)
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{x+ g′(Λ(t))Λ′
β(t)}, Λ′
β(0) = 0, (S9)
dΛ′
γ(t)[v]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{v(t) + g′(Λ(t))Λ′
γ(t)[v]}, Λ′
γ(0)[v] = 0, (S10)
dΛ′
g(t)[w]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{w(Λ(t)) + g′(Λ(t))Λ′
g(t)[w]}, Λ′
g(0)[w] = 0, (S11)
dΛ′′
ββ(t)
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{[x+ g′(Λ(t))Λ′
β(t)][x+ g′(Λ(t))Λ′
β(t)]T
+ g′′(Λ(t))Λ′
β(t)Λ′
β(t)T + g′(Λ(t))Λ′′
ββ(t)}, Λ′′
ββ(0) = 0, (S12)
dΛ′′
γγ(t)[v1,v2]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{(v1(t) + g′(Λ(t))Λ′
γ(t)[v1])(v2(t) + g′(Λ(t))Λ′
γ(t)[v2])
+ g′′(Λ(t))Λ′
γ(t)[v1]Λ′
γ(t)[v2]
+ g′(Λ(t))Λ′′
γγ(t)[v1,v2]}, Λ′′
γγ(0)[v1,v2] = 0, (S13)
dΛ′′
βγ(t)[v]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{(v(t) + g′(Λ(t))Λ′
γ(t)[v])(x+ g′(Λ(t))Λ′
β(t))
+ g′′(Λ(t))Λ′
β(t)Λ′
γ(t)[v]
+ g′(Λ(t))Λ′′
βγ(t)[v]}, Λ′′
βγ(0)[v] = 0, (S14)
dΛ′′
gβ(t)[w]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{(w(Λ(t)) + g′(Λ(t))Λ′
g(t)[w])(x+ g′(Λ(t))Λ′
β(t))
+ w′(Λ(t))Λ′
β(t) + g′′(Λ(t))Λ′
β(t)Λ′
g(t)[w]
+ g′(Λ(t))Λ′′
gβ(t)[w]}, Λ′′
gβ(0)[w] = 0, (S15)
dΛ′′
gγ(t)[w,v]
dt = exp
(
xTβ+ γ(t) + g(Λ(t))
)
{(w(Λ(t)) + g′(Λ(t))Λ′
g(t)[w])(v(t) + g′(Λ(t))Λ′
γ(t)[v])
+ w′(Λ(t))Λ′
γ(t)[v] + g′′(Λ(t))Λ′
γ(t)[v]Λ′
g(t)[w]
+ g′(Λ(t))Λ′′
gγ(t)[w,v]}, Λ′′
gγ(0)[w,v] = 0. (S16)
Next we verify that the derivative of Λ( t,x,β,γ,g ) with respect to β follows the ODE (S9). By
16
plugging in Equation (S8) and (S9), it follows that
lim sup
δ→0
1
|δ||Λ(t,x,β + δ,γ,g ) −Λ(t) −Λ′
β(t)Tδ|
= lim sup
δ→0
1
|δ||
∫ t
0
exp
(
xT(β+ δ) + γ(s) + g(Λ(s,x,β + δ,γ,g ))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
(xTδ+ g′(Λ(s))Λ′
β(s)Tδ)ds
⏐⏐
≤lim sup
δ→0
1
|δ|
∫ t
0
|exp
(
xT(β+ δ) + γ(s) + g(Λ(s,x,β + δ,γ,g ))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
(xTδ+ g′(Λ(s))Λ′
β(s)Tδ)
⏐⏐ds
≤
∫ t
0
lim sup
δ→0
1
|δ||exp
(
xT(β+ δ) + γ(s) + g(Λ(s,x,β + δ,γ,g ))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
−exp
(
xTβ+ γ(s) + g(Λ(s))
)
(xTδ+ g′(Λ(s))Λ′
β(s)Tδ)
⏐⏐ds
=
∫ t
0
exp
(
xTβ+ γ(s) + g(Λ(s))
)
·g′(Λ(s)){lim sup
δ→0
1
|δ||Λ(s,x,β + δ,γ,g ) −Λ(s) −Λ′
β(s)Tδ|}ds
where the second inequality holds due to the reverse Fatou’s lemma. Using the Gronwall’s inequality,
we have that
lim sup
δ→0
1
|δ||Λ(t,x,β + δ,γ,g ) −Λ(t) −Λ′
β(t)Tδ|≤ 0,
which implies that the solution Λ ′
β(t) of (S9) is the derivative of Λ( t,x,β,γ,g ) with respect to β. The
other ﬁrst and second derivatives of of Λ( t,x,β,γ,g ) with respect to β,γ,g can be similarly derived,
and we omit the details here.
Lemma 2.Let ψ(t,x,β,γ,g ) = log λ(t,x,β,γ,g ) = xTβ+γ(t)+ g(Λ(t,x,β,γ,g )), and denote the ﬁrst
derivatives of ψ(t,x,β,γ,g ) with respect to γ and gat the true parameter (β0,γ0,g0) by ψ′
0γ(t,x)[v] and
ψ′
0g(t,x)[w], respectively. For any ψ′
0γ(·)[v] ∈Eγ = {ψ′
0γ(·)[v] : ψ′
0γ(t,x)[v],t ∈[0,τ],x ∈X,v ∈Γp1 },
the L2 norm of ψ′
0γ(·)[v] is deﬁned as
∥ψ′
0γ(·)[v]∥2 =
[∫
X
∫ τ
0
(ψ′
0γ(t,x)[v])2dΛ0(t,x)dFX(x)
]1/2
.
The L2 norm of ψ′
0g(·)[w] ∈Eg = {ψ′
0g(·)[w] : ψ′
0g(t,x)[w],t ∈[0,τ],x ∈X ,w ∈G p2 }is similarly
deﬁned. Under conditions (C2)-(C4), ψ′
0γ[·] : v →ψ′
0γ(·)[v] and ψ′
0g[·] : w →ψ′
0g(·)[w] are bounded
linear operators (from Γp1 to Eγ and from Gp2 to Eg). In particular, the operators ψ′
0γ[·] and ψ′
0g[·] are
bounded from below, i.e.,
∥ψ′
0γ(·)[v]∥2 ≳ ∥v∥2, for any v∈Γp1 , (S17)
17
and
∥ψ′
0g(·)[w]∥2 ≳ ∥w∥2, for any w∈Gp2 . (S18)
Proof of Lemma 2. By solving initial value problems in (S10)-(S11), the ﬁrst derivatives ofψ(t,x,β,γ,g )
with respect to γ and g at the true parameter ( β0,γ0,g0) are given by
ψ′
0γ(t,x)[v] = g′
0(Λ0(t,x))Λ′
0γ(t,x)[v] + v(t)
= g′
0(Λ0(t,x)) exp(g0(Λ0(t,x)))exTβ0
∫ t
0
exp(γ0(s))v(s)ds+ v(t), (S19)
ψ′
0g(t,x)[w] = g′
0(Λ0(t,x))Λ′
0g(t,x)[w] + w(Λ0(t,x))
= g′
0(Λ0(t,x)) exp(g0(Λ0(t,x)))
∫ Λ0(t,x)
0
exp(−g0(s))w(s)ds+ w(Λ0(t,x)), (S20)
We ﬁrst verify that ψ′
0γ[·] is a bounded linear operator. Using ( a+ b)2 ≤2(a2 + b2), the L2 norm
of ψ′
0γ(·)[v] is bounded by
∥ψ′
0γ(·)[v]∥2
2 ≤2
∫
X
∫ τ
0
(
g′
0(Λ0(t,x)) exp(g0(Λ0(t,x)))exTβ0
∫ t
0
exp(γ0(s))v(s)ds
)2
dΛ0(t,x)dFX(x)
+ 2
∫
X
∫ τ
0
v(t)2dΛ0(t,x)dFX(x)
=2
∫
X
∫ τ
0
(g′
0(Λ0(t,x)))2 exp(2g0(Λ0(t,x)))e2xTβ0
(∫ t
0
exp(γ0(s))v(s)ds
)2
dΛ0(t,x)dFX(x)
+ 2
∫
X
∫ τ
0
v(t)2dΛ0(t,x)dFX(x). (S21)
By the Cauchy-Schwarz inequality, we have for t∈[0,τ]
(∫ t
0
exp(γ0(s))v(s)ds
)2
≤
∫ t
0
(v(s))2 ds
∫ t
0
exp(2γ0(s))ds
≤
∫ τ
0
(v(s))2 ds
∫ τ
0
exp(2γ0(s))ds
≤∥v∥2
2τe2c1 ,
where c1 = maxs∈[0,τ] γ0(s) <∞under (C4). It follows that the ﬁrst term in (S21) is bounded above
by
2∥v∥2
2τe2c1 ·
∫
X
∫ τ
0
(g′
0(Λ0(t,x)))2 exp(2g0(Λ0(t,x)))e2xTβ0 dΛ0(t,x)dFX(x) ≲ ∥v∥2
2,
18
because the integral is ﬁnite under (C2)-(C4). The second term in (S21) is also bounded by
2
∫
X
∫ τ
0
v(t)2dΛ0(t,x)dFX(x)
=2
∫
X
∫ τ
0
exp
(
xTβ0 + γ0(t) + g0(Λ0(t,x))
)
v(t)2dtdFX(x)
≤2
∫
X
∫ τ
0
c2v(t)2dtdFX(x) = 2c2∥v∥2
2,
where c2 = maxt∈[0,τ],x∈Xexp
(
xTβ0 + γ0(t) + g0(Λ0(t,x))
)
<∞under (C2)-(C4). Therefore, ∥ψ′
0γ(·)[v]∥2 ≲
∥v∥2 for any v∈Γp1 .
Similarly, we can show that ψ′
0g[·] is a bounded linear operator by
∥ψ′
0g(·)[w]∥2
2 =
∫
X
∫ τ
0
(ψ′
0g(t,x)[w])2dΛ0(t,x)dFX(x)
=
∫
X
∫ Λ0(τ,x)
0
(
g′
0(t) exp(g0(t))
∫ t
0
exp(−g0(s))w(s)ds+ w(t)
)2
dtdFX(x)
≲
∫
X
∫ Λ0(τ,x)
0
(
g′
0(t) exp(g0(t))
∫ t
0
exp(−g0(s))w(s)ds
)2
dtdFX(x)
+
∫
X
∫ Λ0(τ,x)
0
(w(t))2 dtdFX(x)
≲
∫
X
∫ Λ0(τ,x)
0
(g′
0(t))2 exp(2g0(t))
(∫ t
0
exp(−2g0(s))ds
)(∫ t
0
(w(s))2ds
)
dtdFX(x)
+
∫
X
∫ Λ0(τ,x)
0
(w(t))2 dtdFX(x)
≲
∫ µ
0
(g′
0(t))2 exp(2g0(t))
(∫ t
0
exp(−2g0(s))ds
)(∫ t
0
(w(s))2ds
)
dt
+
∫ µ
0
(w(t))2 dt,
where the second last inequality holds by the Cauchy-Schwarz inequality and µ = maxx∈XΛ0(τ,x)
given in condition (C3). The ﬁrst term is further bounded by
(∫ µ
0
(w(t))2dt
)∫ µ
0
(g′
0(t))2 exp(2g0(t))
(∫ t
0
exp(−2g0(s))ds
)
dt≲
∫ µ
0
(w(t))2dt= ∥w∥2
2,
since the second integral is ﬁnite under conditions (C2)-(C4). Thus, ∥ψ′
0g(·)[w]∥2 ≲ ∥w∥2 for any
w∈Gp2 .
Next, we show that linear operators ψ′
0γ[·] and ψ′
0g[·] are bijective functions. Suppose that
ψ′
0γ(·)[v1] = ψ′
0γ(·)[v2] ∈Eγ holds almost surely with respect to the measure ρ(t,x) = Λ0(t,x)×FX(x).
Using the ODE in (S10), we have
vi(t) = ψ′
0γ(t,x)[vi] −g′
0(Λ0(t,x))
∫ t
0
ψ′
0γ(s,x)[vi]dΛ0(s,x), for i= 1,2,
19
and then v1 = v2 almost surely with respect to ρ, i.e.,
∫
X
∫τ
0 (v1(t) −v2(t))2dρ(t,x) = 0. It follows that
∫
X
∫ τ
0
(v1(t) −v2(t))2dρ(t,x) =
∫
X
∫ τ
0
exp
(
xTβ0 + γ0(t) + g0(Λ0(t,x))
)
(v1(t) −v2(t))2dtdFX(x)
≥
∫
X
∫ τ
0
c3(v1(t) −v2(t))2dtdFX(x) = c3∥v1 −v2∥2
2,
where c3 = mint∈[0,τ],x∈Xexp
(
xTβ0 + γ0(t) + g0(Λ0(t,x))
)
< ∞under (C2)-(C4), which implies that
ψ′
0γ[·] is a bijective function from Γ p1 to Eγ.
Similarly, suppose thatψ′
0g(·)[w1] = ψ′
0g(·)[w2] ∈Eg holds almost surely with respect to the measure
ρ(t,x). Using the ODE in (S11), we have
wi(Λ0(t,x)) = ψ′
0g(t,x)[wi] −g′
0(Λ0(t,x))
∫ t
0
ψ′
0g(s,x)[wi]dΛ0(s,x), for i= 1,2,
and then w1(Λ0(t,x)) = w2(Λ0(t,x)) almost surely with respect to ρ. It follows that
0 =
∫
X
∫ τ
0
(w1(Λ0(t,x)) −w2(Λ0(t,x)))2dρ(t,x)
=
∫
X
∫ Λ0(τ,x)
0
(w1(t) −w2(t))2dtdFX(x)
≳
∫ supx∈XΛ0(τ,x)
0
(w1(t) −w2(t))2dt= ∥w1 −w2∥2
2,
where the last inequality holds under condition (C2). So w1 = w2 ∈G p2 and ψ′
0g[·] is a bijective
function from Gp1 to Eg.
By bounded inverse theorem, it follows that the bijective bounded linear operatorsψ′
0γ[·] and ψ′
0g[·]
have bounded inverse operator ( ψ′
0γ)−1[·] and ( ψ′
0g)−1[·]. Then, there is a constant 0 < L <∞such
that
∥v∥2 = ∥(ψ′
0γ)−1 [
ψ′
0γ(·)[v]
]
∥2 ≤L∥ψ′
0γ(·)[v]∥2,
which implies that ψ′
0γ[·] is bounded from below since ∥ψ′
0γ(·)[v]∥2 ≥1/L∥v∥2. Analogously, ψ′
0g[·] is
also bounded from below, which can be obtained using the same argument as above.
Lemma 3. Let ζη(·,β,γ ) be a smooth curve in Hp2 running through ζ(·,β,γ ) at η = 0 , that is
ζη(·,β,γ )|η=0 = ζ(·,β,γ ). For any score function h(·,β,γ ) with ζ(·,β,γ ) = g(Λ(·,β,γ,g )) in
H =
{
h: h(·,β,γ ) = ∂ζη(·,β,γ )
∂η |η=0,ζη ∈Hp2
}
,
under conditions (C1)-(C4), there exists w∈W such that
h(·,β,γ ) = w(Λ(·,β,γ,g )) + g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[w].
Proof of Lemma 3. Since ζη(·,β,γ ) is a smooth curve in Hp2 running through ζ(·,β,γ ) at η = 0, we
can rewrite it in the form of ζη(·,β,γ ) = gη(Λ(·,β,γ,g η)) where gη is a smooth curve in Gp2 running
20
through g at η= 0. For a small η, we have gη = g+ ηw+ o(η) with w= ∂gη
∂η |η=0 ∈W. It follows that
lim
η→0
gη(Λ(·,β,γ,g η)) −g(Λ(·,β,γ,g η))
η = w(Λ(·,β,γ,g )).
Also, by the deﬁnition of functional derivatives, we have
g(Λ(·,β,γ,g η)) −g(Λ(·,β,γ,g )) = g(Λ(·,β,γ,g + ηw+ o(η))) −g(Λ(·,β,γ,g η))
= g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[ηw+ o(η)] + o(∥ηw+ o(η)∥)
= ηg′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[w] + o(η),
where the last equality holds because
lim
η→0
g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[o(η)]
η = g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )
[
lim
η→0
o(η)
η
]
= 0.
Combining these two equations together, we have,
h(·,β,γ ) = lim
η→0
gη(Λ(·,β,γ,g η)) −g(Λ(·,β,γ,g ))
η
= lim
η→0
gη(Λ(·,β,γ,g η)) −g(Λ(·,β,γ,g η)) + g(Λ(·,β,γ,g η)) −g(Λ(·,β,γ,g ))
η
= w(Λ(·,β,γ,g )) + g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[w].
Lemma 4. Denote
l(β,γ,ζ (·,β,γ ); W) = ∆{XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g ))}−Λ(Y,X,β,γ,g )
= ∆{XTβ+ γ(Y) + ζ(Y,X,β,γ )}−
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
dt.
Under conditions (C1)-(C4) , l(β,γ,ζ (·,β,γ ); W) has bounded and continuous ﬁrst and second deriva-
tives with respect to β ∈B, γ ∈Γp2 , and ζ(·,β,γ ) ∈Hp1 .
Proof of Lemma 4. The derivatives with respect to the ﬁrst, the second, and the third argument of
the objective function are
l′
1(β,γ,ζ ; W) = ∆X−X
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
dt,
l′
2(β,γ,ζ ; W)[v] = ∆v(Y) −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
v(t)dt,
l′
3(β,γ,ζ ; W)[h] = ∆h(Y,X,β,γ ) −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
h(t,X,β,γ )dt,
21
l′′
11(β,γ,ζ ; W) = −XXT
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
dt,
l′′
12(β,γ,ζ ; W)[v] = −X
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
v(t)dt,
l′′
13(β,γ,ζ ; W)[h] = −X
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
h(t,X,β,γ )dt,
l′′
23(β,γ,ζ ; W)[v,h] = −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
v(t)h(t,X,β,γ )dt,
l′′
22(β,γ,ζ ; W)[v1,v2] = −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
v1(t)v2(t)dt,
l′′
33(β,γ,ζ ; W)[h1,h2] = −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
h1(t,X,β,γ )h2(t,X,β,γ )dt.
The derivatives with respect to β and γ of ζ(·,β,γ ) are
ζ′
β(·,β,γ ) = g′(Λ(·,β,γ,g ))Λ′
β(·,β,γ,g ),
ζ′
γ(·,β,γ )[v] = g′(Λ(·,β,γ,g ))Λ′
γ(·,β,γ,g )[v],
ζ′′
ββ(·,β,γ ) = g′′(Λ(·,β,γ,g ))Λ′
β(·,β,γ,g )Λ′
β(·,β,γ,g )T + g′(Λ(·,β,γ,g ))Λ′′
ββ(·,β,γ,g ),
ζ′′
γγ(·,β,γ )[v1,v2] = g′′(Λ(·,β,γ,g ))Λ′
γ(·,β,γ,g )[v1]Λ′
γ(·,β,γ,g )[v2]
+ g′(Λ(·,β,γ,g ))Λ′′
γγ(·,β,γ,g )[v1,v2],
ζ′′
βγ(·,β,γ )[v] = g′′(Λ(·,β,γ,g ))Λ′
β(·,β,γ,g )Λ′
γ(·,β,γ,g )[v]
+ g′(Λ(·,β,γ,g ))Λ′′
βγ(·,β,γ,g )[v].
After some calculations using the chain rule, we have
l′
β(β,γ,ζ (·,β,γ ); W) = ∆{X+ ζ′
β(Y,X,β,γ )}
−
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
[ζ′
β(t,X,β,γ ) + X]dt
= ∆{X+ g′(Λ(Y,X,β,γ,g ))Λ′
β(Y,X,β,γ,g )}−Λ′
β(Y,X,β,γ,g ),
l′
γ(β,γ,ζ (·,β,γ ); W)[v] = ∆{v(Y) + ζ′
γ(Y,X,β,γ )[v]}
−
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
{v(t) + ζ′
γ(t,X,β,γ )[v]}dt
= ∆{v(Y) + g′(Λ(Y,X,β,γ,g ))Λ′
γ(Y,X,β,γ,g )[v]}−Λ′
γ(Y,X,β,γ,g )[v],
l′
ζ(β,γ,ζ (·,β,γ ); W)[h] = ∆h(Y,X,β,γ ) −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
h(t,X,β,γ )dt,
22
l′′
ββ(β,γ,ζ (·,β,γ ); W) = ∆ζ′′
ββ(Y,X,β,γ ) −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·[(X+ ζ′
β(t,X,β,γ ))(X+ ζ′
β(t,X,β,γ ))T + ζ′′
ββ(t,X,β,γ )]dt
= ∆{g′′(Λ(Y,X,β,γ,g ))Λ′
β(Y,X,β,γ,g )Λ′
β(Y,X,β,γ,g )T
+ g′(Λ(Y,X,β,γ,g ))Λ′′
ββ(Y,X,β,γ,g )}
−Λ′′
ββ(Y,X,β,γ,g ),
l′′
γγ(β,γ,ζ (·,β,γ ); W)[v1,v2] = ∆ζ′′
γγ(Y,X,β,γ )[v1,v2] −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·{(v1(t) + ζ′
γ(t,X,β,γ )[v1])(v2(t) + ζ′
γ(t,X,β,γ )[v2])
+ ζ′′
γγ(t,X,β,γ )[v1,v2]}dt
= ∆{g′′(Λ(Y,X,β,γ,g ))Λ′
γ(Y,X,β,γ,g )[v1]Λ′
γ(Y,X,β,γ,g )[v2]
+ g′(Λ(Y,X,β,γ,g ))Λ′′
γγ(Y,X,β,γ,g )[v1,v2]}
−Λ′′
γγ(Y,X,β,γ,g )[v1,v2],
l′′
γβ(β,γ,ζ (·,β,γ ); W)[v] = ∆ζ′′
γβ(Y,X,β,γ )[v] −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·{(v(t) + ζ′
γ(t,X,β,γ )[v])(X+ ζ′
β(t,X,β,γ ))
+ ζ′′
γβ(t,X,β,γ )[v]}dt
= ∆{g′′(Λ(Y,X,β,γ,g ))Λ′
γ(Y,X,β,γ,g )[v]Λ′
β(Y,X,β,γ,g )
+ g′(Λ(Y,X,β,γ,g ))Λ′′
γβ(Y,X,β,γ,g )[v]}
−Λ′′
γβ(Y,X,β,γ,g )[v],
l′′
ζβ(β,γ,ζ (·,β,γ ); W)[h] = ∆h′
β(Y,X,β,γ ) −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·{(h(t,X,β,γ ))(X+ ζ′
β(t,X,β,γ )) + h′
β(t,X,β,γ )}dt
= ∆{w′(Λ(Y,X,β,γ,g ))Λ′
β(Y,X,β,γ,g )
+ g′′(Λ(Y,X,β,γ,g ))Λ′
g(Y,X,β,γ,g )[w]Λ′
β(Y,X,β,γ,g )
+ g′(Λ(Y,X,β,γ,g ))Λ′′
gβ(Y,X,β,γ,g )[w]}
−Λ′′
gβ(Y,X,β,γ,g )[w],
23
l′′
ζγ(β,γ,ζ (·,β,γ ); W)[h,v] = ∆h′
γ(Y,X,β,γ )[v] −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·{(h(t,X,β,γ ))(v(t) + ζ′
γ(t,X,β,γ )[v]) + h′
γ(t,X,β,γ )[v]}dt
= ∆{w′(Λ(Y,X,β,γ,g ))Λ′
γ(Y,X,β,γ,g )[v]
+ g′′(Λ(Y,X,β,γ,g ))Λ′
g(Y,X,β,γ,g )[w]Λ′
γ(Y,X,β,γ,g )[v]
+ g′(Λ(Y,X,β,γ,g ))Λ′′
gγ(Y,X,β,γ,g )[w,v]}
−Λ′′
gγ(Y,X,β,γ,g )[w,v],
l′′
ζζ(β,γ,ζ (·,β,γ ); W)[h1,h2] = −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
h1(t,X,β,γ )h2(t,X,β,γ )dt,
= −
∫ Y
0
exp
(
XTβ+ γ(t) + ζ(t,X,β,γ )
)
·{w1(Λ(t,X,β,γ,g )) + g′(Λ(t,X,β,γ,g ))Λ′
g(t,X,β,γ,g )[w1]}
·{w2(Λ(t,X,β,γ,g )) + g′(Λ(t,X,β,γ,g ))Λ′
g(t,X,β,γ,g )[w2]}dt,
All the above derivatives are bounded and continuous under conditions (C1)-(C4) by Lemma 1.
Lemma 5. (Spline approximation) For γ0 ∈Γp1 , there exists a function γ0n ∈Γp1
n such that
∥γ0n −γ0∥∞= O(n−p1ν1 ).
For g0 ∈Gp2 , there exists a function g0n ∈Gp2
n such that
∥g0n −g0∥∞= O(n−p2ν2 ).
Proof of Lemma 5. Since γ0 ∈Γp1 ⊂Sp1 ([0,τ]), by Corollary 6.21 in Schumaker (2007), there exists
a function in the polynomial space with order p1, i.e., ˜γ0n ∈Sn(TK1n,K1
n,p1), such that ∥˜γ0n−γ0∥∞=
O(n−p1ν1 ). It follows that
∥(˜γ0n(·) −˜γ0n(t∗)) −γ0∥∞≤∥˜γ0n −γ0∥∞+ |˜γ0n(t∗)|
= ∥˜γ0n −γ0∥∞+ |˜γ0n(t∗) −γ0(t∗)|
≤2∥˜γ0n −γ0∥∞= O(n−p1ν1 ),
where the second equality holds because γ0(t∗) = 0 for γ0 ∈Γp1 . Let γ0n(·) = ˜γ0n(·) −˜γ0n(t∗), then
γ0n(t∗) = 0 and thereby we ﬁnd γ0n ∈Γp1
n such that ∥γ0n −γ0∥∞= O(n−p1ν1 ). The second part is a
direct result of Corollary 6.21 in Schumaker (2007).
Lemma 6. (Bracket number of l(θ; W)) Let θ0n = (β0,γ0n(·),ζ0n(·,β0,γ0n)) with
ζ0n(t,x,β 0,γ0n) = g0n(Λ(t,x,β 0,γ0n,g0n)),
24
where γ0n and g0n are deﬁned in Lemma 5. Denote Fn = {l(θ; W) −l(θ0n; W) : θ ∈Θn}. Under
conditions (C1)-(C4), the ϵ-bracketing number associated with ∥·∥∞ for Fn, denoted by N[ ] (ϵ,Fn,∥·
∥∞), has the following upper bound for some constants c1 and c2,
N[ ] (ϵ,Fn,∥·∥∞) ≲
(1
ϵ
)c1qn1 +c2qn2 +d
.
Proof of Lemma 6. Denote the ceiling of xby ⌈x⌉. Following the calculation in Shen and Wong (1994,
Page 597), we have that, for any ϵ> 0, there exists a set of ϵ-brackets
{
[γL
l ,γU
l ] : ∥γU
l −γL
l ∥∞≤ϵ,l = 1,··· ,
⌈
(1
ϵ)c1qn1
⌉}
such that for any γ ∈Γp1
n , γL
l (t) ≤γ(t) ≤γU
l (t) holds on [0 ,τ] for some 1 ≤l≤
⌈
(1
ϵ)c1qn1
⌉
. Similarly,
there exists another set of ϵ-brackets
{
[gL
i ,gU
i ] : ∥gU
i −gL
i ∥∞≤ϵ,i = 1,··· ,
⌈
(1
ϵ)c2qn2
⌉}
such that for any g ∈Gp1
n , gL
i (t) ≤g(t) ≤gU
i (t) holds on [0 ,µ] for some 1 ≤i ≤
⌈
(1
ϵ)c2qn2
⌉
. Since
B⊂ Rd is compact, it can be covered by
⌈
c3(1
ϵ)d⌉
balls with radius ϵ, i.e. for any β ∈B, there exists
1 ≤k≤
⌈
c3(1
ϵ)d⌉
such that ∥βk−β∥≤ ϵ. Hence, under condition (C2), |XTβ−XTβk|≤ c4ϵfor some
constant c4 >0 and any X ∈X. By the mean value theorem, we have that
|exp
(
g(Λ) + XTβ+ γ(t)
)
−exp
(
gL
i (Λ) + XTβk + γL
l (t)
)
|
= exp
(
˜ψ(t,Λ)
)
|g(Λ) + XTβ+ γ(t) −gL
i (Λ) + XTβk + γL
l (t)|
≤exp
(
˜ψ(t,Λ)
)
(|g(Λ) −gL
i (Λ)|+ |XTβ−XTβk|+ |γ(t) −γL
l (t)|)
≤exp
(
˜ψ(t,Λ)
)
(∥g−gL
i ∥∞+ |XTβ−XTβk|+ ∥γ−γL
l ∥∞),
where ˜ψ(t,Λ) = gL
i (Λ)+ XTβk+γL
l (t)+ ξ(g(Λ)−gL
i (Λ)+ XT(β−βk)+ γ(t)−γL
l (t)) for some ξ∈(0,1)
and is bounded under conditions (C1)-(C4). Hence,
|exp
(
g(Λ) + XTβ+ γ(t)
)
−exp
(
gL
i (Λ) + XTβk + γL
l (t)
)
|≲ ϵ
over (t,Λ) ∈[0,τ] ×[0,b]. Employing Theorem 12.V of continuous dependence in Walter (1998, page
145), we have |Λ(t,X,β,γ,g ) −Λ(t,X,β k,γL
l ,gL
i )|≤ c5ϵ for some constant c5 >0 and any t∈[0,τ].
Denote Λilk(t,x) = Λ(t,x,β k,γL
l ,gL
i ). Deﬁne
m(θ; W) = l(θ; W) −l(θ0n; W)
= ∆{XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g ))}−Λ(Y,X,β,γ,g ) −l(θ0n; W),
mL
ilk(W) = ∆{XTβk −c4ϵ+ γL
l (Y) + gL
i (ξL
ilk)}−Λilk(Y,X) −c5ϵ−l(θ0n; W),
25
and
mU
ilk(W) = ∆{XTβk + c4ϵ+ γU
l (Y) + gU
i (ξU
ilk)}−Λilk(Y,X) + c5ϵ−l(θ0n; W),
where ξL
ilk = arg min|s|≤c5ϵgL
i (Λilk(Y,X) + s) and ξU
ilk = arg max|s|≤c5ϵgU
i (Λilk(Y,X) + s).
Note that [mL
ilk(W),mU
ilk(W)] is a ϵ-bracket because
|mU
ilk(W) −mL
ilk(W)|= |∆{2c4ϵ+ γU
l (Y) −γL
l (Y) + gU
i (ξU
ilk) −gL
i (ξL
ilk)}+ 2c5ϵ|
≤2c4ϵ+ |γU
l (Y) −γL
l (Y)|+ |gU
i (ξU
ilk) −gL
i (ξU
ilk)|+ |gL
i (ξU
ilk) −gL
i (ξL
ilk)|+ 2c5ϵ
≤2c4ϵ+ ∥γU
l −γL
l ∥∞+ ∥gU
i −gL
i ∥∞+ c7|ξU
ilk −ξL
ilk|+ 2c5ϵ
≤2c4ϵ+ ϵ+ ϵ+ 2c7c5ϵ+ 2c5ϵ≲ ϵ,
where c7 = maxt∈[0,b] |(gL
i )′(t)|in the second inequality. Hence ∥mU
ilk −mL
ilk∥∞≲ ϵ.
For anyθ= (β,γ(·),ζ(·,β,γ )) with ζ(t,x,β,γ ) = g(Λ(t,x,β,γ,g )), there exits 1≤i≤
⌈
(1
ϵ)c2qn2
⌉
,1 ≤
l≤
⌈
(1
ϵ)c1qn1
⌉
,1 ≤k ≤
⌈
c3(1
ϵ)d⌉
such that gL
i (t) ≤g(t) ≤gU
i (t) on t∈[0,µ], γL
l (t) ≤γ(t) ≤γU
l (t) on
t∈[0,τ], and |XTβk −XTβ|≤ c4ϵ. It follows that
mU
ilk(W) = ∆{(XTβk + c4ϵ) + γU
l (Y) + gU
i (ξU
ilk)}+ (c5ϵ−Λilk(Y,X)) −l(θ0n; W)
≥∆{XTβ+ γ(Y) + gU
i (ξU
ilk)}+ (c5ϵ−Λilk(Y,X)) −l(θ0n; W)
≥∆{XTβ+ γ(Y) + gU
i (Λ(t,X,β,γ,g ))}−Λ(t,X,β,γ,g ) −l(θ0n; W)
≥∆{XTβ+ γ(Y) + g(Λ(t,X,β,γ,g ))}−Λ(t,X,β,γ,g ) −l(θ0n; W)
= m(θ; W),
where the second inequality holds because |Λ(Y,X,β,γ,g ) −Λilk(Y,X)|≤ c5ϵ. The other side can be
veriﬁed similarly. Therefore, we have
N[ ] (ϵ,Fn,∥·∥∞) ≲
(1
ϵ
)c1qn1
(1
ϵ
)c2qn2
(1
ϵ
)d
=
(1
ϵ
)c1qn1 +c2qn2 +d
,
which completes the proof.
Lemma 7. For 1 ≤ j ≤ d, denote Fγ
n,j(η) = {l′
γ(θ; W)[v∗
j −vj] : θ ∈ Θn,vj ∈ Γ1
n,d(θ,θ0) ≤
η,∥v∗
j −vj∥∞≤η}and Fζ
n,j(η) = {l′
ζ(θ; W)[h∗
j −hj] : θ∈Θn,hj ∈H2
n,d(θ,θ0) ≤η,∥w∗
j −wj∥∞≤η},
where v∗
j is deﬁned in condition (C7) and h∗
j(·,β,γ ) = w∗
j(Λ(·,β,γ,g ))+g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[w∗
j]
with w∗
j given in condition (C7). Then under conditions (C1)-(C4) and (C7), we have
N[ ] (ϵ,Fγ
n,j(η),∥·∥∞) ≲
(η
ϵ
)c1qn1 +c2qn2 +d
and
N[ ] (ϵ,Fζ
n,j(η),∥·∥∞) ≲
(η
ϵ
)c3qn1 +c4qn2 +d
for some constants c1,c2,c3, and c4.
26
Lemma 8. For 1 ≤j ≤d, denote
F∗β
n,j(η) = {l′
βj(θ; W) −l′
βj(θ0; W) : θ∈Θn,d(θ,θ0) ≤η,∥g′(Λ(·,β,γ,g )) −g′
0(Λ0(·))∥2 ≤η},
F∗γ
n,j(η) = {l′
γ(θ; W)[v∗
j] −l′
γ(θ0; W)[v∗
j] : θ∈Θn,d(θ,θ0) ≤η,∥g′(Λ(·,β,γ,g )) −g′
0(Λ0(·))∥2 ≤η},
and
F∗ζ
n,j(η) = {l′
ζ(θ; W)[h∗
j] −l′
ζ(θ0; W)[h∗
j] : θ∈Θn,d(θ,θ0) ≤η},
where v∗
j is deﬁned in condition (C7) and h∗
j(·,β,γ ) = w∗
j(Λ(·,β,γ,g ))+g′(Λ(·,β,γ,g ))Λ′
g(·,β,γ,g )[w∗
j]
with w∗
j given in condition (C7). Then under conditions (C1)-(C4) and (C7), we have
N[ ] (ϵ,F∗β
n,j(η),∥·∥∞) ≲
(η
ϵ
)c1qn1 +c2qn2 +d
,
N[ ] (ϵ,F∗γ
n,j(η),∥·∥∞) ≲
(η
ϵ
)c3qn1 +c4qn2 +d
,
and
N[ ] (ϵ,F∗ζ
n,j(η),∥·∥∞) ≲
(η
ϵ
)c5qn1 +c6qn2 +d
for some constants ci, i= 1,..., 6.
The proofs of Lemma 7 and 8 follow a similar calculation as in Lemma 6 and therefore are omitted
here.
3.2 Proof of Theorem 1
Proof of Theorem 1. We prove the theorem by checking the conditions C1-3 in Shen and Wong
(1994, Theorem 1). Using the fact P{
∫Y
0 f(t,X)dΛ0(t,X)}= P{∆f(Y,X)}, we have
Pl(β,γ,ζ (·,β,γ ); W) =P{∆[XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g ))
−exp
(
XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g )) −XTβ0 −γ0(Y) −g0(Λ0(Y,X))
)
]}.
It follows that, by the Taylor expansion,
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ (·,β,γ ); W)
= P{∆[exp
(
XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g )) −XTβ0 −γ0(Y) −g0(Λ0(Y,X))
)
−1 −(XTβ+ γ(Y) + g(Λ(Y,X,β,γ,g )) −XTβ0 −γ0(Y) −g0(Λ0(Y,X)))]}
= 1
2A+ o(A), (S22)
27
where A= P{∆[XTβ+γ(Y)+ g(Λ(Y,X,β,γ,g ))−XTβ0 −γ0(Y)−g0(Λ0(Y,X))]2}. After subtracting
and adding the term g(Λ0(Y,X)), we have
A= P{∆[XT(β−β0) + γ(Y) −γ0(Y) + g(Λ(Y,X,β,γ,g )) −g(Λ0(Y,X))
+ g(Λ0(Y,X)) −g0(Λ0(Y,X))]2}
= P{∆[(g′(Λ0(Y,X))Λ′
0β(Y,X) + X)T(β−β0)
+ g′(Λ0(Y,X))Λ′
0γ(Y,X)[γ−γ0] + γ(Y) −γ0(Y)
+ g′(Λ0(Y,X))Λ′
0g(Y,X)[g−g0] + g(Λ0(Y,X)) −g0(Λ0(Y,X))
+ o(∥β−β0∥) + o(∥γ−γ0∥2) + o(∥g−g0∥2)]2},
where the second equality is obtained by using the Taylor expansion. Since Λ ′
0β(t,x) is bounded by
Lemma 1 and Λ ′
0γ(·)[v] and Λ ′
0g(·)[w] are bounded linear operators, which can be veriﬁed using the
same arguments as in Lemma 2, we have
g′(Λ0(Y,X))Λ′
0β(Y,X)T(β−β0) = g′
0(Λ0(Y,X))Λ′
0β(Y,X)T(β−β0) + o(∥β−β0∥) + o(∥g−g0∥2),
g′(Λ0(Y,X))Λ′
0γ(Y,X)[γ−γ0] = g′
0(Λ0(Y,X))Λ′
0γ(Y,X)[γ−γ0] + o(∥γ−γ0∥2) + o(∥g−g0∥2),
g′(Λ0(Y,X))Λ′
0g(Y,X)[g−g0] = g′
0(Λ0(Y,X))Λ′
0g(Y,X)[g−g0] + o(∥g−g0∥2).
Note that under conditions (C1)-(C4), we have
d2(θ,θ0) ≲ ∥β−β0∥2 + ∥γ−γ0∥2
2 + ∥g−g0∥2
2 ≲ d2(θ,θ0). (S23)
Plugging these equations above into A, it follows that
A≳ P{∆[(g′
0(Λ0(Y,X))Λ′
0β(Y,X) + X)T(β−β0)
+ g′
0(Λ0(Y,X))Λ′
0γ(Y,X)[γ−γ0] + γ(Y) −γ0(Y)
+ g′
0(Λ0(Y,X))Λ′
0g(Y,X)[g−g0] + g(Λ0(Y,X)) −g0(Λ0(Y,X))]2}
+ o(d2(θ,θ0)). (S24)
Then, by solving the initial value problem in (S9), we have
g′
0(Λ0(Y,X))Λ′
0β(Y,X) + X = (g′
0(Λ0(Y,X)) exp(g0(Λ0(Y,X)))R(Y)eXTβ0 + 1)X
= (g′
0(˜Λ0(U)) exp
(
g0(˜Λ0(U))
)
U + 1)X
≜ ϵ1(U)X, (S25)
with U given in condition (C5) and ϵ1 is a deterministic function.
28
Note that using equations (S19) and (S20) in Lemma 2, we also have
ψ′
0γ(Y,X)[γ−γ0] = g′
0(Λ0(Y,X))Λ′
0γ(Y,X)[γ−γ0] + γ(Y) −γ0(Y)
= g′
0(˜Λ0(U) exp
(
g0(˜Λ0(U))
)∫ U
0
(γ−γ0)(R−1(se−V))ds+ (γ−γ0)(R−1(Ue−V))
≜ ϵ2(U,V )[γ−γ0], (S26)
which is a deterministic function of U and V given in condition (C5), and
ψ′
0g(Y,X)[g−g0] = g′
0(Λ0(Y,X))Λ′
0g(Y,X)[g−g0] + g(Λ0(Y,X)) −g0(Λ0(Y,X))
= g′
0(˜Λ0(U) exp
(
g0(˜Λ0(U))
)∫ ˜Λ0(U)
0
exp(−g0(s))(g−g0)(s)ds+ (g−g0)(˜Λ0(U))
≜ ϵ3(U)[g−g0], (S27)
which is a deterministic function of U.
Then, it follows from (S24)
A≳ P{∆[ϵ1(U)XT(β−β0) + ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0]]2}+ o(d2(θ,θ0))
= P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}
+ 2P{∆(ϵ1(U)XT(β−β0))(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])}+ o(d2(θ,θ0))
≥P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}
−2|P{∆(ϵ1(U)XT(β−β0))(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])}|+ o(d2(θ,θ0)). (S28)
By using the fact that P{∆f(U,X)}= P{
∫Y
0 f(R(t)eXTβ0 ,X)dΛ0(t,X)}= P{
∫U
0 f(t,X)d˜Λ0(t)},
|P{∆(ϵ1(U)XT(β−β0))(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])}|2
=
(
P
{∫ U
0
ϵ1(t)XT(β−β0)(ϵ2(t,V )[γ−γ0] + ϵ3(t)[g−g0])d˜Λ0(t)
})2
=
(
P
{∫ U
0
ϵ1(t)P{XT(β−β0)|U,V }(ϵ2(t,V )[γ−γ0] + ϵ3(t)[g−g0])d˜Λ0(t)
})2
≤P
{∫ U
0
(ϵ1(t))2 (
P{XT(β−β0)|U,V }
)2
d˜Λ0(t)
}
P
{∫ U
0
(ϵ2(t,V )[γ−γ0] + ϵ3(t)[g−g0])2d˜Λ0(t)
}
,
where the last step is obtained using the Cauchy-Schwartz inequality. Under condition (C5), there
exists η1 ∈(0,1) such that
(1 −η1)(β−β0)TP{XXT|U,V }(β−β0) ≥(P{XT(β−β0)|U,V })2,
29
since the ﬁrst element of β−β0 is zero with the identiﬁability constraint. Thus, we have
|P{∆(ϵ1(U)XT(β−β0))(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])}|2
≤(1 −η1)P
{∫ U
0
(ϵ1(t))2(β−β0)TP{XXT|U,V }(β−β0)d˜Λ0(t)
}
·P
{∫ U
0
(ϵ2(t,V )[γ−γ0] + ϵ3(t)[g−g0])2d˜Λ0(t)
}
=(1 −η1)P
{∫ U
0
(ϵ1(t)XT(β−β0))2d˜Λ0(t)
}
P
{∫ U
0
(ϵ2(t,V )[γ−γ0] + ϵ3(t)[g−g0])2d˜Λ0(t)
}
=(1 −η1)P{∆(ϵ1(U)XT(β−β0))2}P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2},
and it yields from (S28) that
A≥P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}
−2(1 −η1)1/2(P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2})1/2(P{∆(ϵ1(U)XT(β−β0))2})1/2
≥(1 −(1 −η1)1/2){P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}}
≳ P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}
= A1 + A2,
where the second inequality is obtained by 2 ab≤a2 + b2.
For A1, under condition (C3), we have for t∈[0,τ],
P{1 (Y >t)|X}≥ P{1 ((Y >τ)|X}≥ δ0.
Then it follows that,
A1 = P{
∫ Y
0
exp
(
XTβ0 + γ0(t) + g(Λ0(t,X))
)(
ϵ1(R(t)eXTβ0 )XT(β−β0)
)2
dt}
= P{
∫ τ
0
P{1 (Y >t)|X}exp
(
XTβ0 + γ0(t) + g(Λ0(t,X))
)(
ϵ1(R(t)eXTβ0 )XT(β−β0)
)2
dt}
≥δ0P{
∫ τ
0
exp
(
XTβ0 + γ0(t) + g(Λ0(t,X))
)(
ϵ1(R(t)eXTβ0 )XT(β−β0)
)2
dt}
= δ0P{
∫ R(τ)eXTβ0
0
exp
(
g(˜Λ0(s))
)(
ϵ1(s)XT(β−β0)
)2
ds}
≥δ0P{
∫ cR(τ)
0
exp
(
g(˜Λ0(s))
)(
ϵ1(s)XT(β−β0)
)2
ds}
= δ0(β−β0)TP{XXT}(β−β0)
∫ cR(τ)
0
exp
(
g(˜Λ0(s))
)
(ϵ1(s))2ds,
where the fourth equality is derived by variable transformation s= R(t)eXTβ0 and c= minx∈XexTβ0 ,
which is positive since Xis bounded under condition (C2). As condition (C2) implies that the smallest
30
eigenvalue ofP{XXT}, denoted by λ1, is positive as well, we have (β−β0)TP{XXT}(β−β0) ≥λ1∥β−
β0∥2. Also, by deﬁnition ϵ1(s) satisﬁes the equation g′
0(˜Λ0(t))
∫t
0 exp
(
g0(˜Λ0(s))
)
ϵ1(s)ds+ 1 = ϵ1(t),
thus it can not be a constant zero and
∫cR(τ)
0 exp
(
g(˜Λ0(s))
)
(ϵ1(s))2dsis bounded away from 0 below.
Hence, A1 ≳ ∥β−β0∥2.
For A2, it is bounded below by
P{∆(ϵ2(U,V )[γ−γ0] + ϵ3(U)[g−g0])2}
≥P{∆(ϵ2(U,V )[γ−γ0])2}+ P{∆(ϵ3(U)[g−g0])2}−2|P{∆(ϵ2(U,V )[γ−γ0])(ϵ3(U)[g−g0])}|
≥P{∆(ϵ2(U,V )[γ−γ0])2}+ P{∆(ϵ3(U)[g−g0])2}
−2η1/2
2 P{∆}(P{∆(ϵ2(U,V )[γ−γ0])2})1/2(P{∆(ϵ3(U)[g−g0])2})1/2
≥(1 −η1/2
2 P{∆}){P{∆(ϵ2(U,V )[γ−γ0])2}+ P{∆(ϵ3(U)[g−g0])2}}
≳ P{∆(ϵ2(U,V )[γ−γ0])2}+ P{∆(ϵ3(U)[g−g0])2},
where the second inequality holds under condition (C6) because there exists some η2 ∈(0,1) such that
(P{ϵ2(U,Y )[γ−γ0]ϵ3(U)[g−g0]|∆ = 1})2 ≤η2P{(ϵ2(U,Y )[γ−γ0])2|∆ = 1}P{(ϵ3(U)[g−g0])2|∆ = 1}.
Furthermore, the ﬁrst term is bounded under condition (C3)
P{∆(ϵ2(U,V )[γ−γ0])2}= P{∆(ψ′
0γ(Y,X)[γ−γ0])2}
= P{
∫ τ
0
P{1 (Y >t)|X}(ψ′
0γ(t,X)[γ−γ0])2dΛ0(t,X)}
≥δ0P{
∫ τ
0
(ψ′
0γ(t,X)[γ−γ0])2dΛ0(t,X)}
≳ ∥γ−γ0∥2
2,
where the second inequality is obtained by Lemma 2 because γ−γ0 ∈Γp1 . Using the same argument,
we have P{∆(ϵ3(U)[g−g0])2}≳ ∥g−g0∥2
2. Therefore,
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ (·,β,γ ); W) = 1
2A+ o(A)
≳ ∥β−β0∥2 + ∥γ−γ0∥2
2 + ∥g−g0∥2
2
≳ d2(θ,θ0),
which implies that
inf
d(θ,θ0)≥ϵ,θ∈Θn
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ (·,β,γ ); W) ≳ ϵ2.
Hence the condition C1 in Shen and Wong (1994, Theorem 1) holds with α= 1 in their notation.
31
Next, we verify the condition C2 in Shen and Wong (1994, Theorem 1). It follows that
(l(β,γ,ζ (·,β,γ ); W) −l(β0,γ0,ζ0(·,β0,γ0); W))2
= {∆XT(β−β0) + ∆[γ(Y) −γ0(Y)] + ∆[g(Λ(Y,X,β,γ,g )) −g0(Λ0(Y,X))]
−
∫ Y
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]dt}2
≲ (XT(β−β0))2 + ∆(γ(Y) −γ0(Y))2 + ∆[g(Λ(Y,X,β,γ,g )) −g0(Λ0(Y,X))]2
+ {
∫ Y
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]dt}2
≲ ∥β−β0∥2 + ∆(γ(Y) −γ0(Y))2 + ∆[g(Λ(Y,X,β,γ,g )) −g0(Λ0(Y,X))]2
+
∫ τ
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]2dt, (S29)
where the second inequality is obtained by the condition (C2) and the Cauchy-Schwartz inequality
{
∫ Y
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]dt}2
= {
∫ τ
0
[1(Y ≥t) exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]dt}2
≤
∫ τ
0
1(Y ≥t)dt
∫ τ
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]2dt
≤τ
∫ τ
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]2dt.
For the second term in (S29), we have
P{∆(γ(Y) −γ0(Y))2}
= P
∫ τ
0
1(Y ≥t) exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
(γ(t) −γ0(t))2dt
≤
∫ τ
0
P{exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
}(γ(t) −γ0(t))2dt
≲ ∥γ−γ0∥2
2, (S30)
where the last inequality holds because exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
is bounded under conditions
32
(C1)-(C4). For the third term in (S29), we have
P{∆[g(Λ(Y,X,β,γ,g )) −g0(Λ0(Y,X))]2}
= P
∫ Y
0
[g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dΛ0(t,X)
= P
∫ τ
0
1(Y ≥t)[g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dΛ0(t,X)
≤P
∫ τ
0
[g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dΛ0(t,X)
= ∥ζ(·,β,γ ) −ζ0(·,β0,γ0)∥2
2, (S31)
For the fourth term in (S29), using the mean value theorem, it follows that
P{
∫ τ
0
[exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
−exp
(
XTβ0 + γ0(t) + g0(Λ0(t,X))
)
]2dt}
= P{
∫ τ
0
exp
(
2 ˜ψ(t,X)
)
[XT(β−β0) + (γ(t) −γ0(t)) + g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dt}
≲ P{
∫ τ
0
exp
(
2 ˜ψ(t,X)
)
{[XT(β−β0)]2 + [γ(t) −γ0(t)]2 + [g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2}dt}
= I1 + I2 + I3,
where ˜ψ(t,X) = XTβ0 + γ0(t) + g0(Λ0(t,X)) + ξ(XT(β −β0) + γ(t) −γ0(t) + g(Λ(t,X,β,γ,g )) −
g0(Λ0(t,X))) for some ξ∈(0,1) and is bounded under conditions (C1)-(C4). Hence,
I1 ≲ (β−β0)TP(XXT)(β−β0) ≤λd∥β−β0∥2,
where λd is the largest eigenvalue of P(XXT),
I2 ≲ ∥γ−γ0∥2
2,
and
I3 = P{
∫ τ
0
exp
(
2 ˜ψ(t,X) −XTβ0 −γ0(t) −g0(Λ0(t,X))
)
·[g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dΛ0(t,X)}
≲ P{
∫ τ
0
[g(Λ(t,X,β,γ,g )) −g0(Λ0(t,X))]2dΛ0(t,X)}
= ∥ζ(·,β,γ ) −ζ0(·,β0,γ0)∥2
2.
Therefore, we have
P(l(β,γ,ζ (·,β,γ ); W) −l(β0,γ0,ζ0(·,β0,γ0); W))2 ≲ ∥β−β0∥2 + ∥γ−γ0∥2
2 + ∥ζ(·,β,γ ) −ζ0(·,β0,γ0)∥2
2
≲ d2(θ,θ0),
33
which implies that
sup
d(θ,θ0)≤ϵ,θ∈Θn
Var{l(β,γ,ζ (·,β,γ ); W) −l(β0,γ0,ζ0(·,β0,γ0); W)}
≤ sup
d(θ,θ0)≤ϵ,θ∈Θn
P{l(β,γ,ζ (·,β,γ ); W) −l(β0,γ0,ζ0(·,β0,γ0); W)}2 ≲ ϵ2.
Thus the condition C2 in Shen and Wong (1994, Theorem 1) holds with β = 1 in their notation.
Next we verify the condition C3 in Shen and Wong (1994, Theorem 1). By Lemma 6, we have
H(ϵ,Fn,∥·∥∞) = log(N(ϵ,Fn,∥·∥∞)) ≲ (c1qn1 + c2qn2 + d) log(1/ϵ) ≲ nmax{ν1,ν2}log(1/ϵ).
So the C3 holds with constants 2 r0 = max {ν1,ν2}and r = 0 + in their notations, which leads to
τ = 1−max{ν1,ν2}
2 −log logn
2 logn in their main result. We can select slightly large ˜ ν1 and ˜ν2 such that
1−max{˜ν1,˜ν2}
2 ≤ 1−max{ν1,ν2}
2 −log logn
2 logn for suﬃciently large n and still denote ˜ νi by νi for i = 1 ,2.
Then, τ = 1−max{ν1,ν2}
2 . Also, since the sieve estimator ˆθn maximizes the empirical log-likelihood over
the sieve space Θ n, the inequality (1.1) in Shen and Wong (1994) holds with ηn = 0. Therefore, by
Theorem 1 in Shen and Wong (1994), we have
d(ˆθn,θ0) = Op(max{n−1−max{ν1,ν2}
2 ,d(θ0n,θ0),K1/2(θ0n,θ0)}),
where K(θ0n,θ0) = P{l(θ0; W) −l(θ0n; W)}. Further, using the Taylor expansion for P{l(θ0; W) −
l(θ0n; W)}in (S22), we have
K(θ0n,θ0) = 1
2P{∆[γ0(Y) + g0(Λ(Y,X,β 0,γ0,g0)
−γ0n(Y) −g0n(Λ(Y,X,β 0,γ0n,g0n))]2}+ o(d2(θ0n,θ0))
≤P{∆[g0(Λ(Y,X,β 0,γ0,g0)) −g0n(Λ(Y,X,β 0,γ0n,g0n))]2}
+ P{∆(γ0(Y) −γ0n(Y))2}+ o(d2(θ0n,θ0))
≲ ∥ζ0(·,β0,γ0) −ζ0n(·,β0n,γ0n)∥2
2 + ∥γ0 −γ0n∥2
2 + o(d2(θ0n,θ0))
= O(d2(θ0n,θ0)),
where the ﬁrst inequality is obtained by the fact ( a+ b)2 ≤2(a2 + b2) and the second inequality holds
by using the same argument as in (S30) and (S31). Moreover, d2(θ0n,θ0) ≲ ∥γ0 −γ0n∥2
2 +∥g0 −g0n∥2
2 ≲
∥γ0 −γ0n∥2
∞+ ∥g0 −g0n∥2
∞ = O(n−2 min{p1ν1,p2ν2}) due to inequality (S23) and Lemma 5. Thus, we
have
d(ˆθn,θ0) = Op(max{n−1−max{ν1,ν2}
2 ,n−min{p1ν1,p2ν2}}) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }),
which completes the proof.
34
3.3 Proof of Theorem 2
Proof of Theorem 2. We prove the theorem by verifying assumptions (A1)-(A6) in Appendix 2.
By Theorem 1 we know that assumption (A1) holds with ξ = min {p1ν1,p2ν2,1−max{ν1,ν2}
2 }. It is
straightforward to show that assumption (A2) holds based on the fact that score functions have zero
mean. To verify assumption (A3), ﬁrst, we will ﬁnd v∗ = (v∗
1,··· ,v∗
d)′ and h∗ = (h∗
1,··· ,h∗
d)′ with
h∗(·) = w∗(Λ0(·)) + g′
0(Λ0(·))Λ′
0g(·)[w∗] such that for any v ∈V and h ∈H with h(·) = w(Λ0(·)) +
g′
0(Λ0(·))Λ′
0g(·)[w],
S′′
βγ(β0,γ0(·),ζ0(·,β0,γ0))[v] = S′′
γγ(β0,γ0(·),ζ0(·,β0,γ0))[v∗,v]
+ S′′
ζγ(β0,γ0(·),ζ0(·,β0,γ0))[h∗,v], (S32)
S′′
βζ(β0,γ0(·),ζ0(·,β0,γ0))[h] = S′′
γζ(β0,γ0(·),ζ0(·,β0,γ0))[v∗,h]
+ S′′
ζζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗,h]. (S33)
By Lemma 4 and the property P{
∫Y
0 f(t,X)dΛ0(t,X)}= P{∆f(Y,X)}, for any v ∈Vd,v ∈V and
h ∈Hd with h(·) = w(Λ0(·)) + g′
0(Λ0(·))Λ′
0g(·)[w], we have
S′′
βγ(β0,γ0,ζ0)[v] −S′′
γγ(β0,γ0,ζ0)[v,v] −S′′
ζγ(β0,γ0,ζ0)[h,v]
=P{l′′
βγ(β0,γ0,ζ0; W)[v] −l′′
γγ(β0,γ0,ζ0; W)[v,v] −l′′
ζγ(β0,γ0,ζ0; W)[h,v]}
=P{∆
[
g′
0(Λ0(Y,X))Λ′
0β(Y,X) + X−g′
0(Λ0(Y,X))Λ′
0γ(Y,X)[v] −v(Y)
−g′
0(Λ0(Y,X))Λ′
0g(Y,X)T[w] −w(Λ0(Y,X))
]
(g′
0(Λ0(Y,X))Λ′
0γ(Y,X)[v] + v(Y))}
=P{∆ (ϵ1(U)X−ϵ2(U,V )[v] −ϵ3(U)[w]) ψ′
0γ(Y,X)[v]}, (S34)
where the last equality holds with ϵ1,ϵ2,ϵ3,ψ′
0γ given in (S25)-(S27) and U given in the condition (C5).
Similarly, for any v ∈Vd, h ∈Hd and h∈H with h(·) = w(Λ0(·)) + g′
0(Λ0(·))Λ′
0g(·)[w], we have
S′′
βζ(β0,γ0,ζ0)[h] −S′′
γζ(β0,γ0,ζ0)[v,h] −S′′
ζζ(β0,γ0,ζ0)[h,h]
=P{∆ (ϵ1(U)X−ϵ2(U,V )[v] −ϵ3(U)[w]) ψ′
0g(Y,X)[w]}. (S35)
Note that under condition (C7), there exists v∗= (v∗
1,··· ,v∗
d)T and w∗= (w∗
1,··· ,w∗
d)T, where v∗
j ∈
Γ2 and w∗
j ∈G2 for j = 1,··· ,d, such thatP{∆A∗(U,X)ψ′
0γ(Y,X)[v]}= 0 and P{∆A∗(U,X)ψ′
0g(Y,X)[w]}=
0 hold for any v ∈Γp1 and w ∈Gp2 . Since A∗(U,X) = ϵ1(U)X−ϵ2(U,V )[v∗] −ϵ3(U)[w∗], plugging
v = v∗in (S34) and w = w∗in (S35) we have equations (S32) and (S33) hold with v∗and w∗given
35
in condition (C7). Then it follows that
l′
β(β0,γ0,ζ0; W) −l′
γ(β0,γ0,ζ0; W)[v∗] −l′
ζ(β0,γ0,ζ0; W)[h∗(·,β0,γ0)]
=∆A∗(U,X) −
∫ Y
0
A∗(R(t)eXTβ0 ,X)dΛ0(t,X)
=∆A∗(U,X) −
∫ R(Y)eXTβ0
0
A∗(t,X)d˜Λ0(t)
=
∫
A∗(t,X)dM(t) = l∗(β0,γ0,ζ0; W),
with M(t) and l∗given in condition (C8). Based on the zero-mean property of score function together
with the facts in (S32) and (S33), the matrix A in assumption (A3) is given by
A= −S′′
ββ(β0,γ0,ζ0) + S′′
γβ(β0,γ0,ζ0)[v∗] + S′′
ζβ(β0,γ0,ζ0)[h∗]
−S′′
γγ(β0,γ0,ζ0)[v∗,v∗] + S′′
βγ(β0,γ0,ζ0)[v∗] −S′′
ζγ(β0,γ0,ζ0)[h∗,v∗]
−S′′
ζζ(β0,γ0,ζ0)[h∗,h∗] + S′′
βζ(β0,γ0,ζ0)[h∗] −S′′
γζ(β0,γ0,ζ0)[v∗,h∗]
= P{(l′
β(β0,γ0,ζ0; W) −l′
γ(β0,γ0,ζ0; W)[v∗] −l′
ζ(β0,γ0,ζ0; W)[h∗])⊗2}
= P{l∗(β0,γ0,ζ0; W)⊗2},
which is the information matrix for β0 and is nonsingular under condition (C8). Thus, assumption
(A3) holds.
To verify assumption (A4), we ﬁrst note that the ﬁrst part holds because ˆβn satisﬁes S′
β,n(ˆθn) = 0
where ˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn)). Next we need to show that S′
γ,n(ˆθn)[v∗
j] = op(n−1/2). Since
v∗
j ∈Γ2, by Lemma 5 there exists v∗
j,n ∈Γ2
n such that ∥v∗
j,n −v∗
j∥∞ = O(n−2ν1 ). Based on the fact
that v∗
j,n can be written as the linear combination of basis functions B1
k for k = 1,...,q 1
n, we have
S′
γ,n(ˆθn)[v∗
j,n] = 0. 1 Since S′
γ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j −v∗
j,n] = 0, it suﬃces to show that for each
1 ≤j ≤d,
P{l′
γ(ˆθn; W)[v∗
j −v∗
j,n] −l′
γ(θ0; W)[v∗
j −v∗
j,n]}+ (Pn −P){l′
γ(ˆθn; W)[v∗
j −v∗
j,n]}
= I1n + I2n = op(n−1/2).
1Note that we constrain the parameter γ(t∗) = 0 for identiﬁability guarantee. For any γ ∈Γp1
n in the sieve
space, the constraint can be achieved by ﬁxing the coeﬃcient of one speciﬁc B-spline basis (suppose it is indexed
as the ﬁrst basis and let a1 ≡0) and leaving coeﬃcients of other bases as free optimization parameters. Since
ˆθn maximizes ln(θ) in the sieve space and v∗
j,n ∈Γ2
n can be written as the linear combination of bases with the
ﬁrst coeﬃcient a1 ﬁxed as 0, we have the gradient of ln(θ) with respect to γ along the direction v∗
j,n at ˆθn equal
to zero, i.e., S′
γ,n(ˆθn)[v∗
j,n] = 0.
36
We will ﬁrst show that I1n is op(n−1/2). Using the Taylor expansion for l′
γ(ˆθn)[v∗
j −v∗
j,n] at θ0, we have
I1n =P{( ˆβn −β0)Tl′′
βγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n]
+ l′′
γγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆγn −γ0]
+ l′′
γζ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆζn −ζ0]},
where ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn)) is some point between θ0 and ˆθn. Let ˜Λ(t,x) = Λ(t,x, ˜βn,˜γn,˜gn). Note
that by solving initial value problems in Lemma 1, we have ˜Λ′
β(t,x) and ˜Λ′′
ββ(t,x) are bounded on
t∈[0,τ] and x∈X based on the boundedness of ˜γn, ˜gn, ˜g′
n and ˜g′′
n. Also, we have ∥˜Λ′
γ(·)[v]∥∞≲ ∥v∥∞
and supt∈[0,τ],x∈X∥˜Λ′′
βγ(t,x)[v]∥≲ ∥v∥∞. It follows that
sup
t∈[0,τ],x∈X
∥˜ζ′′
βγ(t,x, ˜βn,˜γn)[v∗
j −v∗
j,n]∥
= sup
t∈[0,τ],x∈X
∥˜g′′
n(˜Λ(t,x))˜Λ′
β(t,x)˜Λ′
γ(t,x)[v∗
j −v∗
j,n] + ˜g′
n(˜Λ(t,X))˜Λ′′
βγ(t,x)[v∗
j −v∗
j,n]∥
≲∥v∗
j −v∗
j,n∥∞,
and
P{∥l′′
βγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n]∥}
=P{

∫ τ
0
˜ζ′′
βγ(t,X, ˜βn,˜γn)[v∗
j −v∗
j,n]1 (Y ≥t)dΛ0(t,X) −
∫ τ
0
exp
(
XT ˜βn + ˜γn(t) + ˜gn(˜Λ(t,X))
)
·{(v∗
j(t) −v∗
j,n(t) + ˜g′
n(˜Λ(t,X))˜Λ′
γ(t,X)[v∗
j −v∗
j,n])(X+ ˜g′
n(˜Λ(t,X))˜Λ′
β(t,X))
+ ˜ζ′′
βγ(t,X, ˜βn,˜γn)[v∗
j −v∗
j,n]}1 (Y ≥t)dt
}
≲ sup
t∈[0,τ],x∈X
∥˜ζ′′
βγ(t,x, ˜βn,˜γn)[v∗
j −v∗
j,n]∥+ ∥v∗
j −v∗
j,n∥∞≲ ∥v∗
j −v∗
j,n∥∞.
Therefore, the ﬁrst term in I1n is dominated by
P{|( ˆβn −β0)Tl′′
βγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n]|}
≤∥ˆβn −β0∥P{∥l′′
βγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n]∥}
≲∥ˆβn −β0∥∥v∗
j −v∗
j,n∥∞≤d(ˆθn,θ0)∥v∗
j −v∗
j,n∥∞
=Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }) ·O(n−2ν1 )
=Op(n−min{(p1+2)ν1,p2ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}).
By solving initial value problems in (S10) and (S13) and the Cauchy-Schwarz inequality (similar argu-
ments are used in Lemma 2 to prove that linear operators are bounded above), we have ∥˜Λ′
γ(·)[v]∥2 ≲
37
∥v∥2 and ∥˜Λ′′
γγ(·)[ν1,ν2]∥2 ≲ ∥ν1∥∞∥ν2∥2. It follows that
∥˜ζ′′
γγ(·,˜βn,˜γn)[v∗
j −v∗
j,n,ˆγn −γ0]∥2
=∥˜g′′
n(˜Λ(·))˜Λ′
γ(·)[ˆγn −γ0]˜Λ′
γ(·)[v∗
j −v∗
j,n] + ˜g′
n(˜Λ(·))˜Λ′′
γγ(·)[v∗
j −v∗
j,n,ˆγn −γ0]∥2
≲∥˜Λ′
γ(·)[v∗
j −v∗
j,n]∥∞∥˜Λ′
γ(·)[ˆγn −γ0]∥2 + ∥˜Λ′′
γγ(·)[v∗
j −v∗
j,n,ˆγn −γ0]∥2
≲∥v∗
j −v∗
j,n∥∞·∥ˆγn −γ0∥2,
and by the Cauchy-Schwarz inequality the second term in I1n is bounded by
(P{
⏐⏐l′′
γγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆγn −γ0]
⏐⏐})2
≤P{
⏐⏐l′′
γγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆγn −γ0]
⏐⏐2}
=P
{⏐⏐⏐∆˜ζ′′
γγ(Y,X, ˜βn,˜γn)[v∗
j −v∗
j,n,ˆγn −γ0]
−
∫ τ
0
1 (Y ≥t) exp
(
XT ˜βn + ˜γn(t) + ˜gn(˜Λ(t,X))
)
·{˜ζ′′
γγ(t,X, ˜βn,˜γn)[v∗
j −v∗
j,n,ˆγn −γ0]
+ ((v∗
j −v∗
j,n)(t) + ˜g′
n(˜Λ(t,X))˜Λ′
γ(t,X)[v∗
j −v∗
j,n])
·((ˆγn −γ0)(t) + ˜g′
n(˜Λ(t,X))˜Λ′
γ(t,X)[ˆγn −γ0])}dt
⏐⏐⏐
2}
≲∥˜ζ′′
γγ(·,˜βn,˜γn)[v∗
j −v∗
j,n,ˆγn −γ0]∥2
2 + ∥v∗
j −v∗
j,n∥2
∞·(∥ˆγn −γ0∥2
2 + ∥˜Λ′
γ(·)[ˆγn −γ0]∥2
2)
≲∥v∗
j −v∗
j,n∥2
∞·∥ˆγn −γ0∥2
2 ≤∥v∗
j −v∗
j,n∥2
∞·d2(ˆθn,θ0).
So P{
⏐⏐l′′
γγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆγn−γ0]
⏐⏐}= Op(n−min{(p1+2)ν1,p2ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}).
Also, by subtracting and adding some terms and using ∥a+ b∥2 ≤∥a∥2 + ∥b∥2, we have
∥ˆζ′
n,γ(·,ˆβn,ˆγn)[v∗
j −v∗
j,n] −ζ′
0γ(·,β0,γ0)[v∗
j −v∗
j,n]∥2
=∥ˆg′
n(Λ(·,ˆβn,ˆγn,ˆgn))Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n] −g′
0(Λ0(·))Λ′
0γ(·)[v∗
j −v∗
j,n]∥2
≤∥ˆg′
n(Λ(·,ˆβn,ˆγn,ˆgn))Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n] −g′
0(Λ0(·))Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n]∥2
+ ∥g′
0(Λ0(·))Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n] −g′
0(Λ0(·))Λ′
γ(·,β0,ˆγn,ˆgn)[v∗
j −v∗
j,n]∥2
+ ∥g′
0(Λ0(·))Λ′
γ(·,β0,ˆγn,ˆgn)[v∗
j −v∗
j,n] −g′
0(Λ0(·))Λ′
γ(·,β0,γ0,ˆgn)[v∗
j −v∗
j,n]∥2
+ ∥g′
0(Λ0(·))Λ′
γ(·,β0,γ0,ˆgn)[v∗
j −v∗
j,n] −g′
0(Λ0(·))Λ′
0γ(·)[v∗
j −v∗
j,n]∥2
=J1 + J2 + J3 + J4.
For J1, since ˆγn, ˆgn and ˆg′
n are bounded, we have ∥Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n]∥∞≲ ∥v∗
j −v∗
j,n∥∞and it
38
follows that
J1 ≤∥ˆg′
n(Λ(·,ˆβn,ˆγn,ˆgn)) −g′
0(Λ0(·))∥2 ·∥Λ′
γ(·,ˆβn,ˆγn,ˆgn)[v∗
j −v∗
j,n]∥∞
≲ ∥ˆg′
n(Λ(·,ˆβn,ˆγn,ˆgn)) −g′
0(Λ0(·))∥2 ·∥v∗
j −v∗
j,n∥∞
= Op(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }) ·O(n−2ν1 )
= Op(n−min{(p1+2)ν1,(p2−1)ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}),
where the third equality holds based on the same argument of Ding and Nan (2011) on their page
3058. For J2, by using the mean value theorem, it follows that
J2 = ∥g′
0(Λ0(·))(Λ′′
γβ(·,˜βn,ˆγn,ˆgn)[v∗
j −v∗
j,n])T( ˆβn −β0)∥2
≲ ∥Λ′′
γβ(·,˜βn,ˆγn,ˆgn)[v∗
j −v∗
j,n]∥2∥ˆβn −β0∥
≲ ∥v∗
j −v∗
j,n∥∞·∥ˆβn −β0∥
= Op(n−min{(p1+2)ν1,p2ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}),
where ˜βn is a point between ˆβn and β0, the second inequality is based on the boundedness ofg′
0, and the
third inequality is obtained by solving the initial value problem in (S14) along with the boundedness
of ˆγn, ˆgn, ˆg′
n, ˆg′′
n and Λ′
β(·,˜βn,ˆγn,ˆgn). By a similar argument that we used for the second term in I1n,
we have for J3,
J3 = ∥g′
0(Λ0(·))Λ′′
γγ(·,β0,˜γn,ˆgn)[v∗
j −v∗
j,n,ˆγn −γ0]∥2
≲ ∥v∗
j −v∗
j,n∥∞·∥ˆγn −γ0∥2 = Op(n−min{(p1+2)ν1,p2ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}),
and for J4
J4 = ∥g′
0(Λ0(·))Λ′′
γg(·,β0,γ0,˜gn)[v∗
j −v∗
j,n,ˆgn −g0]∥2
≲ ∥Λ′′
γg(·,β0,γ0,˜gn)[v∗
j −v∗
j,n,ˆgn −g0]∥2
≲ (∥ˆgn(Λ(·,ˆβn,ˆγn,ˆgn)) −g0(Λ0(·))∥2 + ∥ˆg′
n(Λ(·,ˆβn,ˆγn,ˆgn)) −g′
0(Λ0(·))∥2) ·∥v∗
j −v∗
j,n∥∞
= Op(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }) ·O(n−2ν1 ),
where ˜γn is a point between ˆγn and γ0 and ˜gn is a point between ˆgn and g0. Thus, we have
∥ˆζ′
n,γ(·,ˆβn,ˆγn)[v∗
j −v∗
j,n] −ζ′
0γ(·,β0,γ0)[v∗
j −v∗
j,n]∥2
≲Op(n−min{(p1+2)ν1,(p2−1)ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}),
39
and it follows that for the third term in I1n is bounded by
(P{
⏐⏐l′′
γζ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆζn −ζ0]
⏐⏐})2
≤P{
⏐⏐l′′
γζ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[v∗
j −v∗
j,n,ˆζn −ζ0]
⏐⏐2}
=P
{⏐⏐⏐∆(ˆζ′
n,γ(Y,X, ˆβn,ˆγn)[v∗
j −v∗
j,n] −ζ′
0γ(Y,X,β 0,γ0)[v∗
j −v∗
j,n])
−
∫ τ
0
1 (Y ≥t) exp
(
XT ˜βn + ˜γn(t) + ˜gn(˜Λ(t,X))
)
·{(ˆζn(t,X, ˆβn,ˆγn) −ζ0(t,X,β 0,γ0))
·((v∗
j −v∗
j,n)(t) + ˜g′
n(˜Λ(t,X))˜Λ′
γ(t,X)[v∗
j −v∗
j,n])
+ ˆζ′
n,γ(t,X, ˆβn,ˆγn)[v∗
j −v∗
j,n] −ζ′
0γ(t,X,β 0,γ0)[v∗
j −v∗
j,n]}dt
⏐⏐⏐
2}
≲∥ˆζ′
n,γ(·,ˆβn,ˆγn)[v∗
j −v∗
j,n] −ζ′
0γ(·,β0,γ0)[v∗
j −v∗
j,n]∥2
2 + ∥ˆζn(·,ˆβn,ˆγn) −ζ0(·,β0,γ0)∥2
2 ·∥v∗
j −v∗
j,n∥2
∞
=Op(n−2 min{(p1+2)ν1,(p2−1)ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}).
Thus, we have I1n = Op(n−min{(p1+2)ν1,(p2−1)ν2+2ν1,1−max{ν1,ν2}
2 +2ν1}) = op(n−1/2), because (p1 +2)ν1 >
1/2, (p2 −1)ν2 + 2ν1 >1/2, and 4ν1 >max{ν1,ν2}under the restrictions listed in Theorem 1.
Next we will use the maximal inequality in Lemma 3.4.2 of Van Der Vaart and Wellner (1996) (on
page 324) and the Markov’s inequality to show that I2n = op(n−1/2). By Lemma 7, the ϵ-bracketing
number associated with ∥·∥ ∞ norm for the class Fγ
n,j(η) is bounded by ( η/ϵ)c1qn1 +c2qn2 +d, which
implies that
log N[ ] (ϵ,Fγ
n,j(η),L2(P)) ≤log N[ ] (ϵ,Fγ
n,j(η),∥·∥∞) ≲ (c1qn1 + c2qn2 ) log(η/ϵ).
It follows that the bracketing integral satisﬁes
J[ ] (ϵ,Fγ
n,j(η),L2(P)) =
∫ η
0
√
1 + logN[ ] (ϵ,Fγ
n,j(η),L2(P))dϵ≲ (c1qn1 + c2qn2 )1/2η.
Here we choose ηn = O(n−min{2ν1,p2ν2,1−max{ν1,ν2}
2 }) such that ∥v∗
j −v∗
j,n∥∞ = O(n−2ν1 ) ≤ηn and
d(ˆθn,θ0) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }) ≤ηn for p1 ≥2, then l′
γ(ˆθn); W)[v∗
j −v∗
j,n] ∈Fγ
n,j(ηn). For
any l′
γ(θ; W)[v∗
j −vj] ∈Fγ
n,j(ηn), we have
P{l′
γ(θ; W)[v∗
j −vj]}2
=P{∆((v∗
j −vj)(Y) + g′(Λ(Y,X,β,γ,g ))Λ′
γ(Y,X,β,γ,g )[v∗
j −vj])
−
∫ Y
0
exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
{(v∗
j −vj)(t) + ζ′
γ(t,X,β,γ )[v∗
j −vj]}dt}2
≲∥v∗
j −vj∥2
∞+ ∥Λ′
γ(·,β,γ,g )[v∗
j −vj]∥2
∞
≲∥v∗
j −vj∥2
∞.
Also, sup θ:d(θ,θ0)≤ηn;vj:∥v∗
j−vj∥∞≤ηn |l′
γ(θ; W)[v∗
j −vj]|is bounded by some constant 0 < M <∞(or
slowly growing with nand it can be treated as bounded by the same argument used in Shen and Wong
40
(1994, page 591)). By the maximal inequality, it follows that
EP∥Gn∥Fγ
n,j(ηn) ≲ J[ ] (ϵ,Fγ
n,j(ηn),L2(P))
(
1 +
J[ ] (ϵ,Fγ
n,j(ηn),L2(P))
η2n
√n M
)
≲ (c1qn1 + c2qn2 )1/2ηn + (c1qn1 + c2qn2 )n−1/2
= O(n
max{ν1,ν2}
2 ) ·O(n−min{2ν1,p2ν2,1−max{ν1,ν2}
2 }) + O(nmax{ν1,ν2}−1/2)
= O(n−min{2ν1−max{ν1,ν2}
2 ,p2ν2−max{ν1,ν2}
2 ,1/2−max{ν1,ν2}}) + O(nmax{ν1,ν2}−1/2)
= o(1),
where Gn = √n(Pn−P) and the last equality holds because 0 <ν1,ν2 <1/2, 4ν1 >max{ν1,ν2}, and
p2ν2 >2ν2 >max{ν1,ν2}. Then by the Markov’s inequality, we have
I2n = n−1/2Gnl′
γ(ˆθn; W)[v∗
j −v∗
j,n] = op(n−1/2).
By combining I1n = op(n−1/2) and I2n = op(n−1/2), we have S′
γ,n(ˆθn)[v∗
j] = op(n−1/2).
Next, to verify the last part of (A4), we need to show thatS′
ζ,n(ˆθn)[h∗
j] = op(n−1/2) with h∗
j(·,ˆβn,ˆγn) =
w∗
j(ˆΛ(·)) + ˆg′
n(ˆΛ(·))ˆΛ′
g(·)[w∗
j], where we write ˆΛ(·) = Λ( ·,ˆβn,ˆγn,ˆgn) for notational simplicity. Since
w∗
j ∈G2, by Lemma 5 there exists w∗
j,n ∈G2
n such that ∥w∗
j,n −w∗
j∥∞ = O(n−2ν2 ). It follows that
S′
ζ,n( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn))[h∗
j,n] = 0 with h∗
j,n(·,ˆβn,ˆγn) = w∗
j,n(ˆΛ(·)) + ˆg′
n(ˆΛ(·))ˆΛ′
g(·)[w∗
j,n]. Then it
suﬃces to show that for each 1 ≤j ≤d,
S′
ζ,n(ˆθn)[h∗
j] = S′
ζ,n(ˆθn)[h∗
j −h∗
j,n]
= P{l′
ζ(ˆθn; W)[h∗
j −h∗
j,n] −l′
ζ(θ0; W)[h∗
j −h∗
j,n]}+ (Pn −P){l′
ζ(ˆζn; W)[h∗
j −h∗
j,n]}
= I3n + I4n = op(n−1/2),
since S′
ζ(θ0)[h∗
j −h∗
j,n] = 0. We will take the similar arguments used in the proof of S′
γ,n(ˆθn)[v∗
j] =
op(n−1/2) to show that both I3n and I4n equal to op(n−1/2).
For I3n, using the Taylor expansion for l′
ζ(ˆθn)[h∗
j −h∗
j,n] at θ0, we have
I3n =P{( ˆβn −β0)Tl′′
βζ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[h∗
j −h∗
j,n]
+ l′′
ζγ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[h∗
j −h∗
j,n,ˆγn −γ0]
+ l′′
ζζ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn); W)[h∗
j −h∗
j,n,ˆζn −ζ0]}
where ( ˜βn,˜γn(·),˜ζn(·,˜βn,˜γn)) is some point between θ0 and ˆθn. Let ˜Λ(t,x) = Λ(t,x, ˜βn,˜γn,˜gn). Note
that by solving initial value problems in Lemma 1, we have ˜Λ′
β(t,x) is bounded on t ∈[0,τ] and
x∈X based on the boundedness of ˜γn, ˜gn, and ˜g′
n. Also, we have ∥˜Λ′
g(·)[w]∥∞≲ ∥w∥∞, ∥˜Λ′
γ(·)[v]∥2 ≲
∥v∥2, and furthermore, sup t∈[0,τ],x∈X∥˜Λ′′
gβ(t,x)[w]∥≲ ∥w∥∞+ ∥w′∥∞ and ∥˜Λ′′
gγ(·)[w,v]∥2 ≲ (∥w∥∞+
41
∥w′∥∞)∥v∥2. Using the triangle inequality, it follows that
∥(h∗
j −h∗
j,n)(·,˜βn,˜γn)∥∞= ∥(w∗
j −w∗
j,n)(˜Λ(·)) + ˜g′
n(˜Λ(·))˜Λ′
g(·)[w∗
j −w∗
j,n]∥∞
≲ ∥w∗
j −w∗
j,n∥∞,
sup
t∈[0,τ],x∈X
∥(h∗
j −h∗
j,n)′
β(t,x, ˜βn,˜γn)∥= sup
t∈[0,τ],x∈X
∥(w∗
j −w∗
j,n)′(˜Λ(t,x))˜Λ′
β(t,x)
+ ˜g′
n(˜Λ(t,x))˜Λ′′
gβ(t,x)[w∗
j −w∗
j,n]
+ ˜g′′
n(˜Λ(t,x))˜Λ′
g(t,x)[w∗
j −w∗
j,n]˜Λ′
β(t,x)∥
≲ ∥w∗
j −w∗
j,n∥∞+ ∥(w∗
j −w∗
j,n)′∥∞,
and
∥(h∗
j −h∗
j,n)′
γ(·,˜βn,˜γn)[ˆγn −γ0]∥2 = ∥(w∗
j −w∗
j,n)′(˜Λ(·))˜Λ′
γ(·)[ˆγn −γ0]
+ ˜g′
n(˜Λ(·))˜Λ′′
gγ(·)[w∗
j −w∗
j,n,ˆγn −γ0]
+ ˜g′′
n(˜Λ(·))˜Λ′
g(·)[w∗
j −w∗
j,n]˜Λ′
γ(·)[ˆγn −γ0]∥2
≲ (∥w∗
j −w∗
j,n∥∞+ ∥(w∗
j −w∗
j,n)′∥∞)∥ˆγn −γ0∥2.
Therefore, by plugging the derivatives in Lemma 4 and using the triangle inequality and the Cauchy-
Schwarz inequality, I3n is dominated by
I3n ≲ sup
t∈[0,τ],x∈X
|( ˆβn −β0)T(h∗
j −h∗
j,n)′
β(t,x, ˜βn,˜γn)|+ ∥(h∗
j −h∗
j,n)′
γ(·,˜βn,˜γn)[ˆγn −γ0]∥2
+ ∥(h∗
j −h∗
j,n)(·,˜βn,˜γn)∥∞·P
{∫ τ
0
exp
(
XT ˜βn + ˜γn(t) + ˜gn(˜Λ(t,X))
)
·
(
(X+ ˜g′
n(˜Λ(t,X))T( ˆβn −β0) + ˆγn(t) −γ0(t) + ˜g′
n(˜Λ(t,X))˜Λ′
γ(t,X)[ˆγn −γ0]
+ ˆζn(t,X) −ζ0(t,X)
)2dt
}1/2
≲ ∥ˆβn −β0∥· sup
t∈[0,τ],x∈X
∥(h∗
j −h∗
j,n)′
β(t,x, ˜βn,˜γn)∥+ ∥(h∗
j −h∗
j,n)′
γ(·,˜βn,˜γn)[ˆγn −γ0]∥2
+ ∥(h∗
j −h∗
j,n)(·,˜βn,˜γn)∥∞·
(
∥ˆβn −β0∥2 + ∥ˆγn −γ0∥2
2 + ∥ˆζn −ζ0∥2
2
)1/2
≲ (∥w∗
j −w∗
j,n∥∞+ ∥(w∗
j −w∗
j,n)′∥∞)d(ˆθn,θ0).
Based on the Corollary 6.21 in Schumaker (2007), we have ∥(w∗
j −w∗
j,n)′∥∞= O(n−ν2 ) and
I3n = O(n−ν2 ) ·Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 })
= Op(n−min{p1ν1+ν2,(p2+1)ν2,1−max{ν1,ν2}
2 +ν2})
= op(n−1/2),
where the last equality holds because p1ν1 + ν2 >1/2, (p2 + 1)ν2 >1/2, and 2ν2 >max{ν1,ν2}.
Next, we use the maximal inequality and the Markov’s inequality to show that I4n = op(n−1/2).
42
By Lemma 7, the ϵ-bracketing number associated with ∥·∥∞ norm for the class Fζ
n,j(η) is bounded
by (η/ϵ)c3qn1 +c4qn2 +d, which implies that
log N[ ] (ϵ,Fζ
n,j(η),L2(P)) ≤log N[ ] (ϵ,Fζ
n,j(η),∥·∥∞) ≲ (c3qn1 + c4qn2 ) log(η/ϵ).
It follows that the bracketing integral satisﬁes
J[ ] (ϵ,Fζ
n,j(η),L2(P)) =
∫ η
0
√
1 + logN[ ] (ϵ,Fζ
n,j(η),L2(P))dϵ≲ (c3qn1 + c4qn2 )1/2η.
Here we choose ηn = O(n−min{p1ν1,2ν2,1−max{ν1,ν2}
2 }) such that ∥w∗
j −w∗
j,n∥∞ = O(n−2ν2 ) ≤ηn and
d(ˆθn,θ0) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }) ≤ηn for p2 ≥3, then l′
ζ(ˆθn; W)[h∗
j −h∗
j,n] ∈Fζ
n,j(ηn). For
any l′
ζ(θ; W)[h∗
j −hj] ∈Fζ
n,j(ηn), we have
P{l′
ζ(θ; W)[h∗
j −hj]}2
=P{∆((w∗
j −wj)(Y) + g′(Λ(Y,X,β,γ,g ))Λ′
g(Y,X,β,γ,g )[w∗
j −wj])
−
∫ Y
0
exp
(
XTβ+ γ(t) + g(Λ(t,X,β,γ,g ))
)
{(w∗
j −wj)(t) + ζ′
g(t,X,β,γ )[w∗
j −wj]}dt}2
≲∥w∗
j −wj∥2
∞+ ∥Λ′
g(·,β,γ,g )[w∗
j −wj]∥2
∞
≲∥w∗
j −wj∥2
∞≤ηn.
Also, supθ:d(θ,θ0)≤ηn;wj:∥w∗
j−wj∥∞≤ηn |l′
ζ(θ; W)[h∗
j −hj]|is bounded by some constant 0 < M <∞. By
the maximal inequality, it follows that
EP∥Gn∥Fζ
n,j(ηn) ≲ J[ ] (ϵ,Fζ
n,j(ηn),L2(P))
(
1 +
J[ ] (ϵ,Fζ
n,j(ηn),L2(P))
η2n
√n M
)
≲ (c3qn1 + c4qn2 )1/2ηn + (c3qn1 + c4qn2 )n−1/2
= O(n
max{ν1,ν2}
2 ) ·O(n−min{p1ν1,2ν2,1−max{ν1,ν2}
2 }) + O(nmax{ν1,ν2}−1/2)
= O(n−min{p1ν1−max{ν1,ν2}
2 ,2ν2−max{ν1,ν2}
2 ,1/2−max{ν1,ν2}}) + O(nmax{ν1,ν2}−1/2)
= o(1),
where the last equality holds because 0 < ν1,ν2 < 1/2, 2 ν2 > max{ν1,ν2}> max{ν1,ν2}/2, and
p1ν1 ≥2ν1 >max{ν1,ν2}/2 for p1 ≥2. Then by the Markov’s inequality, we have
I4n = n−1/2Gnl′
ζ(ˆθn; W)[h∗
j −h∗
j,n] = op(n−1/2).
By combining I3n = op(n−1/2) and I4n = op(n−1/2), we verify that S′
ζ,n(ˆθn)[h∗
j] = op(n−1/2). This
completes the veriﬁcation of the assumption (A4).
Now we verify assumption (A5). Since the proofs of three stochastic equicontinuity equations are
essentially based on the identical arguments, we only present the proof of the ﬁrst equation as follows.
43
First, by Lemma 8, theϵ-bracketing number associated with∥·∥∞norm for the classF∗β
n,j(η) is bounded
by (η/ϵ)c1qn1 +c2qn2 +d, which implies that the bracketing integral is bounded by (c1qn1 +c2qn2 )1/2η, i.e.
J[ ] (ϵ,F∗β
n,j(η),L2(P)) ≲ (c1qn1 + c2qn2 )1/2η.
For any l′
βj(θ; W) −l′
βj(θ0; W) ∈F∗β
n,j(ηn), by taking the Taylor expansion at θ0, it follows that
l′
βj(θ; W) −l′
βj(θ0; W) =(β−β0)Tl′′
βjβ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)
+ l′′
βjγ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)[γ−γ0]
+ l′′
βjζ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)[ζ−ζ0]
where ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ)) is some point between θ0 and θ. By applying the triangle inequality and the
Cauchy-Schwarz inequality, we have
P{l′
βj(θ; W) −l′
βj(θ0; W)}2 ≤∥β−β0∥2P{∥l′′
βjβ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)∥2}
+ P{l′′
βjγ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)[γ−γ0]}2
+ P{l′′
βjζ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ); W)[ζ−ζ0]}2
=B1 + B2 + B3.
For B1, by Lemma 4, l′′
βjβ(˜θ; W) is bounded and it follows that B1 ≲ ∥β−β0∥2. For B2, since ˜g, ˜g′,
˜g′′, ˜Λ′
βj(t,x) are bounded and ∥˜Λ′′
βjγ(·)[v]∥2 ≲ ∥v∥2, by applying the Cauchy-Schwarz inequality and
the same arguments that are used in Lemma 2 to prove that linear operators are bounded above, it
follows that
B2 =P
{
∆˜ζ′′
βjγ(Y,X)[γ−γ0] −
∫ Y
0
(
˜ζ′′
βjγ(t,X)[γ−γ0] + (Xj + ˜g′(˜Λ(t,X))˜Λ′
βj(t,X))
·(γ(t) −γ0(t) + ˜g′(˜Λ(t,X))˜Λ′
γ(t,X)[γ−γ0])
)
d˜Λ(t,X)
}2
≲P{
∫ Y
0
(˜ζ′′
βjγ(t,X)[γ−γ0])2dΛ0(t,X)}
+ P{
∫ Y
0
(γ(t) −γ0(t) + ˜g′(˜Λ(t,X))˜Λ′
γ(t,X)[γ−γ0])2d˜Λ(t,X)}
≲P{
∫ Y
0
(˜g′(˜Λ(t,X))˜Λ′′
βjγ(t,X)[γ−γ0])2dΛ0(t,X)}
+ P{
∫ Y
0
(˜g′′(˜Λ(t,X))˜Λ′
γ(t,X)[γ−γ0]˜Λ′
βj(t,X))2dΛ0(t,X)}
+ P{
∫ Y
0
(γ(t) −γ0(t) + ˜g′(˜Λ(t,X))˜Λ′
γ(t,X)[γ−γ0])2d˜Λ(t,X)}
≲∥γ−γ0∥2
2 ≤η2.
44
For B2, similarly, we can show that
B3 =P
{
−
∫ Y
0
(
(ζ′
βj −ζ′
0βj)(t,X) + (Xj + ˜g′(˜Λ(t,X))˜Λ′
βj(t,X))(ζ−ζ0)(t,X)d˜Λ(t,X)
+ ∆(ζ′
βj −ζ′
0βj)(Y,X)
}2
≲∥ζ−ζ0∥2
2 + ∥ζ′
βj −ζ′
0βj∥2
2 ≤η2 + ∥ζ′
βj −ζ′
0βj∥2
2.
Furthermore, by using the triangle inequality together with the boundedness of Λ ′
βj and g′
0, it follows
that
∥ζ′
βj −ζ′
0βj∥2
2 = ∥g′(Λ(·,β,γ,g ))Λ′
βj(·,β,γ,g ) −g′
0(Λ0(·))Λ′
0βj(·)∥2
2
≤∥g′(Λ(·,β,γ,g ))Λ′
βj(·,β,γ,g ) −g′
0(Λ0(·))Λ′
βj(·,β,γ,g )∥2
2
+ ∥g′
0(Λ0(·))Λ′
βj(·,β,γ,g ) −g′
0(Λ0(·))Λ′
0βj(·)∥2
2
≲ ∥g′(Λ(·,β,γ,g )) −g′
0(Λ0(·))∥2
2 + ∥Λ′
βj(·,β,γ,g ) −Λ′
0βj(·)∥2
2
≲ ∥g′(Λ(·,β,γ,g )) −g′
0(Λ0(·))∥2
2 + d2(θ,θ0) ≤η2.
Therefore, we have P{l′
βj(θ; W) −l′
βj(θ0; W)}2 ≲ η2. By Lemma 4, we also have ∥l′
βj(θ; W) −
l′
βj(θ0; W)∥∞ is bounded. We choose ηn = O(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }). Then by the maxi-
mal inequality, it follows that
EP∥Gn∥F∗β
n,j(ηn) ≲ (c1qn1 + c2qn2 )1/2ηn + (c1qn1 + c2qn2 )n−1/2
= O(n
max{ν1,ν2}
2 ) ·O(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }) + O(nmax{ν1,ν2}−1/2)
= O(n−min{p1ν1−max{ν1,ν2}
2 ,(p2−1)ν2−max{ν1,ν2}
2 ,1/2−max{ν1,ν2}}) + O(nmax{ν1,ν2}−1/2)
= o(1),
where the last equality holds because p1ν1 ≥ν1 > max{ν1,ν2}/2, (p2 −1)ν2 ≥2ν2 > max{ν1,ν2}/2
for p2 ≥ 3, and 0 < ν1,ν2 < 1/2. Thus, for ξ = min {p1ν1,p2ν2,1−max{ν1,ν2}
2 } and Cn−ξ =
O(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 ), by Markov’s inequality, we have
sup
d(θ,θ0)≤Cn−ξ,θ∈Θn
|Gn{l′
βj(θ; W) −l′
βj(θ0; W)}|= op(1),
which completes the veriﬁcation of the ﬁrst equation in the assumption (A5). The other two stochastic
equicontinuity equations in (A5) can be veriﬁed using the same arguments.
Finally, we verify assumption (A6) using the Taylor expansion. Similarly, we just prove the ﬁrst
equation, since the proofs of the other two equations are based on the same arguments. By taking the
Taylor expansion of l′
β(θ; W) at θ0, it follows that
l′
β(θ; W) −l′
β(θ0; W) =l′′
ββ(˜θ; W)(β−β0) + l′′
βγ(˜θ; W)[γ−γ0] + l′′
βζ(˜θ; W)[ζ−ζ0]
45
where ˜θ= ( ˜β,˜γ(·),˜ζ(·,˜β,˜γ)) is a point between θ and θ0. Thus,
P{l′
β(θ; W) −l′
β(θ0; W) −l′′
ββ(θ0; W)(β−β0) −l′′
βγ(θ0; W)[γ−γ0] −l′′
βζ(θ0; W)[ζ−ζ0]}
= P
{
(l′′
ββ(˜θ; W) −l′′
ββ(θ0; W))(β−β0)
}
+ P
{
l′′
βγ(˜θ; W)[γ−γ0] −l′′
βγ(θ0; W)[γ−γ0]
}
+ P
{
l′′
βζ(˜θ; W)[ζ−ζ0] −l′′
βζ(θ0; W)[ζ−ζ0]
}
After some direct calculation, we have
⏐⏐⏐P
{
l′′
ββ(˜θ; W) −l′′
ββ(θ0; W)
}⏐⏐⏐
≤P
{∫ Y
0
⏐⏐⏐
(
exp
(
XTβ0 + γ0(t) + ζ0(t,X)
)
−exp
(
XT ˜β+ ˜γ(t) + ˜ζ(t,X)
))
˜ζ′′
ββ(t,X)
⏐⏐⏐dt
}
+ P
{⏐⏐⏐
∫ Y
0
(X+ ζ′
0β(t,X))(X+ ζ′
0β(t,X))T −(X+ ˜ζ′
β(t,X))(X+ ˜ζ′
β(t,X))TdΛ0(t,X)
⏐⏐⏐
}
+ P
{∫ Y
0
⏐⏐⏐
(
exp
(
XTβ0 + γ0(t) + ζ0(t,X)
)
−exp
(
XT ˜β+ ˜γ(t) + ˜ζ(t,X)
))
·(X+ ˜ζ′
β(t,X))(X+ ˜ζ′
β(t,X))T
⏐⏐⏐dt
}
=K1 + K2 + K3.
For K1, by the mean value theorem and the Cauchy-Schwarz inequality, it follows that
K1 = P
{∫ Y
0
⏐⏐⏐exp
(
˜ψ(t,X)
)(
XT(β0 −˜β) + (γ0 −˜γ)(t) + ζ0(t,X) −˜ζ(t,X)
)
˜ζ′′
ββ(t,X)
⏐⏐⏐dt
}
≲ ∥β0 −˜β∥+ ∥γ0 −˜γ∥2 + ∥ζ0 −˜ζ∥2 ≤d(θ0,θ)
= O(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }),
where ˜ψ(t,X) = XTβ0 + γ0(t) +ζ0(t,X) +ξ(XT( ˜β−β0) + ˜γ(t) −γ0(t) + ˜ζ(t,X) −ζ0(t,X)) for some
ξ ∈(0,1) and is bounded. For K2, by the Cauchy-Schwarz inequality and the same arguments that
are used to verify assumption (A4), we have
K2 ≲ P
{∫ τ
0
⏐⏐⏐(ζ′
0β(t,X) −˜ζ′
β(t,X))(X+ ζ′
0β(t,X) + ˜ζ′
β(t,X))T
⏐⏐⏐
2
dΛ0(t,X)
}1/2
≲ ∥ζ′
0β(·) −˜ζ′
β(·)∥2
≲ d(θ0,θ) + ∥g′
0(Λ0(·)) −g(Λ(·,β,γ,g ))∥2
= O(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }).
For K3, by applying the same arguments for K1, we can show that
K3 ≲ ∥β0 −˜β∥+ ∥γ0 −˜γ∥2 + ∥ζ0 −˜ζ∥2 = O(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }).
46
Therefore,
P
{⏐⏐⏐(l′′
ββ(˜θ; W) −l′′
ββ(θ0; W))(β−β0)
⏐⏐⏐
}
= O(n−min{p1ν1,(p2−1)ν2,1−max{ν1,ν2}
2 }) ·O(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 })
= O(n−min{2p1ν1,p1ν1+(p2−1)ν2,(2p2−1)ν2,1
2 +p1ν1−max{ν1,ν2}
2 ,1
2 +(p2−1)ν2−max{ν1,ν2}
2 ,1−max{ν1,ν2}})
= o(n−1/2),
where the last equality holds because p1 ≥2 and p2 ≥3, thus 2 p1ν1 > p1/(p1 + 2) ≥1/2, p1ν1 +
(p2 −1)ν2 > p1
2(p1+2) + p2−1
2(p2+1) ≥ 1
2·2 + 1
2·2 = 1
2 , (2p2 −1)ν2 > 2p2−1
2(p2+1) > 1
2 , p1ν1 ≥2ν1 > max{ν1,ν2}
2 ,
(p2 −1)ν2 >ν2 > max{ν1,ν2}
2 , and max{ν1,ν2}<1/2. Similarly, we can show that
P
{⏐⏐⏐l′′
βγ(˜θ; W)[γ−γ0] −l′′
βγ(θ0; W)[γ−γ0]
⏐⏐⏐
}
= O(n−min{2p1ν1,p1ν1+(p2−1)ν2,(2p2−1)ν2,1
2 +p1ν1−max{ν1,ν2}
2 ,1
2 +(p2−1)ν2−max{ν1,ν2}
2 ,1−max{ν1,ν2}})
= o(n−1/2)
and
P
{⏐⏐⏐l′′
βζ(˜θ; W)[ζ−ζ0] −l′′
βζ(θ0; W)[ζ−ζ0]
⏐⏐⏐
}
= O(n−min{2p1ν1,p1ν1+(p2−1)ν2,(2p2−1)ν2,1
2 +p1ν1−max{ν1,ν2}
2 ,1
2 +(p2−1)ν2−max{ν1,ν2}
2 ,1−max{ν1,ν2}})
= o(n−1/2).
Thus, it follows that
P{l′
β(θ; W) −l′
β(θ0; W) −l′′
ββ(θ0; W)(β−β0) −l′′
βγ(θ0; W)[γ−γ0] −l′′
βζ(θ0; W)[ζ−ζ0]}
= O(n−min{2p1ν1,p1ν1+(p2−1)ν2,(2p2−1)ν2,1
2 +p1ν1−max{ν1,ν2}
2 ,1
2 +(p2−1)ν2−max{ν1,ν2}
2 ,1−max{ν1,ν2}}) = O(n−αξ)
where α= min{2p1ν1,p1ν1 +(p2 −1)ν2,(2p2 −1)ν2,1
2 +p1ν1 −max{ν1,ν2}
2 ,1
2 +(p2 −1)ν2 −max{ν1,ν2}
2 ,1−
max{ν1,ν2}}/min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }>1 and αξ >1/2. This completes the veriﬁcation of (A6).
Therefore, we have veriﬁed (A1)-(A6) and by Theorem 3, we have
√n( ˆβn −β0) = A−1√nPnl∗(β0,γ0,ζ0; W) + op(1) →d N(0,A−1B(A−1)T),
where l∗(β0,γ0,ζ0; W) = l′
β(β0,γ0,ζ0; W) −l′
γ(β0,γ0,ζ0; W)[v∗] −l′
ζ(β0,γ0,ζ0; W)[h∗(·,β0,γ0)] and
A is given by P{l∗(β0,γ0,ζ0; W)⊗2} = I(β0), as shown in the above veriﬁcation of (A3). Thus,
A= B = I(β0) and A−1B(A−1)T = I−1(β0). Therefore, we have
√n( ˆβn −β0) = √nI−1(β0)Pnl∗(β0,γ0,ζ0; W) + op(1) →d N(0,I−1(β0)),
which completes the proof.
47
3.4 Explanation of Condition (C7)
Condition (C7) assumes the existence of the least favorable directions which are essential for semi-
parametric eﬃciency. We may ﬁnd v∗ and w∗ through equations in (C7). Speciﬁcally, v∗ and w∗
need to satisfy P{∆A∗(U,X)ψ′
0γ(Y,X)[v]}= 0 and P{∆A∗(U,X)ψ′
0g(Y,X)[w]}= 0 for any v ∈Γp1
and w∈Gp2 .
For the ﬁrst equation, using the fact ofP{
∫Y
0 f(t,X)dΛ0(t,X)}= P{∆f(Y,X)}and the equations
in (S19), we have for any v∈Γp1
P{∆A∗(U,X)ψ′
0γ(Y,X)[v]}
=P{
∫ Y
0
A∗(R(t)eXTβ0 ,X)
·
(
g′
0(Λ0(t,X)) exp(g0(Λ0(t,X)))eXTβ0
∫ t
0
exp(γ0(s))v(s)ds+ v(t)
)
dΛ0(t,X)}
=P{
∫ R(Y)eXTβ0
0
exp
(
g0(˜Λ0(t))
)
A∗(t,X)
·
(
g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ t
0
v(R−1(e−XTβ0 s))ds+ v(R−1(e−XTβ0 t)
)
dt}
=P{
∫ R(Y)eXTβ0
0
v(R−1(e−XTβ0 s))
∫ R(Y)eXTβ0
s
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)dtds
+
∫ R(Y)eXTβ0
0
v(R−1(e−XTβs)) exp
(
g0(˜Λ0(s))
)
A∗(s,X)ds}
=P{
∫ ∞
0
1 (R(Y) ≥s) ·v(R−1(s)) ·eXTβ0
·


∫ R(Y)eXTβ0
seXTβ0
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)dt+ exp
(
g0(˜Λ0(seXTβ0 ))
)
A∗(seXTβ0 ,X)

ds}
=
∫ ∞
0
v(R−1(s)) ·P
{
1 (R(Y) ≥s) ·eXTβ0
·


∫ R(Y)eXTβ0
seXTβ0
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)dt+ exp
(
g0(˜Λ0(seXTβ0 ))
)
A∗(seXTβ0 ,X)


}
ds,
(S36)
where the second equality is obtained by the variable transformation ˜t = R(t)xXTβ0 and further
replacing the notation ˜t with t in the integral, and the third equality holds by switching the order
of integration. To make the equation (S36) equal to zero for any v ∈Γp1 , we can take v∗ and w∗
48
satisfying
P{
∫ R(Y)eXTβ0
seXTβ0
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)eXTβ0 dt}
= −P{1 (R(Y) ≥s) exp
(
g0(˜Λ0(seXTβ0 ))
)
A∗(seXTβ0 ,X)eXTβ0 }. (S37)
For the second equation in (C7), similarly, we have
P{∆A∗(U,X)ψ′
0g(Y,X)[w]}
=P{
∫ Y
0
A∗(R(t)eXTβ0 ,X)
·
(
g′
0(Λ0(t,X)) exp(g0(Λ0(t,X)))
∫ Λ0(t,X)
0
exp(−g0(s))w(s)ds+ w(Λ0(t,X))
)
dΛ0(t,X)}
=P{
∫ R(Y)eXTβ0
0
exp
(
g0(˜Λ0(t))
)
A∗(t,X)
·
(
g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ t
0
w(˜Λ0(s))ds+ w(˜Λ0(t))
)
dt}
=P{
∫ U
0
(∫ U
s
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)dt+ exp
(
g0(˜Λ0(η))
)
A∗(t,X)
)
·w(˜Λ0(s))ds}
=
∫ ∞
0
w(˜Λ0(s)) ·P{
∫ U
s
g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
A∗(t,X)dt+ 1 (U ≥s) exp
(
g0(˜Λ0(s))
)
A∗(s,X)}ds.
To make it equal to zero for any w∈Gp2 , we can take v∗and w∗such that, for any η, A∗(t,x) satisﬁes
∫ ∞
s
P{1 (U ≥t)A∗(t,X)}g′
0(˜Λ0(t)) exp
(
2g0(˜Λ0(t))
)
dt
= −exp
(
g0(˜Λ0(s))
)
P{1 (U ≥s)A∗(s,X)}. (S38)
By taking derivatives with respect to s on both sides, we have
exp
(
g0(˜Λ0(s))
)dP{1 (U ≥s)A∗(s,X)}
ds = 0,
which implies that P{1 (U ≥s)A∗(s,X)}is a constant. Then equation (S38) holds only if
P{1 (U ≥s)A∗(s,X)}= 0. (S39)
Therefore, we can take v∗and w∗such that A∗(t,x) satisﬁes equations (S37) and (S39).
Next, we provide solutions for the Cox model and the linear transformation model with a known
transformation function as illustration.
For the Cox model where g0 ≡0, it suﬃces to ﬁnd v∗such that the equation in (S37) holds with
A∗(t,x) = −x+ v∗(R−1(te−xTβ0 )), which implies that P{1 (R(Y) ≥t)eXTβ0 (v∗(R−1(t)) −X)}= 0.
49
We can take
v∗(t) = P{1 (Y ≥t)eXTβ0 X}
P{1 (Y ≥t)eXTβ0 } .
For the linear transformation model whereγ0 is known, it suﬃces to ﬁnd w∗such that the equation
in (S39) holds with
A∗(t,x) = −(g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1)x
+ g′
0(˜Λ0(t) exp
(
g0(˜Λ0(t))
)∫ ˜Λ0(t)
0
exp(−g0(s))w∗(s)ds+ w∗(˜Λ0(t)).
It follows that w∗satisﬁes
g′
0(˜Λ0(t) exp
(
g0(˜Λ0(t))
)∫ ˜Λ0(t)
0
exp(−g0(s))w∗(s)ds+ w∗(˜Λ0(t))
= (g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1)P{1 (U ≥t)X}
P{1 (U ≥t)} .
By taking the variable transformation ˜t= ˜Λ0(t) and further replacing ˜t with t, it is suﬃcient to take
w∗such that g′
0(t) exp(g0(t))
∫t
0 exp(−g0(s))w∗(s)ds+ w∗(t) = φφφ(t) where φφφ(t) is given by
φφφ(t) =
(
g′
0(t) exp(g0(t))˜Λ−1
0 (t) + 1
)P{1 (Λ0(Y,X) ≥t)X}
P{1 (Λ0(Y,X) ≥t)} .
It is straightforward to verify that w∗can be taken as w∗(t) = φφφ(t) −g′
0(t)
∫t
0 φφφ(s)ds.
3.5 Simpliﬁcation of Condition (C8)
Condition (C8) assumes non-singularity assumption of the information matrix. We may simplify it to
some suﬃcient conditions if we can ﬁnd the least favorable directions required in the condition (C7).
Recall that we have provided explicit constructions of the least favorable directions for the Cox model
and for the linear transformation model with a known transformation respectively in Section 3.4. We
further reduce the non-singularity assumption for the above two cases as follows.
For the Cox model, we have g0 ≡0, ˜Λ0(t) ≡t, and the least favorable function v∗can be derived
as
v∗(t) = P{1 (Y ≥t)eXTβ0 X}
P{1 (Y ≥t)eXTβ0 } .
It follows that the eﬃcient score for β is
l∗(β0,γ0; W) =
∫ ∞
0
A∗(t,x)dM(t) =
∫ ∞
0
[−X+ P{1 (U ≥t)eXTβ0 X}
P{1 (U ≥t)eXTβ0 } ]dM(t),
where U = eXTβ0
∫Y
0 exp(γ0(s))ds as deﬁned in (C5) and M(t) = ∆1 (U ≤t) −
∫t
0 1 (U ≥s)ds is the
event counting process martingale. Let µ(t) = P{1 (U≥t)eXTβ0 X}
P{1 (U≥t)eXTβ0 }
. Then by the property of martingale,
50
the information matrix is given by
I(β0) = P(l∗(β0,γ0; W)⊗2) = P
(∫ ∞
0
[−X+ µ(t)]⊗2 1 (U ≥t)dt
)
=
∫ ∞
0
P
(
[−X+ µ(t)]⊗2 1 (U ≥t)
)
dt,
which reduces to the same information matrix of the MPLE for the Cox model. The above information
matrix is similarly assumed to be positive deﬁnite in Kalbﬂeisch and Prentice (2011, page 175). The
non-singularity condition in (C8) can be satisﬁed if P
(
[−X+ µ(t)]⊗2 1 (U ≥t)
)
is positive deﬁnite
over a set of t with non-zero measure.
For the linear transformation model with a known transformation, i.e. γ0 is known, given the least
favorable direction w∗in Remark 8, the eﬃcient score for β is
l∗(β0,ζ0(·,β0); W) =
∫ ∞
0
m(t) [P(X|U ≥t) −X] dM(t),
with m(t) = g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1, and the information matrix is
I(β0) = P
(∫ ∞
0
m(t)2 [P(X|U ≥t) −X]⊗2 1 (U ≥t) d˜Λ0(t)
)
=
∫ ∞
0
m2(t) ·P
(
[P(X|U ≥t) −X]⊗2 1 (U ≥t)
)
·exp
(
g0(˜Λ0(t))
)
dt
=
∫ ∞
0
m2(t) ·Var(X|U ≥t) ·P(U ≥t) ·exp
(
g0(˜Λ0(t))
)
dt.
The information matrix takes a similar form as that in Ding and Nan (2011), where it is assumed to
be positive deﬁnite. Here we further investigate some suﬃcient conditions for its non-singularity. The
condition (C8) can be satisﬁed if m2(t) ·Var(X|U ≥t) ·P(U ≥t) is positive deﬁnite over a set of
t with non-zero measure. In particular, when the event time follows the AFT model with a Weibull
error, i.e., γ0 ≡0 and ˜Λ0(t) = ktv, the information matrix becomes
I(β0) =
∫ ∞
0
v2 ·Var(X|CeXTβ0 ≥t) ·P(CeXTβ0 ≥t) dF0(t),
where F0(t) = 1 −exp(−ktv) and C is the censoring time. This information matrix is nonsingular if
the conditional variance Var(X|CeXTβ0 ≥t) is positive deﬁnite for t over certain interval.
4 Proof of Propositions 1 and 2
The proof of Proposition 1 is based on the existing identiﬁability conditions for the linear transfor-
mation model (Horowitz, 1996) when both the transformation function and the error distribution are
unknown.
51
Proof of Proposition 1. Suppose two groups of parameters ( qi(·),βi,αi(·)) for i = 1,2 give the same
survival distribution. Let Hi(u) =
∫−ln u
0 q−1
i (v)dv, Gi(u) = H−1
i (u), and ϕi(t) = log
∫t
0 αi(s)ds for
i= 1,2. In the equivalent linear regression representation, we have that ϕi(T) = −xTβi + ϵi speciﬁes
the same distribution of event time T for i= 1,2, where the survival function of exp(ϵi) is given by Gi.
Note that, for the linear transformation model ϕ(T) = −x⊤β+ϵwith both ϕand the distribution of ϵ
unspeciﬁed, Horowitz (1996, page 105) stated that the model parameters are identiﬁable up to a scale
and a location normalization when at least one of the covariates x has a non-zero β coeﬃcient and
the conditional probability distribution of this covariate given the remaining covariates is absolutely
continuous with respect to Lebesgue measure. Since we assume that there is at least one of the
covariates in x is continuous and this covariate has a non-zero coeﬃcient, following the identiﬁability
conditions stated in Horowitz (1996), there exist constants c1 > 0 and c2 such that β1 = c1β2,
ϕ1(t) = c1ϕ2(t) + c2 for any t> 0, and ϵ1 has the same distribution as c1ϵ2 + c2, i.e.,
G1(t) = Pr(exp(ϵ1) >t) = Pr(exp(c1ϵ2 + c2) >t) = Pr(exp(ϵ2) >(te−c2 )1/c1 )) = G2((te−c2 )1/c1 ).
After plugging the deﬁnitions of ϕi along with some calculations, we have for any t> 0
∫ t
0
α1(s)ds= ec2
(∫ t
0
α2(s)ds
)c1
.
Let exp(−s) = G1(t) = G2((te−c2 )1/c1 ). Then by the deﬁnitions of Gi we have
t= H1(exp(−s)) =
∫ s
0
q−1
1 (v)dv and ( te−c2 )1/c1 = H2(exp(−s)) =
∫ s
0
q−1
2 (v)dv.
It follows that
∫s
0 q−1
1 (v)dv= ec2
(∫s
0 q−1
2 (v)dv
)c1
for any s> 0, which completes the proof.
As a direct result of Proposition 1, Proposition 2 provides the necessary and suﬃcient degeneration
condition for AFT and Cox models.
Proof of Proposition 2. The linear transformation model in (5) coincides with the Cox model if and
only if there exists some positive function ˜ α such that parameters (1 ,˜β,˜α(·)) and ( q(·),β,α (·)) give
the same survival distribution. By Proposition 1, there exists positive constants c1 and c2 such that
∫ t
0
q−1(s)ds= c2tc1 ,β = c1 ˜β, and
∫ t
0
α(s)ds= c2
(∫ t
0
˜α(s)ds
)c1
.
It implies that the functionqsatisﬁes q(t) = 1
c1c2
t1−c1 . Similarly, when the linear transformation model
coincides with the AFT model, there exists some positive function ˜ q such that parameters (˜q(·),˜β,1)
and (q(·),β,α (·)) give the same survival distribution. By Proposition 1, there exists positive constants
c1 and c2 such that
∫ t
0
q−1(s)ds= c2
(∫ t
0
˜q(s)ds
)c1
,β = c1 ˜β, and
∫ t
0
α(s)ds= c2tc1 .
52
It follows that the function α takes the form α(t) = c1c2tc1−1, which completes the proof.
5 Theoretical Properties for the General Class of ODE
Models and Their Proofs
In this section, we further establish the convergence rate and the asymptotic normality of the proposed
sieve estimator for the general class of ODE models in the presence of covariates Z with time-varying
coeﬃcients. We reformulate the model to ensure the positivity of α(·) and q(·) in (2) below,
{
Λ′(t) = exp
(
xTβ+ γ(t) + zTη(t) + g(Λ(t))
)
Λ(0) = 0
, (S40)
where γ(·) = log α(·) and g(·) = log q(·). Recall that, when there is at least one non-zero time-varying
eﬀect, i.e., η(t) ̸= 0, two groups of parameters ( β,γ,g, η) and ( ˜β,˜γ,˜g, ˜η) give the same survival
distribution if only if β = ˜β, γ = ˜γ+ c, g = ˜g−c, and η= ˜η for some constant c. To guarantee the
identiﬁability, we constrain γ(t∗) = 0 with some ﬁxed time point t∗.
Before stating the regularity conditions and main theorems, we ﬁrstly update the notation to
make them consistent with the model in (S40). Let Z ∈Rd2+1 substitute (1,ZT)T and γ(·) substitute
(γ(·),η1(·),...,η d2 (·))T for notational simplicity, then the general class of ODE models is equivalent
to {
Λ′(t) = exp
(
xTβ+ zTγ(t) + g(Λ(t))
)
Λ(0) = 0
, (S41)
with the ﬁrst component of γ ﬁxed at the time point t∗, i.e., γ1(t∗) = c. We denote the solution of
(S41) by Λ(t,x,z,β, γ,g) and the true parameters associated with the data generating distribution by
(β0,γ0,g0) and simplify Λ( t,x,z,β 0,γ0,g0) as Λ0(t,x,z ).
To accommodate covariatesZwith time-varying coeﬃcients, we update the conditions (C1)-(C8) to
(C1′)-(C8′) with additional regularity conditions on covariates Z and provide the theorem statements
and the sketch of proof in the following subsections.
5.1 Regularity conditions and main theorems
We assume additional regularity conditions on Z and list the updated conditions below.
(C1′) The true parameter β0 is an interior point of a compact set B⊂ Rd1 .
(C2′) The joint density of X and Z is bounded below by a constant c >0 over the compact domain
X×Z⊂ Rd1+d2+1. P(XXT) and P(ZZT) are nonsingular.
(C3′) There exists a truncation time τ < ∞ such that, for some positive constant δ0, Pr(Y >
τ|X,Z) ≥δ0 almost surely with respect to the joint probability measure of X and Z. Then
53
there is a constant µ= supx∈X,z∈ZΛ0(τ,x,z ) ≤−log δ0 such that Λ0(τ,X,Z ) = −log Pr(T >
τ|X,Z) ≤µ almost surely with respect to the joint probability measure of X and Z.
(C4′) Let Sp([a,b]) denote the collection of bounded functionsf on [a,b] deﬁned in (C4). The true func-
tion γ0(·) belongs to Γp1
t∗ ×Γp1 ×···× Γp1
  
d2
, where Γp1 := Sp1 ([0,τ]) and Γp1
t∗ := {γ ∈Sp1 ([0,τ]) :
γ(t∗) = 0}with p1 ≥2, and the true function g0(·) belongs to Gp2 := Sp2 ([0,µ + δ1]) with some
positive constant δ1 and p2 ≥3.
(C5′) Denote Rz(t) =
∫t
0 exp
(
zTγ0(s)
)
ds, V = XTβ0, and U = eVRZ(Y). There exists η1 ∈(0,1)
such that for all u∈Rd1 with ∥u∥= 1,
uTVar(X |U,V,Z )u≥η1uTP(XXT |U,V,Z )u almost surely.
(C6′) Let ψ(t,x,z,β, γ,g) = xTβ+ zTγ(t) + g(Λ(t,x,z,β, γ,g)) and denote its functional derivatives
with respect to the entirety ¯γ(·) = zTγ(·) and g(·) along the direction v(·) and w(·) at the true
parameter by ψ′
0¯γ(t,x,z )[v] and ψ′
0g(t,x,z )[w] respectively, whose rigorous deﬁnitions are given
by (S45)-(S46). For any v(·) = (v1,...,v d2+1)T with vj ∈Γp1 , 1 ≤j ≤d2 + 1, and w(·) ∈Gp2 ,
there exists η2 ∈(0,1) such that
(P{ψ′
0¯γ(Y,X,Z )[ZTv]ψ′
0g(Y,X,Z )[w] |∆ = 1})2
≤η2P{(ψ′
0¯γ(Y,X,Z )[ZTv])2 |∆ = 1}P{(ψ′
0g(Y,X,Z )[w])2 |∆ = 1}
almost surely.
(C7′) There exist v∗
j = (v∗
j1,··· ,v∗
jd1 )T and w∗ = (w∗
1,··· ,w∗
d1 )T, where v∗
jk ∈Γ2 and w∗
k ∈G2 for
1 ≤j ≤d2 + 1,1 ≤k≤d1, such that
P{∆A∗(U,X,Z )ψ′
0γℓ(Y,X,Z )[v]}= 0 and P{∆A∗(U,X,Z )ψ′
0g(Y,X,Z )[w]}= 0
hold for any v ∈Γp1 , 1 ≤ℓ≤d2 + 1, and w ∈Gp2 . Here ψ′
0γℓ(t,x,z )[v] denotes the functional
derivative with respect to theℓ-th component of γalong the direction v(·) at the true parameter,
U and V are deﬁned in condition (C5 ′), and
A∗(t,X,Z ) = −
(
g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)
t+ 1
)
X
+
d2+1∑
j=1
[
g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ t
0
Zjv∗
j(R−1
Z (se−V))ds+ Zjv∗
j(R−1
Z (te−V))
]
+ g′
0(˜Λ0(t)) exp
(
g0(˜Λ0(t))
)∫ ˜Λ0(t)
0
exp(−g0(s))w∗(s)ds+ w∗(˜Λ0(t)),
where ˜Λ0(t) is the solution of ˜Λ′
0(t) = exp
(
g0(˜Λ0)
)
with ˜Λ0(0) = 0.
54
(C8′) Let l∗(β0,γ0,ζ0; W) =
∫
A∗(t,X,Z )dM(t), where M(t) = ∆ 1 (U ≤t) −
∫t
0 1 (U ≥s)d˜Λ0(s) is
the event counting process martingale. The information matrix I(β0) = P(l∗(β0,γ0,ζ0; W)⊗2)
is nonsingular. Here for a vector a, a⊗2 = aaT.
In the presence of covariates Z with time-varying coeﬃcients, conditions (C2 ′)-(C3′) contain ad-
ditional common regularity assumptions for Z in survival analysis. Condition (C4 ′) controls the error
rates of the spline approximation for the true time-varying coeﬃcients. The expectation in condition
(C5′) is further conditioned on covariates Z. Condition (C6 ′) is similarly assumed to avoid strong
collinearity between ψ′
0¯γ(Y,X,Z )[v] and ψ′
0g(Y,X,Z )[w] while ¯γ denotes the linear combination zTγ.
Condition (C7′) additionally requires the existence of the least favorable directions for time-varying
coeﬃcients and the information matrix in (C8 ′) also depends on the additional least favorable direc-
tions. In particular, conditions (C1 ′)-(C8′) are equivalent to conditions (C1)-(C8) respectively when
Z only contains the intercept.
Given the above regularity conditions, for the general class of ODE models in (2), we can establish
the same convergence rate of the sieve estimator as that in Theorem 1 and the asymptotic normality
as in Theorem 2. Since the theory is investigated with the ﬁxed number of covariates d1 and d2 as the
sample size n grows, including additional covariates Z with time-varying coeﬃcients does not change
the nature of the proof. For presentation integrity, we provide rigorous deﬁnitions of the corresponding
parameter space, the sieve space, theorem statements, and a sketch of proof that summarizes the main
steps in the following subsection.
First, we deﬁne the parameter space and the associated distance when including covariates Z with
time-varying coeﬃcients. We similarly deﬁne the collection of functions
Hp2 = {ζ(·,β,γ) : ζ(t,x,z,β, γ) = g(Λ(t,x,z,β, γ,g)),t ∈[0,τ],x ∈X,z ∈Z,β ∈B,
γ∈Γp1
t∗ ×Γp1 ×···× Γp1
  
d2
,g ∈Gp2 such that sup
t∈[0,τ],x∈X,z∈Z
|Λ(t,x,z,β, γ,g)|≤ µ+ δ1},
with δ1 given in condition (C4′). For any ζ(·,β, γ) ∈Hp2 , we deﬁne its norm as
∥ζ(·,β, γ)∥2 =
[∫
X×Z
∫ τ
0
[ζ(t,x,z,β, γ)]2dΛ0(t,x,z )dFX,Z(x,z)
]1/2
,
where FX,Z(x,z) is the cumulative distribution function of ( X,Z). Denote the parameter θ =
(β,γ(·),ζ(·,β, γ)) and the true parameter θ0 = (β0,γ0(·),ζ0(·,β0,γ0)) with
ζ0(t,x,z,β 0,γ0) = g0(Λ(t,x,z,β 0,γ0,g0)).
Denote the parameter space by Θ = B×Γp1
t∗×Γp1 ×···× Γp1
  
d2
×Hp2 . For any θ1 and θ2 in Θ, we deﬁne
the distance
d(θ1,θ2) =
(
∥β1 −β2∥2 + ∥γ1 −γ2∥2
2 + ∥ζ1(·,β1,γ1) −ζ2(·,β2,γ2)∥2
2
)1/2
,
55
where ∥·∥ is the Euclidean norm and ∥γ∥2 = (∑d2+1
j=1
∫τ
0 (γj(t))2dt)1/2.
Next, we construct the sieve space by using the space of polynomial splines in a similar way. Let
Γp1
n = Sn(TK1n,K1
n,p1), Γp1
t∗,n = {γ ∈Sn(TK1n,K1
n,p1) : γ(t∗) = 0}, Gp2
n = Sn(TK2n,K2
n,p2), and
Hp2
n = {ζ(·,β, γ) : ζ(t,x,z,β, γ) = g(Λ(t,x,z,β, γ,g)), t∈[0,τ],x ∈X,z ∈Z,β ∈B,
γ∈Γp1
t∗,n ×Γp1
n ×···× Γp1
n  
d2
,g ∈Gp2
n }.
Let Θn = B×Γp1
t∗,n×Γp1
n ×···× Γp1
n  
d2
×Hp2
n be the sieve space. The sieve estimatorˆθn = ( ˆβn,ˆγn(·),ˆζn(·,ˆβn,ˆγn))
maximizes the log-likelihood (6) over the sieve space Θ n. The convergence rate of the sieve MLE ˆθn
and the asymptotic normality of the sieve MLE ˆβn of the regression parameter are then established
in Theorem 4 and Theorem 5 respectively.
Theorem 4.(Convergence rate of ˆθn.) Let ν1 and ν2 satisfy the restrictions max{ 1
2(2+p1) , 1
2p1
−ν2
p1
}<
ν1 < 1
2p1
, max{ 1
2(1+p2) , 1
2(p2−1) − 2ν1
p2−1 } < ν2 < 1
2p2
, and 2 min{2ν1,ν2} > max{ν1,ν2}. Suppose
conditions (C1′)-(C6′) hold, then we have
d(ˆθn,θ0) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }).
Theorem 5. (Asymptotic normality of ˆβn) Suppose the conditions in Theorem 4 and (C7 ′)-(C8′)
hold, then we have
√n( ˆβn −β0) = √nI−1(β0)Pnl∗(β0,γ0,ζ0; W) + op(1) →d N(0,I−1(β0))
with I(β0) given in condition (C8 ′) and →d denoting convergence in distribution.
5.2 Sketch of proof
Given the updated conditions (C1 ′)-(C8′), the proof of Theorems 4 and 5 is based on the similar
techniques and arguments as that of Theorems 1 and 2. We provide the sketch of proof and highlight
their main diﬀerences below.
Lemmas. The corresponding Lemmas 1-8 in the presence of covariates Z still hold under new
conditions (C1′)-(C7′), which are used to prove Theorems 4 and 5. Speciﬁcally,
• The existence and uniqueness of the solution Λ( t,x,z,β, γ,g) of the initial value problem in
(S40) along with its derivatives in Lemma 1, and the boundedness and continuity of derivatives of
l(β,γ,ζ; W) in Lemma 4 both hold due to the boundedness of Z and the smoothness of ηunder
conditions (C1 ′)-(C4′). In particular, the derivatives are characterized by the corresponding
56
updated initial value problems with covariates Z. For example, initial value problems (S9)-
(S11) become (S42)-(S44) respectively as follows
dΛ′
β(t)
dt = exp
(
xTβ+ zTγ(t) + g(Λ(t))
)
{x+ g′(Λ(t))Λ′
β(t)}, Λ′
β(0) = 0, (S42)
dΛ′
γj(t)[v]
dt = exp
(
xTβ+ zTγ(t) + g(Λ(t))
)
{zjv(t) + g′(Λ(t))Λ′
γj(t)[v]}, Λ′
γj(0)[v] = 0, (S43)
dΛ′
g(t)[w]
dt = exp
(
xTβ+ zTγ(t) + g(Λ(t))
)
{w(Λ(t)) + g′(Λ(t))Λ′
g(t)[w]}, Λ′
g(0)[w] = 0. (S44)
• In Lemma 2, we show that the operators ψ′
0¯γ[·] and ψ′
0g[·] are bounded from below by the
continuous dependence of the IVP solution on parameters in Walter (1998, page 145), where
ψ′
0¯γ[·] denotes the functional derivatives with respect to the entirety ¯ γ(·) = zTγ(·). By solving
initial value problem in (S43), the ﬁrst derivatives of ψ(t,x,β,γ,g ) with respect to ¯γ and g at
the true parameter ( β0,γ0,g0) are updated as
ψ′
0¯γ(t,x,z )[v] = g′
0(Λ0(t,x,z ))Λ′
0¯γ(t,x)[v] + v(t)
= g′
0(Λ0(t,x,z )) exp(g0(Λ0(t,x,z )))exTβ0
∫ t
0
exp
(
zTγ0(s)
)
v(s)ds+ v(t), (S45)
ψ′
0g(t,x,z )[w] = g′
0(Λ0(t,x,z ))Λ′
0g(t,x,z )[w] + w(Λ0(t,x,z ))
= g′
0(Λ0(t,x,z )) exp(g0(Λ0(t,x,z )))
∫ Λ0(t,x,z)
0
exp(−g0(s))w(s)ds+ w(Λ0(t,x,z )).
(S46)
• The upper bounds of the ϵ-bracketing numbers associated with Fn, Fγℓ
n,j(η), Fζ
n,j(η), F∗β
n,j(η),
F∗γℓ
n,j (η), F∗ζ
n,j(η) for 1 ≤ℓ≤d2+1,1 ≤j ≤d1 in Lemmas 6-8 are updated as (1
ϵ)c1qn1 (d2+1)+c2qn2 +d1
and ( η
ϵ)c1qn1 (d2+1)+c2qn2 +d1 , where d1 and d2 are dimensions of covariates X and Z respectively.
Since we consider the number of covariates di ﬁxed as the sample size increases, the updated
upper bounds in the presence of Z would not change the convergence rate of the sieve estimator
and the nature of the proof.
Proof of Theorem 4. To establish the overall convergence rate of the sieve MLEˆθn in Theorem 4,
we verify three conditions C1-C3 required in the main theorem in Shen and Wong (1994). Speciﬁcally,
• The condition C1 in Shen and Wong (1994) speciﬁes the increasing rate of the expected log-
likelihood ratio as the parameter θ moves away from the true value θ0. We will prove that
inf
d(θ,θ0)≥ϵ,θ∈Θn
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ (·,β,γ ); W) ≳ ϵ2.
57
In the presence of covariate Z, we update
Pl(β,γ,ζ(·,β, γ); W) = P{∆[XTβ+ ZTγ(Y) + g(Λ(Y,X,Z,β, γ,g))
−exp
(
XTβ+ ZTγ(Y) + g(Λ(Y,X,Z,β, γ,g)) −XTβ0 −ZTγ0(Y) −g0(Λ0(Y,X,Z ))
)
]}.
Using the Taylor expansion along with the same arguments, we have
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ(·,β, γ); W)
≳ P{∆[(g′
0(Λ0(Y,X,Z ))Λ′
0β(Y,X,Z ) + X)T(β−β0)
+
d2+1∑
j=1
g′
0(Λ0(Y,X,Z ))Λ′
0γj(Y,X,Z )[(γ−γ0)Tej] + ZT(γ(Y) −γ0(Y))
+ g′
0(Λ0(Y,X,Z ))Λ′
0g(Y,X,Z )[g−g0] + g(Λ0(Y,X,Z )) −g0(Λ0(Y,X,Z ))]2}
+ o(d2(θ,θ0))
= P{∆[ϵ1(U)XT(β−β0) + ϵ2(U,V,Z )[(γ(Y) −γ0(Y))TZ] + ϵ3(U)[g−g0]]2}+ o(d2(θ,θ0)),
where ϵ1, ϵ2, and ϵ3 are deterministic functions of Z, U, V given in condition (C5 ′). Under the
updated conditions (C5′)-(C6′), we can similarly derive that
Pl(β0,γ0,ζ0(·,β0,γ0); W) −Pl(β,γ,ζ(·,β, γ); W)
≳ P{∆(ϵ1(U)XT(β−β0))2}+ P{∆(ϵ2(U,V,Z )[(γ(Y) −γ0(Y))TZ])2}
+ P{∆(ϵ3(U)[g−g0])2}+ o(d2(θ,θ0)).
Given the boundedness of Z, the ﬁrst and third terms are similarly bounded below by ∥β−β0∥2
and ∥g−g0∥2
2 respectively. The second term is bounded below by
P{∆(ϵ2(U,V,Z )[(γ(Y) −γ0(Y))TZ])2}≳ ∥(γ(Y) −γ0(Y))TZ∥2
2
=
∫ τ
0
(γ−γ0)(t)TP{ZZT}(γ−γ0)(t)dt+
≥
∫ τ
0
λ(Z)
1 (γ−γ0)(t)T(γ−γ0)(t)dt
= λ(Z)
1 ∥γ−γ0∥2
2,
where λ(Z)
1 is the smallest eigenvalue of P{ZZT}, which is positive due to the nonsingularity in
the updated condition (C2 ′). Therefore, we have
Pl(β0,γ0,ζ0(·,β0,γ0); W)−Pl(β,γ,ζ(·,β, γ); W) ≳ ∥β−β0∥2 +∥γ−γ0∥2
2 +∥g−g0∥2
2 ≳ d2(θ,θ0).
• The condition C2 in Shen and Wong (1994) controls the decreasing rate of the variance of the
log-likelihood ratio as the parameter θapproaches the true value θ0. We use the same arguments
58
to show that
sup
d(θ,θ0)≤ϵ,θ∈Θn
Var{l(β,γ,ζ(·,β, γ); W) −l(β0,γ0,ζ0(·,β0,γ0); W)}≲ ϵ2.
Note that the second term in (S29) is replaced and upper bounded by
P{∆
(
ZT(γ(Y) −γ0(Y))
)2
}
= P
∫ τ
0
1(Y ≥t) exp
(
XTβ0 + ZTγ0(t) + g0(Λ0(t,X,Z ))
)(
ZT(γ(t) −γ0(t))
)2
dt
≤
∫ τ
0
sup
x∈X,z∈Z,t∈[0,τ]
{exp
(
xTβ0 + ZTγ0(t) + g0(Λ0(t,x,z ))
)
}P
(
ZT(γ(t) −γ0(t))
)2
dt
≤
∫ τ
0
sup
x∈X,z∈Z,t∈[0,τ]
{exp
(
xTβ0 + ZTγ0(t) + g0(Λ0(t,x,z ))
)
}λ(Z)
d2+1 ∥γ(t) −γ0(t)∥2 dt
≲ ∥γ−γ0∥2
2,
where λ(Z)
d2+1 is the largest eigenvalue of P(ZZT).
• The condition C3 in Shen and Wong (1994) bounds the size of the space of log-likelihood ratio
induced by θ, i.e., Fn = {l(θ; W) −l(θ0n; W) : θ ∈Θn}. By Lemma 6, we have the L∞-metric
entropy of the space Fn bounded by
H(ϵ,Fn,∥·∥∞) = log(N(ϵ,Fn,∥·∥∞)) ≲ c1qn1 (d2 + 1) +c2qn2 + d1 ≲ nmax{ν1,ν2}log(1/ϵ),
as the number of covariates di is considered as ﬁxed.
After verifying the conditions C1-C3, by Theorem 1 in Shen and Wong (1994), we have for the sieve
MLE ˆθn
d(ˆθn,θ0) = Op(max{n−1−max{ν1,ν2}
2 ,d(θ0n,θ0),K1/2(θ0n,θ0)}),
where K(θ0n,θ0) = P{l(θ0; W)−l(θ0n; W)}. We can similarly show thatK(θ0n,θ0) ≲ O(d2(θ0n,θ0)) by
the Taylor expansion, so the convergence rate ofˆθn depends on the sieve approximation errord(θ0n,θ0).
Here θ0n = (β0,γ0n(·),ζ0n(·,β0,γ0n)) ∈Θn with ζ0n(t,x,z,β 0,γ0n) = g0n(Λ(t,x,z,β 0,γ0n,g0n)). Note
that γ0n,j ∈Γp1
n and g0n ∈Gp2 are deﬁned in Lemma 5 such that ∥γ0n,j −γ0,j∥∞ = O(n−p1ν1 ) and
∥g0n−g0∥∞= O(n−p2ν2 ), which is based on the existing spline approximation error in Corollary 6.21
in Schumaker (2007). Since d2(θ0n,θ0) ≲ ∥β0 −β0∥2 + ∥γ0n −γ0∥2
2 + ∥g0n −g0∥2
2 ≲ ∥γ0 −γ0n∥2
∞+
∥g0 −g0n∥2
∞= O(n−2 min{p1ν1,p2ν2}), it follows that
d(ˆθn,θ0) = Op(n−min{p1ν1,p2ν2,1−max{ν1,ν2}
2 }).
Proof of Theorem 5. To establish the asymptotic normality in Theorem 5, we similarly verify the
assumptions (A1)-(A6) for the proposed general M-theorem in Theorem 3 under the updated conditions
59
(C1′)-(C8′). For example, to verify assumption (A3), ﬁrst, we need to ﬁnd v∗
j = ( v∗
j1,··· ,v∗
jd1 )′,
1 ≤j ≤d2 + 1, and h∗= (h∗
1,··· ,h∗
d1 )′with h∗(·) = w∗(Λ0(·)) +g′
0(Λ0(·))Λ′
0g(·)[w∗] such that for any
v∈V and h∈H with h(·) = w(Λ0(·)) + g′
0(Λ0(·))Λ′
0g(·)[w],
S′′
βγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v] =
d2+1∑
j=1
S′′
γjγℓ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,v]
+ S′′
ζγl(β0,γ0(·),ζ0(·,β0,γ0))[h∗,v], (S47)
S′′
βζ(β0,γ0(·),ζ0(·,β0,γ0))[h] =
d2+1∑
j=1
S′′
γjζ(β0,γ0(·),ζ0(·,β0,γ0))[v∗
j,h]
+ S′′
ζζ(β0,γ0(·),ζ0(·,β0,γ0))[h∗,h]. (S48)
By Lemma 4 and the property P{
∫Y
0 f(t,X,Z )dΛ0(t,X,Z )}= P{∆f(Y,X,Z )}, for any vj ∈Vd1 ,v ∈
V and h ∈Hd1 with h(·) = w(Λ0(·)) + g′
0(Λ0(·))Λ′
0g(·)[w], we have for 1 ≤ℓ≤d2 + 1
S′′
βγℓ(β0,γ0(·),ζ0)[v] −
d2+1∑
j=1
S′′
γjγℓ(β0,γ0(·),ζ0)[vj,v] −S′′
ζγl(β0,γ0(·),ζ0)[h,v]
=P{l′′
βγℓ(β0,γ0,ζ0; W)[v] −
d2+1∑
j=1
l′′
γjγℓ(β0,γ0,ζ0; W)[vj,v] −l′′
ζγℓ(β0,γ0,ζ0; W)[h,v]}
=P


∆

g′
0(Λ0(Y,X,Z ))Λ′
0β(Y,X,Z ) + X−
d2+1∑
j=1
(
g′
0(Λ0(Y,X,Z ))Λ′
0γj(Y,X,Z )[vj] + vj(Y)Zj
)
−g′
0(Λ0(Y,X,Z ))Λ′
0g(Y,X,Z )T[w] −w(Λ0(Y,X,Z ))
]
(
g′
0(Λ0(Y,X,Z ))Λ′
0γℓ(Y,X,Z )[v] + v(Y)Zℓ
)
}
.
Under the updated condition (C7′), there exist v∗
j = (v∗
j1,··· ,v∗
jd1 )T and w∗= (w∗
1,··· ,w∗
d1 )T, where
v∗
jk ∈Γ2 and w∗
k ∈G2 for 1 ≤j ≤d2 + 1,1 ≤k≤d1, such that P{∆A∗(U,X,Z )ψ′
0γℓ(Y,X,Z )[v]}= 0
hold for any v ∈Γp1 , 1 ≤ℓ≤d2 + 1. Therefore, we have that the equation (S47) holds with v∗
j and
w∗given in condition (C7′). Similarly, we can show that the equation (S48) holds as well.
6 Additional Simulation Studies
In this section, we provide full results of simulation studies with various sample sizes and investigate 1)
how the numerical performance of the proposed method depends on the knot selection by comparing
multiple natural knot selections; 2) a heuristic parametric approach that applies the uniﬁed ODE
framework along with the proposed estimation and inference procedure for model diagnostics.
60
6.1 Time-varying Cox model
Table S1 summarizes the estimates of regression coeﬃcients β3 and β4 in the time-varying Cox model
that is considered in subsection 5.1. The proposed sieve estimators for β3 and β4 perform similarly
to those for β1 and β2 as shown in Table 1. The bias of the estimators for β3 and β4 decreases
and becomes negligible as the sample size increases. The estimated standard error by inverting the
estimated information matrix for all parameters including the coeﬃcients of spline basis are close to
the sample standard error and the corresponding 95% conﬁdence intervals obtain reasonable coverage
proportion.
Table S1: Simulation results under time-varying Cox model.
N Method β3 = −1 β4 = 1
Bias SE ESE CP Bias SE ESE CP
1000 ODE -.009 .076 .078 .942 .009 .068 .070 .948
Cox-MPLE -.007 .076 .075 .938 .007 .068 .068 .943
2000 ODE -.004 .052 .054 .965 .005 .047 .048 .955
Cox-MPLE -.003 .052 .053 .966 .004 .047 .048 .952
4000 ODE -.003 .037 .038 .951 .004 .034 .034 .951
Cox-MPLE -.003 .037 .037 .950 .003 .034 .034 .950
8000 ODE .000 .026 .026 .959 -.001 .024 .024 .947
Cox-MPLE .000 .026 .026 .952 -.001 .024 .024 .949
Bias is the diﬀerence between mean of estimates and the true value; SE is the sample
standard error of the estimates; ESE is the mean of the standard error estimators by
inverting the estimated information matrix of all parameters including the coeﬃcients
of spline basis, and CP is the corresponding coverage proportion of 95% conﬁdence
intervals.
6.2 Comparison with the method in Royston and Parmar (2002)
under the Cox model
In setting 1), we compare the proposed sieve MLE under the Cox model with the parametric method
in Royston and Parmar (2002), where the log-transformed baseline cumulative hazard is modeled as a
natural cubic spline function of the log-transformed time. We implement it using the “ﬂexsurvspline”
function in the R package ﬂexsurv with the same number of interior knots, i.e., ⌊N′1
5 ⌋. The sample
size N varies from 1000 to 8000.
Table S2 summarizes the estimates of regression coeﬃcients based on 1000 replicates. We can see
that both the proposed estimation method (ODE-Cox) and the method in Royston and Parmar (2002)
(ﬂexsurv) perform similarly to maximum partial likelihood estimation (MPLE) in terms of estimation
accuracy. As shown in Figure S1, the proposed method ODE-Cox achieves comparable integrated mean
square errors (IMSE) of the estimated cumulative hazard function to those of “ﬂexsurv”. In addition,
61
the relative computing time (the computing time with respect to that with the smallest sample size
1000) of proposed method ODE-Cox increases slowly than that of “ﬂexsurv” as the sample size grows.
We note that the increasing rate of the relative computing time of the ODE-Cox is even slower than the
linear rate, which may be beneﬁted from eﬃcient implementation of existing numerical ODE solvers.
Table S2: Simulation results under the Cox model.
N Method β1 = 1 β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1000
MPLE .006 .153 .152 .948 .010 .157 .152 .944 .004 .152 .152 .950
ODE-Cox .009 .153 .157 .952 .013 .157 .157 .952 .007 .152 .158 .961
Flexsurv .007 .153 .152 .948 .011 .156 .151 .943 .005 .151 .152 .952
2000
MPLE .005 .106 .107 .954 -.002 .107 .107 .949 .006 .105 .107 .958
ODE-Cox .007 .106 .109 .956 -.001 .107 .109 .955 .007 .105 .109 .961
Flexsurv .006 .105 .107 .956 -.001 .107 .107 .950 .007 .105 .107 .955
4000
MPLE .002 .076 .075 .934 -.003 .075 .075 .941 -.001 .074 .075 .954
ODE-Cox .003 .076 .076 .936 -.002 .075 .076 .942 .000 .074 .076 .955
Flexsurv .002 .076 .075 .934 -.002 .075 .075 .942 -.001 .074 .075 .953
8000
MPLE -.002 .053 .053 .953 .000 .052 .053 .954 -.001 .053 .053 .944
ODE-Cox -.002 .053 .054 .953 -.000 .052 .054 .957 -.002 .054 .054 .947
Flexsurv -.001 .053 .053 .954 .000 .052 .053 .952 -.001 .053 .053 .944
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error
of the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage
proportion of 95% conﬁdence intervals.
Figure S1: Integrated mean square error (IMSE) of estimated baseline cumulative hazard func-
tions and the log-log plot of mean relative computing time with respect to the sample size under
the Cox model are provided from left to right.
62
6.3 Comparison with the NPMLE (Zeng and Lin, 2007b) under the
linear transformation model
We have compared the proposed ODE approach and the NPMLE for the logarithmic transformation
model in (Zeng and Lin, 2007b). Speciﬁcally, in the simulation setting (2), we generate event times
from the ODE
Λ′
x(t) = q(Λx(t)) exp(β1x1 + β2x2 + β3x3)α(t),
where functions q(t) = exp( −t) and α(t) = 2. It is equivalent to generate event times with the
cumulative hazard function
Λx(t) = G{exp(β1x1 + β2x2 + β3x3)Λ0(t)},
where G(u) = log(1 + u) and Λ 0(t) =
∫t
0 α(s)ds = 2 t. For the NPMLE in Zeng and Lin (2007b),
note that the function G(·) is known and the baseline cumulative hazard Λ 0(·) is unknown. An EM
algorithm was implemented in Matlab to compute the NPMLE. To make fair comparison, we set the
function q(·) known, i.e., q(t) = exp( −t), and the function α(·) unknown for the ODE-LT. We ﬁt
log α(·) by cubic B-splines and set the number of knots Kn as the largest integer below N′1
5 , where
N′is the number of distinct observation time points. The sample size N varies from 1,000 to 8,000.
Table S3 summarizes the estimates of regression coeﬃcients β based on 1000 replicates. The pro-
posed estimation method (ODE-LT) achieves similar estimation accuracy of bothβand the cumulative
hazard (shown in the left panel of Figure S2) as the NPMLE. However, the relative computing time
of the proposed method ODE-LT increase linearly as the sample size grows while that of the NPMLE
increases in a quadratic rate as shown in the right panel of Figure S2.
Table S3: Simulation results under the linear transformation model.
N Method β1 = 1 β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1000 NPMLE .003 .227 .230 .954 .003 .236 .230 .949 .003 .229 .230 .954
ODE-LT .005 .227 .231 .956 .005 .237 .231 .949 .004 .229 .231 .955
2000 NPMLE -.002 .159 .162 .946 .003 .169 .162 .933 .006 .157 .162 .963
ODE-LT -.001 .159 .163 .947 .003 .169 .163 .933 .007 .157 .163 .961
4000 NPMLE .004 .117 .115 .949 -.001 .114 .115 .951 .003 .113 .115 .960
ODE-LT .005 .117 .115 .950 -.000 .114 .115 .951 .003 .113 .115 .961
8000 NPMLE -.005 .079 .081 .956 .000 .078 .081 .963 -.001 .079 .081 .950
ODE-LT -.004 .079 .081 .957 .001 .078 .081 .963 -.000 .079 .081 .951
Bias is the diﬀerence between the mean of estimates and the true value, SE is the sample standard error of the
estimates, and Mean is the mean of IMSE. ESE is the mean of the standard error estimators, and CP is the
corresponding coverage proportion of 95% conﬁdence intervals.
63
Figure S2: Integrated mean square error (IMSE) of estimated baseline cumulative hazard func-
tions and the log-log plot of mean relative computing time with respect to the sample size under
the linear transformation model are provided from left to right.
6.4 Comparison with the rank-based method under the AFT model
In setting 3), we compare the proposed sieve MLE for the ODE-AFT model, where the function α is
set to 1, with the rank-based estimation approach implemented using the R package aftgee. For the
ODE-AFT model, we ﬁt log q(t) by cubic B-splines with ⌊N
1
7 ⌋interior knots. Note that the argument
of the function q(·) is the cumulative hazard. Unlike ﬁtting the function α(·) whose argument is the
event time in the ODE-Cox model, we do not observe the corresponding cumulative hazard directly.
Therefore, we use the estimated cumulative hazards under the Cox model as a remedy. Let ˆΛCox
i
denote the estimated cumulative hazard for individual iunder the Cox model. The interior knots are
located at the quantiles of {ˆΛCox
i }n
i=1.
Table S4 summarizes the estimates of regression coeﬃcients βwith varying sample sizes. Although
the bias of the ODE approach is relatively greater than that of the rank-based method when the sample
size is small, the bias of the estimates becomes negligible as the sample size increases. As shown in
Figure S3, the relative computing time of the proposed ODE approach increases in a slower rate than
that of the rank-based method for the semi-parametric ODE-AFT model. Remarkably, the proposed
ODE approach takes just 6 seconds for estimating the ODE-AFT model but the rank-based method
takes 349 seconds when the sample size is 8 ,000.
6.5 Comparison with the smoothed partial rank method under the
general linear transformation model
In settings 1)-4), we compare the sieve MLE for the general linear transformation model (ODE-Flex),
where both q(·) and α(·) are unspeciﬁed, with the smoothed partial rank (SPR) estimation method
in Song et al. (2006), which is a rank-based estimation method for censored data. As the original
code of SPR is not available, we implement the SPR estimation and inference methods by our own,
and we verify that our implementations are able to reproduce the simulation results in Song et al.
64
Table S4: Simulation results under the AFT model.
N Method β1 = 1 β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1000 Rank-based -.000 .204 .206 .952 -.009 .213 .205 .925 -.013 .200 .206 .942
ODE-AFT -.014 .197 .191 .944 -.024 .209 .192 .931 -.032 .199 .192 .932
2000 Rank-based -.002 .147 .145 .938 .005 .147 .145 .951 .004 .146 .146 .945
ODE-AFT -.010 .144 .137 .932 -.006 .144 .137 .937 -.005 .142 .137 .943
4000 Rank-based .004 .105 .102 .944 -.001 .102 .102 .950 .002 .100 .103 .954
ODE-AFT .000 .102 .097 .944 -.005 .100 .097 .944 -.002 .097 .097 .950
8000 Rank-based -.003 .071 .073 .956 .001 .071 .073 .962 .000 .072 .073 .949
ODE-AFT -.006 .070 .069 .950 -.003 .068 .069 .967 -.004 .071 .069 .945
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error of
the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage proportion
of 95% conﬁdence intervals.
Figure S3: Integrated mean square error (IMSE) of estimated baseline cumulative hazard func-
tions and the log-log plot of mean relative computing time with respect to the sample size under
the AFT model are provided from left to right.
(2006). Note that SPR introduces an additional parameter cin the objective function to improve the
estimation accuracy. We evaluate SPR with various values of the parameter c and the sample size
N under our data settings 1)-4). We observe that SPR may return extreme estimates, so we count
estimates with more than 5 deviation from the truth as failed replications.
Tables S5-S6 summarize the estimates of β2 under settings 1)-4) over 1 ,000 replications. (We
observe similar performance for β3 and so we omit its results here.) In terms of estimation accuracy,
both the SPR estimator and ODE-Flex estimator show negligible biases when the sample size is large.
However, two inference methods in Song et al. (2006) are sensitive to the choices of the parameter
c : the sandwich estimator seriously underestimates the standard deviation for various values of the
parameter c and the corresponding coverage proportion is far below the nominal level; the weighted
65
Figure S4: The log-log plot of mean relative computing time with respect to the sample size
under the nonparametric linear transformation model.
bootstrap estimator overestimates the standard deviation for small values of c and underestimates
it for relatively large values of c. In contrast, the proposed ODE-Flex method performs well across
various sample sizes: the standard error estimators approximate the empirical standard deviations
well and the coverage proportions are close to the nominal level. In terms of numerical stability, the
proposed ODE-Flex method can stably return good estimates over 1 ,000 replications, especially for
large sample sizes: only less than 1% replications meet with numerical errors when N = 4,000 and
100% replications successfully return accurate estimates when N = 8,000. We note that this result is
reported under a universal precision for ODE solvers and we ﬁnd that these failed replications can be
easily ﬁxed by adjusting the precision of the ODE solver. However, the SPR method fails to return
a reasonable point estimator for more than 12% realized resampling on average when computing the
standard error estimator by the weighted bootstrap. We also observe that it is diﬃcult to obtain the
SPR point estimator for larger sample size such as N = 8,000 or larger parameter c such as 10 −1
and 1 (success rate less than 10%). In terms of computation eﬃciency, as shown in Figure S4, the
computing time of ODE-Flex increases in a much smaller rate than that of SPR as the sample size
grows, which implies that the proposed estimation method is computationally more eﬃcient for large
sample size.
6.6 Dependence on knots selection
To investigate how the numerical performance of the proposed method depends on the knot selection,
we have done several simulation studies to compare two natural placements of knots for the ODE-Cox
model, the ODE-AFT model, and the general linear transformation model. Speciﬁcally,
• For the ODE-Cox model, we compare the following two placements of knots when using the
B-spline to ﬁt the function log α(·): (K1) the interior knots are located at the Kn = ⌊N
1
5 ⌋
quantiles of the distinct observation time points; (K2) the interior knots equally separate the
time interval from 0 to the maximum of observed times.
• For the ODE-AFT model, we compare the following two placements of knots when using the B-
66
Table S5: Simulation results of β2 under the general linear transformation model with both q(·)
and α(·) unspeciﬁed in settings 1) and 2).
Method N c Sandwich Bootstrap Bootstrap
Bias SE ESE CP ESE CP Succ. % Succ. %
1)
SPR
1000
10−4 .030 .331 .000 .000 .697 .974 98.3 87.3
10−3 .034 .250 .000 .003 .478 .960 97.4 84.0
10−2 .048 .295 .002 .020 .103 .432 80.1 75.5
2000
10−4 -.003 .313 .000 .000 .668 .989 98.4 85.0
10−3 .013 .210 .000 .003 .314 .906 94.5 80.4
10−2 .007 .159 .002 .022 .033 .279 71.8 70.9
4000
10−4 .007 .153 .000 .001 .552 .994 97.9 83.1
10−3 .008 .120 .000 .000 .136 .762 95.2 77.7
10−2 .005 .105 .002 .022 .016 .222 67.7 67.8
N Bias SE ESE CP Succ. %
ODE-Flex
1000 .067 .248 .243 .958 93.6
2000 .024 .162 .158 .950 98.4
4000 .008 .106 .107 .947 99.5
8000 .012 .076 .075 .946 100.0
Method N c Sandwich Bootstrap Bootstrap
Bias SE ESE CP ESE CP Succ. % Succ. %
2)
SPR
1000
10−4 .082 .522 .000 .000 .739 .949 97.8 87.2
10−3 .091 .449 .000 .002 .538 .910 96.5 84.2
10−2 .104 .464 .003 .015 .166 .457 81.8 74.1
2000
10−4 .020 .347 .000 .000 .702 .988 98.3 85.5
10−3 .015 .320 .000 .000 .393 .895 95.5 80.3
10−2 .044 .337 .002 .005 .052 .262 75.7 69.3
4000
10−4 .014 .244 .000 .000 .585 .995 98.5 83.9
10−3 .019 .191 .000 .001 .183 .709 93.6 77.4
10−2 .022 .171 .002 .010 .019 .158 67.1 65.2
N Bias SE ESE CP Succ. %
ODE-Flex
1000 .024 .357 .312 .918 98.5
2000 .009 .246 .218 .931 99.5
4000 -.019 .161 .151 .927 100.0
8000 -.020 .113 .107 .939 100.0
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error
of the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage
proportion of 95% conﬁdence intervals.
67
Table S6: Simulation results of β2 under the general linear transformation model with both q(·)
and α(·) unspeciﬁed in settings 3) and 4).
Method N c Sandwich Bootstrap Bootstrap
Bias SE ESE CP ESE CP Succ. % Succ. %
3)
SPR
1000
10−4 .053 .369 .000 .000 .779 .980 97.3 86.1
10−3 .056 .386 .000 .000 .529 .945 95.7 82.9
10−2 .079 .372 .004 .010 .128 .454 79.9 73.4
2000
10−4 .004 .304 .000 .000 .721 .992 97.8 84.4
10−3 .010 .308 .000 .000 .357 .888 96.0 79.2
10−2 .010 .222 .002 .016 .040 .251 74.8 68.3
4000
10−4 .005 .194 .000 .000 .602 .996 97.5 82.4
10−3 .007 .146 .000 .001 .154 .732 92.4 76.0
10−2 .011 .141 .002 .025 .020 .194 68.1 63.4
N Bias SE ESE CP Succ. %
ODE-Flex
1000 .016 .293 .270 .940 95.9
2000 .014 .197 .191 .948 99.0
4000 -.014 .134 .131 .941 99.7
8000 -.019 .088 .092 .957 100.0
Method N c Sandwich Bootstrap Bootstrap
Bias SE ESE CP ESE CP Succ. % Succ. %
4)
SPR
1000
10−4 .023 .349 .000 .000 .756 .987 97.1 84.3
10−3 .030 .226 .000 .003 .473 .963 95.4 80.5
10−2 .032 .227 .003 .022 .083 .417 77.9 71.8
2000
10−4 -.006 .253 .000 .000 .719 .993 97.5 82.0
10−3 .006 .147 .000 .002 .274 .902 95.2 76.8
10−2 .007 .136 .004 .034 .027 .275 73.8 66.9
4000
10−4 .001 .146 .000 .000 .574 .995 96.9 79.5
10−3 .004 .089 .000 .004 .108 .781 94.2 73.9
10−2 .000 .086 .002 .029 .019 .240 66.3 64.1
N Bias SE ESE CP Succ. %
ODE-Flex
1000 .020 .182 .191 .954 96.7
2000 .016 .132 .131 .958 98.8
4000 .001 .092 .090 .938 99.9
8000 .008 .062 .064 .960 100.0
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error
of the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage
proportion of 95% conﬁdence intervals.
68
Table S7: Simulation results for two placements of knots under the Cox model.
N Knots β1 = 1 β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1000 K1 .009 .153 .157 .952 .013 .157 .157 .952 .007 .152 .158 .961
K2 .009 .153 .157 .953 .013 .157 .157 .951 .007 .152 .158 .960
2000 K1 .007 .106 .109 .956 -.001 .107 .109 .955 .007 .105 .109 .961
K2 .006 .106 .110 .958 -.000 .107 .109 .956 .007 .105 .109 .960
4000 K1 .003 .076 .076 .936 -.002 .075 .076 .942 .000 .074 .076 .955
K2 .002 .076 .076 .937 -.002 .075 .077 .944 -.000 .074 .077 .955
8000 K1 -.002 .053 .054 .953 -.000 .052 .054 .957 -.002 .054 .054 .947
K2 -.001 .053 .054 .957 .001 .053 .054 .955 -.001 .053 .054 .949
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error
of the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage
proportion of 95% conﬁdence intervals. In (K1), the interior knots are located at the Kn = ⌊N
1
5 ⌋quantiles
of the distinct observation time points. In (K2), the interior knots equally separate the time interval from 0
to the maximum of observed times.
spline to ﬁt the function log q(·): (K1) the interior knots are located at the Kn = ⌊N
1
7 ⌋quantiles
of the estimated cumulative hazards {ˆΛCox
i }n
i=1 under the Cox model; (K2) the interior knots
equally separate the interval from 0 to 2 max 1≤i≤n{ˆΛCox
i }.
• For the general linear transformation model, we compare combinations of the above knots place-
ments when using the B-spline to ﬁt both functions log α(·) and log q(·): (K1) the interior knots
for both functions are located at the corresponding quantiles; (K2) the interior knots for both
functions equally separate the corresponding intervals.
Tables S7-S9 compare the estimates of regression coeﬃcients β with two natural placements of
knots for the ODE-Cox model, the ODE-AFT model, and the general linear transformation model
respectively. Figures S5-S6 compare the integrated mean square errors (IMSE) of estimated functions,
and the computing time associated with K1 and K2 from left to right for the ODE-Cox model and the
ODE-AFT model. We can see that both two types of knot locations K1 and K2 return good estimates
of parameters and standard errors. Overall, our numerical results suggest that knot selection does not
appear critical for the proposed method in various simulation settings.
6.7 Model diagnostics
In this section, we use the linear transformation model as an example to illustrate how the uniﬁcation
of the proposed ODE framework along with the proposed estimation and inference procedure can be
applied to model diagnostics and provide preliminary numerical results.
Recall that, under certain regularity conditions in Proposition 2, the linear transformation model,
69
Figure S5: Integrated mean square error (IMSE) of estimated α(·) and the log-log plot of the
computing time with respect to the sample size under the Cox model are provided from left to
right.
Table S8: Simulation results for two placements of knots under the AFT model.
N Knots β1 = 1 β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP Bias SE ESE CP
1000 K1 -.014 .197 .191 .944 -.024 .209 .192 .931 -.032 .199 .192 .932
K2 -.001 .194 .197 .954 -.010 .203 .197 .943 -.017 .195 .197 .945
2000 K1 -.010 .144 .137 .932 -.006 .144 .137 .937 -.005 .142 .137 .943
K2 -.005 .143 .139 .941 .000 .143 .139 .942 -.001 .141 .139 .953
4000 K1 .000 .102 .097 .944 -.005 .100 .097 .944 -.002 .097 .097 .950
K2 .002 .102 .098 .936 -.002 .100 .098 .938 .001 .097 .098 .950
8000 K1 -.006 .070 .069 .950 -.003 .068 .069 .967 -.004 .071 .069 .945
K2 -.005 .070 .069 .951 -.001 .068 .069 .958 -.004 .071 .069 .942
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample standard error
of the estimates. ESE is the mean of the standard error estimators, and CP is the corresponding coverage
proportion of 95% conﬁdence intervals. In (K1), the interior knots are located at the Kn = ⌊N
1
7 ⌋quantiles
of the estimated cumulative hazards under the Cox model. In (K2), the interior knots equally separate the
interval from 0 to two times the maximum of the estimated cumulative hazards.
70
Figure S6: Integrated mean square error (IMSE) of estimated α(·) and the log-log plot of the
computing time with respect to the sample size under the AFT model are provided from left
to right.
Table S9: Simulation results for two placements of knots under the general linear transformation
model where both functions α(·) and q(·) are unknown.
Setting Knots β2 = 1 β3 = 1
Bias SE ESE CP Bias SE ESE CP
1) K1 .008 .106 .107 .947 .012 .104 .107 .959
K2 -.002 .098 .097 .946 .000 .095 .097 .955
2) K1 -.019 .161 .151 .927 -.016 .159 .151 .938
K2 .005 .152 .142 .936 .009 .155 .142 .931
3) K1 -.014 .134 .131 .941 -.012 .131 .132 .945
K2 .002 .131 .124 .936 .004 .131 .128 .939
4) K1 .001 .092 .090 .939 .005 .091 .090 .954
K2 -.002 .087 .084 .940 .002 .085 .084 .957
Bias is the diﬀerence between the mean of estimates and the true value, and SE is
the sample standard error of the estimates. ESE is the mean of the standard error
estimators, and CP is the corresponding coverage proportion of 95% conﬁdence
intervals.
71
i.e., {
Λ′(t) = exp
(
xTβ+ γ(t) + g(Λ(t))
)
Λ(0) = 0
,
reduces to the Cox model if and only if there exist positive constants c1 and c2 such that g(t) =
log c2 + (1 −c1) logt, and it reduces to the AFT model if and only if there exist positive constants
c1 and c2 such that α(t) = log c2 + (c1 −1) logt for t >0. Therefore, to check whether the Cox or
the AFT model is correctly speciﬁed, we can artiﬁcially create an additional basis function, B(t), that
does not belong to the linear span of {1,log t}and make inference about its coeﬃcient.
Speciﬁcally, for checking the Cox model, we consider the following linear transformation model
Λ′
x(t) = exp
(
a1 log(Λx) + a2B(Λx) + xTβ+ γ(t)
)
, (S49)
with unspeciﬁed γ(·). Then a local test of the null hypothesis H0 : a2 = 0 is a test for checking the
Cox model speciﬁcation. Correspondingly, a local test of the null hypothesis H0 : b2 = 0 under the
model with unspeciﬁed g(·):
Λ′
x(t) = exp
(
g(Λx) + xTβ+ b1 log(t) + b2B(t)
)
(S50)
is a test for checking the AFT model speciﬁcation. We note that, under H0, the models (S49) and
(S50) are identiﬁable up to a constant respectively, which is a direct result of Proposition 2. Thus, to
guarantee the identiﬁability, we constraina1 = 0 and b1 = 0 in the models (S49) and (S50) respectively.
The proposed estimation and inference procedure can be applied to obtain the estimates of (a2,β,γ (·))
or (b2,β,g (·)) along with the local test of the corresponding H0.
Next, we examine the above method under the simulation settings (1) and (3) in the main text,
where the Cox and the AFT model are correctly speciﬁed respectively. We consider two choices of the
known basis function: B(t) = tand B(t) = log(1 + t). And we ﬁt the unknown functions γ(·) and g(·)
by cubic B-splines with the same placements of knots as described in the main text. The sample size
varies from 1000 to 8000.
Table S10 summarizes the estimates of the coeﬃcients of interests based on 1000 replications. We
can see that the bias of the estimator is nearly negligible in all settings. When the sample size is large,
the coverage proportion of 95% conﬁdence intervals, where the standard error estimator is obtained by
inverting the estimated information matrix of all parameters including the coeﬃcients of spline bases,
is slightly greater than the nominal level. The corresponding t-statistics would lead to a conservative
local test for H0. We also ﬁnd that the sample standard errors of the estimates vary with the choice
of the basis B(·), and the ability to detect the model speciﬁcation depends on B(·) as well. It may be
preferable to make both functions γ(·) and g(·) unknown in the nonparametric linear transformation
model for model diagnostics, which requires the asymptotic distributional theory for the functional
parameters. We leave this interesting direction for future work.
72
Table S10: Simulation results for checking the Cox and the AFT model speciﬁcation.
Setting B(t) = t B (t) = log(1 + t)
N 1000 2000 4000 8000 1000 2000 4000 8000
Cox is correctly
speciﬁed: a2 = 0
Bias .023 .007 .006 .003 .041 .000 .009 .007
SE .133 .087 .056 .033 .459 .314 .206 .112
ESE .136 .092 .062 .043 .467 .325 .226 .159
CP .949 .954 .956 .968 .943 .945 .956 .977
AFT is correctly
speciﬁed: b2 = 0
Bias -.003 .000 .003 .000 -.011 .003 .002 -.002
SE .182 .130 .083 .067 .423 .295 .198 .156
ESE .214 .155 .111 .079 .515 .376 .271 .195
CP .968 .960 .978 .961 .975 .963 .979 .964
Bias is the diﬀerence between the mean of estimates and the true value, and SE is the sample
standard error of the estimates. ESE is the mean of the standard error estimators, and CP is
the corresponding coverage proportion of 95% conﬁdence intervals.
73